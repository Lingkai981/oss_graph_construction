"""
LLM è¯„åˆ†æ¨¡å—

ä½¿ç”¨ DeepSeek API å¯¹ç¤¾åŒºæ°›å›´æŒ‡æ ‡è¿›è¡Œè¯„åˆ†ã€‚
åŸºäºæ¯’æ€§æŒ‡æ ‡å’Œ CHAOSS æŒ‡æ ‡ï¼Œç»™å‡ºç»¼åˆè¯„åˆ†å’Œè¯„ä»·ç†ç”±ã€‚
"""

from __future__ import annotations

import json
import os
import re
import time
import threading
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import requests
from dotenv import load_dotenv

from src.utils.logger import get_logger

logger = get_logger()

# åŠ è½½.envæ–‡ä»¶
env_path = Path(__file__).parent.parent.parent / ".env"
if env_path.exists():
    load_dotenv(env_path)
    PROJECT_ROOT = env_path.parent
else:
    load_dotenv()
    PROJECT_ROOT = Path(__file__).parent.parent.parent


# Prompt æ¨¡æ¿
LLM_SCORING_PROMPT = """ã€ç³»ç»Ÿè§’è‰²ã€‘
ä½ æ˜¯ä¸€ä¸ªå¼€æºç¤¾åŒºå¥åº·åº¦è¯„ä¼°ä¸“å®¶ï¼Œç²¾é€š CHAOSSï¼ˆCommunity Health Analytics for Open Source Softwareï¼‰æŒ‡æ ‡ä½“ç³»å’Œç¤¾åŒºæ¯’æ€§åˆ†æã€‚è¯·æ ¹æ®ç»™å®šçš„æœˆåº¦æŒ‡æ ‡æ•°æ®ï¼Œå¯¹ç¤¾åŒºæ°›å›´è¿›è¡Œå®¢è§‚è¯„åˆ†ã€‚

ã€æŒ‡æ ‡å®šä¹‰ä¸è§£é‡Šã€‘

## æ¯’æ€§æŒ‡æ ‡ï¼ˆæ¥æºï¼šToxiCR æ¯’æ€§æ£€æµ‹æ¨¡å‹ï¼‰

1. toxicity_meanï¼ˆå¹³å‡æ¯’æ€§åˆ†æ•°ï¼‰
   - å®šä¹‰ï¼šè¯¥æœˆæ‰€æœ‰è¯„è®ºçš„æ¯’æ€§æ¦‚ç‡å‡å€¼
   - èŒƒå›´ï¼š[0, 1]ï¼Œ0 è¡¨ç¤ºå®Œå…¨æ— æ¯’ï¼Œ1 è¡¨ç¤ºå®Œå…¨æœ‰æ¯’
   - å‚è€ƒï¼šå¼€æºç¤¾åŒºçš„å¥åº·å€¼é€šå¸¸åœ¨ 0.03-0.10 ä¹‹é—´

2. toxicity_p95ï¼ˆæ¯’æ€§95åˆ†ä½æ•°ï¼‰
   - å®šä¹‰ï¼š95%çš„è¯„è®ºæ¯’æ€§ä½äºæ­¤å€¼
   - æ„ä¹‰ï¼šåæ˜ æç«¯æ¯’æ€§è¡Œä¸ºçš„ä¸¥é‡ç¨‹åº¦

3. toxic_rate_0_5ï¼ˆé«˜æ¯’æ€§è¯„è®ºå æ¯”ï¼‰
   - å®šä¹‰ï¼šæ¯’æ€§æ¦‚ç‡ >= 0.5 çš„è¯„è®ºå æ€»è¯„è®ºçš„æ¯”ä¾‹
   - æ„ä¹‰ï¼šç›´æ¥åæ˜ æœ‰å®³è¯„è®ºçš„é¢‘ç‡

4. comment_analyzed_countï¼ˆè¢«åˆ†æè¯„è®ºæ•°ï¼‰
   - æ„ä¹‰ï¼šæ ·æœ¬é‡ï¼Œæ ·æœ¬è¶Šå¤§æŒ‡æ ‡è¶Šå¯é ï¼›æ ·æœ¬é‡è¿‡å°æ—¶åº”è°¨æ…è¯„ä»·

## CHAOSS æŒ‡æ ‡

5. change_request_closure_ratioï¼ˆå˜æ›´è¯·æ±‚å…³é—­ç‡ï¼‰
   - å®šä¹‰ï¼š(closed_prs + closed_issues) / (opened_prs + opened_issues)
   - æ„ä¹‰ï¼šè¡¡é‡ç¤¾åŒºå¤„ç†è´¡çŒ®çš„èƒ½åŠ›
   - è§£è¯»ï¼š
     - > 1.0ï¼šæ¶ˆåŒ–èƒ½åŠ›å¼ºï¼Œå¤„ç†é€Ÿåº¦å¿«äºæäº¤é€Ÿåº¦
     - = 1.0ï¼šå¹³è¡¡çŠ¶æ€
     - < 1.0ï¼šå­˜åœ¨ç§¯å‹ï¼Œå¯èƒ½éœ€è¦æ›´å¤šç»´æŠ¤è€…
     - å€¼ä¸º 0 æ—¶éœ€æ£€æŸ¥ opened_prs å’Œ opened_issues æ˜¯å¦ä¹Ÿä¸º 0ï¼ˆè¡¨ç¤ºæ— æœ‰æ•ˆæ•°æ®ï¼‰

6. time_to_first_response_medianï¼ˆé¦–æ¬¡å“åº”æ—¶é—´ä¸­ä½æ•°ï¼Œå•ä½ï¼šå°æ—¶ï¼‰
   - å®šä¹‰ï¼šä» Issue/PR åˆ›å»ºåˆ°ç¬¬ä¸€æ¡è¯„è®ºçš„æ—¶é—´é—´éš”çš„ä¸­ä½æ•°
   - æ„ä¹‰ï¼šåæ˜ ç¤¾åŒºæ´»è·ƒåº¦å’Œå“åº”é€Ÿåº¦ï¼Œæ—¶é—´è¶ŠçŸ­è¶Šå¥½
   - å‚è€ƒï¼šæ´»è·ƒç¤¾åŒºé€šå¸¸åœ¨ 2-12 å°æ—¶å†…å“åº”

7. time_to_first_response_p95ï¼ˆé¦–æ¬¡å“åº”æ—¶é—´95åˆ†ä½æ•°ï¼Œå•ä½ï¼šå°æ—¶ï¼‰
   - æ„ä¹‰ï¼šåæ˜ æœ€æ…¢å“åº”æƒ…å†µï¼Œ95%çš„è¯·æ±‚åœ¨æ­¤æ—¶é—´å†…å¾—åˆ°å“åº”

8. items_with_response / items_without_response
   - æ„ä¹‰ï¼šå“åº”è¦†ç›–ç‡ = with / (with + without)
   - è§£è¯»ï¼šè¦†ç›–ç‡è¶Šé«˜è¯´æ˜ç¤¾åŒºå“åº”è¶Šå…¨é¢

ã€è¯„åˆ†è¯´æ˜ã€‘

æ€»åˆ† 100 åˆ†ï¼Œåˆ†ä¸ºä¸¤ä¸ªç»´åº¦ï¼š
- æ¯’æ€§æ°´å¹³ï¼ˆ40 åˆ†ï¼‰ï¼šè¯„ä¼°ç¤¾åŒºäº¤æµçš„å¥åº·ç¨‹åº¦
- å“åº”æ•ˆç‡ï¼ˆ60 åˆ†ï¼‰ï¼šè¯„ä¼°ç¤¾åŒºå¤„ç†è´¡çŒ®çš„èƒ½åŠ›å’Œé€Ÿåº¦

è¯·æ ¹æ®æŒ‡æ ‡çš„å®é™…å«ä¹‰ï¼Œç»“åˆä½ å¯¹å¼€æºç¤¾åŒºå¥åº·åº¦çš„ç†è§£ï¼Œç»™å‡ºåˆç†çš„è¯„åˆ†ã€‚

ç‰¹åˆ«æ³¨æ„ï¼š
- å¦‚æœæŸäº›æŒ‡æ ‡æ•°æ®ä¸º 0 æˆ–æ ·æœ¬é‡è¿‡å°ï¼Œè¯·åœ¨ç†ç”±ä¸­è¯´æ˜æ•°æ®å¯ä¿¡åº¦é—®é¢˜
- è¯„åˆ†åº”åŸºäºæŒ‡æ ‡çš„å®¢è§‚å«ä¹‰ï¼Œè€Œéä¸»è§‚å°è±¡

ã€è¾“å…¥æ•°æ®ã€‘

ä»“åº“ï¼š{repo_name}
æœˆä»½ï¼š{month}

æŒ‡æ ‡æ•°æ®ï¼š
- toxicity_mean: {toxicity_mean}
- toxicity_p95: {toxicity_p95}
- toxic_rate_0_5: {toxic_rate_0_5}
- toxic_comment_count_0_5: {toxic_comment_count_0_5}
- comment_analyzed_count: {comment_analyzed_count}
- change_request_closure_ratio: {change_request_closure_ratio}
- opened_prs: {opened_prs}
- closed_prs: {closed_prs}
- opened_issues: {opened_issues}
- closed_issues: {closed_issues}
- time_to_first_response_median: {time_to_first_response_median}
- time_to_first_response_mean: {time_to_first_response_mean}
- time_to_first_response_p95: {time_to_first_response_p95}
- items_with_response: {items_with_response}
- items_without_response: {items_without_response}

ã€è¾“å‡ºè¦æ±‚ã€‘

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ï¼š

{{
  "score": <0-100çš„æ•´æ•°>,
  "toxicity_score": <0-40çš„æ•´æ•°>,
  "response_score": <0-60çš„æ•´æ•°>,
  "toxicity_reason": "<æ¯’æ€§è¯„åˆ†ç†ç”±ï¼Œ50å­—ä»¥å†…>",
  "response_reason": "<å“åº”æ•ˆç‡è¯„åˆ†ç†ç”±ï¼Œ50å­—ä»¥å†…>",
  "overall_reason": "<ç»¼åˆè¯„ä»·ï¼Œ80å­—ä»¥å†…>"
}}"""


class LLMScorer:
    """
    LLM è¯„åˆ†å™¨
    
    ä½¿ç”¨ DeepSeek API å¯¹ç¤¾åŒºæ°›å›´æŒ‡æ ‡è¿›è¡Œè¯„åˆ†ã€‚
    æ”¯æŒç¼“å­˜å’Œæ–­ç‚¹ç»­ä¼ ã€‚
    """
    
    API_BASE_URL = "https://api.deepseek.com/v1/chat/completions"
    DEFAULT_TIMEOUT = 60  # ç§’
    MAX_RETRIES = 3
    RETRY_DELAY = 2  # ç§’
    
    def __init__(self, api_key: Optional[str] = None, cache_file: Optional[str] = None) -> None:
        """
        åˆå§‹åŒ– LLM è¯„åˆ†å™¨
        
        Args:
            api_key: DeepSeek API å¯†é’¥ï¼Œå¦‚æœä¸º None åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
        """
        if api_key is None:
            api_key = os.getenv("DEEPSEEK_API_KEY")
        self.api_key = api_key
        self._available = self.api_key is not None and len(self.api_key.strip()) > 0
        
        # ç¼“å­˜è®¾ç½®
        if cache_file is None:
            cache_dir = PROJECT_ROOT / "output" / "community-atmosphere-analysis"
            cache_dir.mkdir(parents=True, exist_ok=True)
            self._cache_path = cache_dir / "llm_score_cache.json"
        else:
            self._cache_path = Path(cache_file)
        
        self._cache_lock = threading.Lock()
        self._cache: Dict[str, Dict[str, Any]] = self._load_cache()
        
        if self._available:
            logger.info(f"âœ“ LLM è¯„åˆ†å™¨åˆå§‹åŒ–æˆåŠŸï¼ŒAPI Key å·²é…ç½®")
        else:
            logger.warning("âœ— LLM è¯„åˆ†å™¨åˆå§‹åŒ–å¤±è´¥ï¼ŒDEEPSEEK_API_KEY æœªé…ç½®")
    
    def _load_cache(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½ç¼“å­˜"""
        if not self._cache_path.exists():
            return {}
        try:
            with open(self._cache_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, dict):
                logger.info(f"åŠ è½½ LLM è¯„åˆ†ç¼“å­˜: {len(data)} æ¡è®°å½•")
                return data
        except Exception as e:
            logger.warning(f"åŠ è½½ LLM è¯„åˆ†ç¼“å­˜å¤±è´¥: {e}")
        return {}
    
    def _save_cache(self) -> None:
        """ä¿å­˜ç¼“å­˜"""
        try:
            with self._cache_lock:
                cache_snapshot = dict(self._cache)
            with open(self._cache_path, "w", encoding="utf-8") as f:
                json.dump(cache_snapshot, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ä¿å­˜ LLM è¯„åˆ†ç¼“å­˜å¤±è´¥: {e}")
    
    @staticmethod
    def _make_cache_key(repo_name: str, month: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{repo_name}:{month}"
    
    def is_available(self) -> bool:
        """æ£€æŸ¥ API æ˜¯å¦å¯ç”¨"""
        return self._available
    
    def get_cached_score(self, repo_name: str, month: str) -> Optional[Dict[str, Any]]:
        """
        è·å–ç¼“å­˜çš„è¯„åˆ†ç»“æœ
        
        Args:
            repo_name: ä»“åº“åç§°
            month: æœˆä»½
            
        Returns:
            ç¼“å­˜çš„è¯„åˆ†ç»“æœï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        cache_key = self._make_cache_key(repo_name, month)
        with self._cache_lock:
            return self._cache.get(cache_key)
    
    def score_monthly_metrics(
        self,
        repo_name: str,
        month: str,
        metrics: Dict[str, Any],
        use_cache: bool = True,
    ) -> Dict[str, Any]:
        """
        å¯¹æœˆåº¦æŒ‡æ ‡è¿›è¡Œ LLM è¯„åˆ†
        
        Args:
            repo_name: ä»“åº“åç§°
            month: æœˆä»½
            metrics: æœˆåº¦æŒ‡æ ‡æ•°æ®
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            è¯„åˆ†ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - score: æ€»åˆ† (0-100)
            - toxicity_score: æ¯’æ€§å¾—åˆ† (0-40)
            - response_score: å“åº”æ•ˆç‡å¾—åˆ† (0-60)
            - toxicity_reason: æ¯’æ€§è¯„åˆ†ç†ç”±
            - response_reason: å“åº”æ•ˆç‡è¯„åˆ†ç†ç”±
            - overall_reason: ç»¼åˆè¯„ä»·
        """
        if not self.is_available():
            logger.error("DeepSeek API ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œ LLM è¯„åˆ†")
            return self._default_score("API ä¸å¯ç”¨")
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._make_cache_key(repo_name, month)
        if use_cache:
            with self._cache_lock:
                cached = self._cache.get(cache_key)
            if cached is not None:
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„ LLM è¯„åˆ†: {repo_name} {month}")
                return cached
        
        # æ„å»º prompt
        prompt = LLM_SCORING_PROMPT.format(
            repo_name=repo_name,
            month=month,
            toxicity_mean=metrics.get("toxicity_mean", 0.0),
            toxicity_p95=metrics.get("toxicity_p95", 0.0),
            toxic_rate_0_5=metrics.get("toxic_rate_0_5", 0.0),
            toxic_comment_count_0_5=metrics.get("toxic_comment_count_0_5", 0),
            comment_analyzed_count=metrics.get("comment_analyzed_count", 0),
            change_request_closure_ratio=metrics.get("change_request_closure_ratio", 0.0),
            opened_prs=metrics.get("opened_prs", 0),
            closed_prs=metrics.get("closed_prs", 0),
            opened_issues=metrics.get("opened_issues", 0),
            closed_issues=metrics.get("closed_issues", 0),
            time_to_first_response_median=metrics.get("time_to_first_response_median", 0.0),
            time_to_first_response_mean=metrics.get("time_to_first_response_mean", 0.0),
            time_to_first_response_p95=metrics.get("time_to_first_response_p95", 0.0),
            items_with_response=metrics.get("items_with_response", 0),
            items_without_response=metrics.get("items_without_response", 0),
        )
        
        # è°ƒç”¨ API
        result = self._call_api(prompt, repo_name, month)
        
        # ç¼“å­˜ç»“æœ
        with self._cache_lock:
            self._cache[cache_key] = result
        self._save_cache()
        
        return result
    
    def _call_api(self, prompt: str, repo_name: str, month: str) -> Dict[str, Any]:
        """
        è°ƒç”¨ DeepSeek API
        
        Args:
            prompt: å®Œæ•´çš„ prompt
            repo_name: ä»“åº“åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            month: æœˆä»½ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            è§£æåçš„è¯„åˆ†ç»“æœ
        """
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,  # è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¯„åˆ†
            "max_tokens": 512,
        }
        
        last_error = None
        for attempt in range(self.MAX_RETRIES):
            try:
                response = requests.post(
                    self.API_BASE_URL,
                    headers=headers,
                    json=payload,
                    timeout=self.DEFAULT_TIMEOUT,
                )
                response.raise_for_status()
                
                # è§£æå“åº”
                result = response.json()
                content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
                content = content.strip()
                
                # è§£æ JSON
                parsed = self._parse_response(content)
                if parsed is not None:
                    logger.info(f"  LLM è¯„åˆ†å®Œæˆ: {repo_name} {month} -> {parsed['score']}åˆ†")
                    return parsed
                else:
                    logger.warning(f"  LLM å“åº”è§£æå¤±è´¥: {repo_name} {month}, å“åº”: {content[:100]}")
                    last_error = "å“åº”è§£æå¤±è´¥"
                    
            except requests.exceptions.Timeout:
                last_error = "è¯·æ±‚è¶…æ—¶"
                logger.warning(f"  LLM è¯„åˆ†è¶…æ—¶ (attempt {attempt + 1}/{self.MAX_RETRIES}): {repo_name} {month}")
            except requests.exceptions.RequestException as e:
                last_error = str(e)
                logger.warning(f"  LLM è¯„åˆ†è¯·æ±‚å¤±è´¥ (attempt {attempt + 1}/{self.MAX_RETRIES}): {repo_name} {month}, é”™è¯¯: {e}")
            except Exception as e:
                last_error = str(e)
                logger.warning(f"  LLM è¯„åˆ†å¼‚å¸¸ (attempt {attempt + 1}/{self.MAX_RETRIES}): {repo_name} {month}, é”™è¯¯: {e}")
            
            # é‡è¯•å‰ç­‰å¾…
            if attempt < self.MAX_RETRIES - 1:
                time.sleep(self.RETRY_DELAY * (attempt + 1))
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(f"  LLM è¯„åˆ†å¤±è´¥: {repo_name} {month}, æœ€åé”™è¯¯: {last_error}")
        return self._default_score(f"API è°ƒç”¨å¤±è´¥: {last_error}")
    
    def _parse_response(self, content: str) -> Optional[Dict[str, Any]]:
        """
        è§£æ LLM å“åº”
        
        Args:
            content: LLM è¿”å›çš„å†…å®¹
            
        Returns:
            è§£æåçš„å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› None
        """
        try:
            # å°è¯•ç›´æ¥è§£æ JSON
            parsed = json.loads(content)
            return self._validate_and_normalize(parsed)
        except json.JSONDecodeError:
            pass
        
        # å°è¯•ä»å†…å®¹ä¸­æå– JSON
        # åŒ¹é… {...} æ ¼å¼
        json_match = re.search(r'\{[^{}]*\}', content, re.DOTALL)
        if json_match:
            try:
                parsed = json.loads(json_match.group())
                return self._validate_and_normalize(parsed)
            except json.JSONDecodeError:
                pass
        
        return None
    
    def _validate_and_normalize(self, parsed: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        éªŒè¯å¹¶è§„èŒƒåŒ–è§£æç»“æœ
        
        Args:
            parsed: è§£æåçš„å­—å…¸
            
        Returns:
            è§„èŒƒåŒ–åçš„å­—å…¸ï¼Œå¦‚æœéªŒè¯å¤±è´¥è¿”å› None
        """
        try:
            score = int(parsed.get("score", 0))
            toxicity_score = int(parsed.get("toxicity_score", 0))
            response_score = int(parsed.get("response_score", 0))
            
            # è¾¹ç•Œæ£€æŸ¥
            score = max(0, min(100, score))
            toxicity_score = max(0, min(40, toxicity_score))
            response_score = max(0, min(60, response_score))
            
            return {
                "score": score,
                "toxicity_score": toxicity_score,
                "response_score": response_score,
                "toxicity_reason": str(parsed.get("toxicity_reason", ""))[:100],
                "response_reason": str(parsed.get("response_reason", ""))[:100],
                "overall_reason": str(parsed.get("overall_reason", ""))[:150],
            }
        except (ValueError, TypeError):
            return None
    
    def _default_score(self, reason: str) -> Dict[str, Any]:
        """
        è¿”å›é»˜è®¤è¯„åˆ†ï¼ˆç”¨äº API è°ƒç”¨å¤±è´¥æ—¶ï¼‰
        
        Args:
            reason: å¤±è´¥åŸå› 
            
        Returns:
            é»˜è®¤è¯„åˆ†å­—å…¸
        """
        return {
            "score": 0,
            "toxicity_score": 0,
            "response_score": 0,
            "toxicity_reason": "",
            "response_reason": "",
            "overall_reason": f"è¯„åˆ†å¤±è´¥: {reason}",
        }

    def score_batch(
        self,
        tasks: List[Tuple[str, str, Dict[str, Any]]],
        max_workers: int = 8,
        rate_limit_delay: float = 0.1,
    ) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡å¹¶å‘è¯„åˆ†
        
        Args:
            tasks: è¯„åˆ†ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡æ˜¯ (repo_name, month, metrics) å…ƒç»„
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤ 8
            rate_limit_delay: æ¯æ¬¡è¯·æ±‚ä¹‹é—´çš„æœ€å°é—´éš”ï¼ˆç§’ï¼‰ï¼Œç”¨äºé™æµ
            
        Returns:
            {cache_key: score_result} å­—å…¸ï¼Œcache_key æ ¼å¼ä¸º "repo_name:month"
        """
        if not self.is_available():
            logger.error("DeepSeek API ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ‰¹é‡ LLM è¯„åˆ†")
            return {}
        
        # è¿‡æ»¤æ‰å·²ç¼“å­˜çš„ä»»åŠ¡
        tasks_to_process: List[Tuple[str, str, Dict[str, Any]]] = []
        results: Dict[str, Dict[str, Any]] = {}
        
        for repo_name, month, metrics in tasks:
            cache_key = self._make_cache_key(repo_name, month)
            cached = self.get_cached_score(repo_name, month)
            if cached is not None:
                results[cache_key] = cached
                logger.debug(f"ä½¿ç”¨ç¼“å­˜: {repo_name} {month}")
            else:
                tasks_to_process.append((repo_name, month, metrics))
        
        if not tasks_to_process:
            logger.info(f"æ‰¹é‡è¯„åˆ†: å…¨éƒ¨ {len(tasks)} ä¸ªä»»åŠ¡å·²æœ‰ç¼“å­˜ï¼Œæ— éœ€è°ƒç”¨ API")
            print(f"    âœ” å…¨éƒ¨ {len(tasks)} ä¸ªæœˆä»½å·²æœ‰ç¼“å­˜ï¼Œè·³è¿‡ API è°ƒç”¨", flush=True)
            return results
        
        total = len(tasks_to_process)
        cached_count = len(tasks) - total
        logger.info(f"æ‰¹é‡è¯„åˆ†: å…± {len(tasks)} ä¸ªä»»åŠ¡ï¼Œ{cached_count} ä¸ªå·²ç¼“å­˜ï¼Œ{total} ä¸ªéœ€è°ƒç”¨ API")
        print(f"    ğŸ“Š LLM è¯„åˆ†: {cached_count} ä¸ªå·²ç¼“å­˜, {total} ä¸ªéœ€è°ƒç”¨ API (max {max_workers} å¹¶å‘)", flush=True)
        
        # ç”¨äºé™æµçš„é”å’Œè®¡æ•°å™¨
        rate_limit_lock = threading.Lock()
        last_request_time = [0.0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
        completed_count = [0]
        
        def process_single_task(task: Tuple[str, str, Dict[str, Any]]) -> Tuple[str, Dict[str, Any]]:
            repo_name, month, metrics = task
            cache_key = self._make_cache_key(repo_name, month)
            
            # é™æµæ§åˆ¶
            with rate_limit_lock:
                now = time.time()
                elapsed = now - last_request_time[0]
                if elapsed < rate_limit_delay:
                    time.sleep(rate_limit_delay - elapsed)
                last_request_time[0] = time.time()
            
            # è°ƒç”¨è¯„åˆ†
            result = self.score_monthly_metrics(repo_name, month, metrics, use_cache=True)
            
            # æ›´æ–°è¿›åº¦
            with rate_limit_lock:
                completed_count[0] += 1
                # æ¯ 5 ä¸ªæˆ–æœ€åä¸€ä¸ªæ˜¾ç¤ºè¿›åº¦
                if completed_count[0] % 5 == 0 or completed_count[0] == total:
                    pct = completed_count[0] / total * 100
                    print(f"    â³ API è°ƒç”¨è¿›åº¦: {completed_count[0]}/{total} ({pct:.0f}%)", flush=True)
            
            return cache_key, result
        
        # å¹¶å‘æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(process_single_task, task): task
                for task in tasks_to_process
            }
            
            for future in as_completed(futures):
                task = futures[future]
                try:
                    cache_key, result = future.result()
                    results[cache_key] = result
                except Exception as e:
                    repo_name, month, _ = task
                    cache_key = self._make_cache_key(repo_name, month)
                    logger.error(f"æ‰¹é‡è¯„åˆ†å¼‚å¸¸: {repo_name} {month}, é”™è¯¯: {e}")
                    results[cache_key] = self._default_score(str(e))
        
        print(f"    âœ” LLM è¯„åˆ†å®Œæˆ: {total} ä¸ª API è°ƒç”¨ + {cached_count} ä¸ªç¼“å­˜", flush=True)
        logger.info(f"æ‰¹é‡ LLM è¯„åˆ†å®Œæˆ: {total} ä¸ªä»»åŠ¡")
        
        return results
