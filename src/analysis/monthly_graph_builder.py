"""
æŒ‰æœˆæ„å»ºé¡¹ç›®å›¾ï¼ˆä¸‰ç±»æ—¶åºå›¾ï¼‰

åŠŸèƒ½ï¼š
1. è¯»å– data/filtered/ ä¸‹çš„æ—¥ç²’åº¦æ•°æ®
2. æŒ‰æœˆèšåˆï¼ŒæŒ‰é¡¹ç›®åˆ†ç»„
3. ä¸ºæ¯ä¸ªé¡¹ç›®çš„æ¯ä¸ªæœˆæ„å»ºä¸‰ç±»å›¾ï¼š
   - Actor-Actor åä½œå›¾
   - Actor-Repository äºŒéƒ¨å›¾
   - Actor-Discussion äºŒéƒ¨å›¾
4. ä¿å­˜å›¾æ•°æ®ä¾›åç»­åˆ†æ

è¾“å‡ºç»“æ„ï¼š
output/monthly-graphs/
â”œâ”€â”€ facebook-react/
â”‚   â”œâ”€â”€ actor-actor/
â”‚   â”‚   â”œâ”€â”€ 2023-01.graphml
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ actor-repo/
â”‚   â”‚   â”œâ”€â”€ 2023-01.graphml
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ actor-discussion/
â”‚       â”œâ”€â”€ 2023-01.graphml
â”‚       â””â”€â”€ ...
â””â”€â”€ ...
"""

from __future__ import annotations

import json
import os
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

import re

import networkx as nx


def sanitize_graphml_attributes(g: nx.Graph) -> None:
    """å°±åœ°æ¸…æ´—å›¾ã€èŠ‚ç‚¹ã€è¾¹å±æ€§ï¼Œé¿å…å†™å‡ºçš„ GraphML ä¸æ˜¯åˆæ³• XMLã€‚"""
    # graph-level
    for k, v in list(g.graph.items()):
        if isinstance(v, str) or v is None:
            g.graph[k] = _sanitize_xml_text(v)
        elif isinstance(v, (dict, list)):
            g.graph[k] = _sanitize_xml_text(json.dumps(v, ensure_ascii=False))
        else:
            # numbers/bools etc are fine
            pass

    # nodes
    for n, attrs in list(g.nodes(data=True)):
        for k, v in list(attrs.items()):
            if isinstance(v, str) or v is None:
                attrs[k] = _sanitize_xml_text(v)
            elif isinstance(v, (dict, list)):
                attrs[k] = _sanitize_xml_text(json.dumps(v, ensure_ascii=False))
            else:
                pass

    # edges (MultiDiGraph compatible)
    if hasattr(g, "edges"):
        for u, v, key, attrs in list(g.edges(keys=True, data=True)):
            for k, val in list(attrs.items()):
                if isinstance(val, str) or val is None:
                    attrs[k] = _sanitize_xml_text(val)
                elif isinstance(val, (dict, list)):
                    attrs[k] = _sanitize_xml_text(json.dumps(val, ensure_ascii=False))
                else:
                    pass

from src.utils.logger import get_logger

logger = get_logger()


# ==================== æ•°æ®ç±» ====================

@dataclass
class ActorStats:
    """Actor ç»Ÿè®¡ä¿¡æ¯"""
    actor_id: int
    login: str
    event_count: int = 0
    event_types: Dict[str, int] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "node_type": "Actor",
            "actor_id": self.actor_id,
            "login": _sanitize_xml_text(self.login),
            "event_count": self.event_count,
            "event_types": json.dumps(dict(self.event_types)),
        }


@dataclass 
class RepoStats:
    """Repository ç»Ÿè®¡ä¿¡æ¯"""
    repo_id: int
    name: str
    event_count: int = 0
    event_types: Dict[str, int] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "node_type": "Repository",
            "repo_id": self.repo_id,
            "name": _sanitize_xml_text(self.name),
            "event_count": self.event_count,
            "event_types": json.dumps(dict(self.event_types)),
        }


@dataclass
class DiscussionStats:
    """Issue/PR ç»Ÿè®¡ä¿¡æ¯"""
    key: str  # issue:repo_id:number æˆ– pr:repo_id:number
    node_type: str  # Issue æˆ– PullRequest
    number: int
    title: str = ""
    state: str = ""
    creator_id: Optional[int] = None
    creator_login: str = ""
    comment_count: int = 0
    participants: Set[int] = field(default_factory=set)
    created_at: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "node_type": self.node_type,
            "key": _sanitize_xml_text(self.key),
            "number": self.number,
            "title": (_sanitize_xml_text(self.title)[:100] if self.title else ""),
            "state": _sanitize_xml_text(self.state),
            "creator_id": self.creator_id or 0,
            "creator_login": _sanitize_xml_text(self.creator_login),
            "comment_count": self.comment_count,
            "participants_count": len(self.participants),
            "created_at": _sanitize_xml_text(self.created_at or ""),
        }


# ==================== æ•°æ®åŠ è½½ ====================

def load_filtered_data(data_dir: str) -> Dict[str, List[Dict]]:
    """åŠ è½½è¿‡æ»¤åçš„æ•°æ®ï¼ŒæŒ‰æ—¥æœŸç»„ç»‡"""
    data_path = Path(data_dir)
    all_data = defaultdict(list)
    
    files = sorted(data_path.glob("*-filtered.json"))
    total_files = len(files)
    total_events = 0
    
    print(f"æ‰¾åˆ° {total_files} ä¸ªæ•°æ®æ–‡ä»¶")
    
    for idx, file_path in enumerate(files, 1):
        filename = file_path.stem
        parts = filename.replace("-filtered", "").split("-")
        if len(parts) >= 3:
            date_str = f"{parts[0]}-{parts[1]}-{parts[2]}"
            
            file_events = 0
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line:
                        try:
                            event = json.loads(line)
                            all_data[date_str].append(event)
                            file_events += 1
                        except json.JSONDecodeError:
                            continue
            
            total_events += file_events
            
            if idx % 50 == 0 or idx == total_files:
                print(f"  åŠ è½½è¿›åº¦: {idx}/{total_files} æ–‡ä»¶ï¼Œç´¯è®¡ {total_events} äº‹ä»¶")
    
    print(f"æ•°æ®åŠ è½½å®Œæˆ: {len(all_data)} å¤©ï¼Œ{total_events} ä¸ªäº‹ä»¶")
    return dict(all_data)


def group_by_month_and_repo(
    daily_data: Dict[str, List[Dict]]
) -> Dict[str, Dict[str, List[Dict]]]:
    """æŒ‰æœˆå’Œé¡¹ç›®åˆ†ç»„"""
    result = defaultdict(lambda: defaultdict(list))
    
    for date_str, events in daily_data.items():
        month = date_str[:7]
        for event in events:
            repo_name = event.get("repo", {}).get("name", "").lower()
            if repo_name:
                result[month][repo_name].append(event)
    
    logger.info(f"æŒ‰æœˆåˆ†ç»„åå…± {len(result)} ä¸ªæœˆ")
    return {k: dict(v) for k, v in result.items()}


# ==================== å›¾æ„å»ºå‡½æ•° ====================

def _clean_text_for_xml(text: str) -> str:
    """
    æ¸…ç†æ–‡æœ¬ä¸­çš„æ§åˆ¶å­—ç¬¦ï¼Œä½¿å…¶å¯ä»¥ä½œä¸º XML æ–‡æœ¬å®‰å…¨å†™å…¥
    
    ç§»é™¤ XML 1.0 ä¸å…è®¸çš„æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº† \t \n \rï¼‰ï¼Œä¿ç•™å…¶ä»–æ‰€æœ‰å­—ç¬¦ã€‚
    NetworkX/ElementTree ä¼šè‡ªåŠ¨å¤„ç† XML è½¬ä¹‰ï¼ˆ& < > " 'ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œåªæ¸…ç†æ§åˆ¶å­—ç¬¦ã€‚
    
    é€‚ç”¨äºæ‰€æœ‰æ–‡æœ¬å­—æ®µï¼šlogin, name, title, creator_login ç­‰
    """
    if not text:
        return ""
    
    # è¿‡æ»¤ XML ä¸å…è®¸çš„æ§åˆ¶å­—ç¬¦ï¼ˆä¸ _sanitize_comment_text ä½¿ç”¨ç›¸åŒçš„é€»è¾‘ï¼‰
    return "".join(
        ch
        for ch in text
        if (
            ch == "\t"
            or ch == "\n"
            or ch == "\r"
            or 0x20 <= ord(ch) <= 0xD7FF
            or 0xE000 <= ord(ch) <= 0xFFFD
        )
    )


def _escape_xml_text(text: str) -> str:
    """
    è½¬ä¹‰ XML ç‰¹æ®Šå­—ç¬¦å¹¶æ¸…ç†æ§åˆ¶å­—ç¬¦
    
    å…ˆæ¸…ç† XML ä¸å…è®¸çš„æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº† \t \n \rï¼‰ï¼Œå†è¿›è¡Œ XML è½¬ä¹‰ï¼Œ
    ç¡®ä¿ç”Ÿæˆçš„ XML æ–‡ä»¶æ ¼å¼æ­£ç¡®ã€‚ä¸ _sanitize_comment_text ä½¿ç”¨ç›¸åŒçš„æ§åˆ¶å­—ç¬¦è¿‡æ»¤é€»è¾‘ã€‚
    
    æ³¨æ„ï¼šæ­¤å‡½æ•°ä¼šè½¬ä¹‰ XML ç‰¹æ®Šå­—ç¬¦ï¼Œé€‚ç”¨äºéœ€è¦æ‰‹åŠ¨è½¬ä¹‰çš„åœºæ™¯ã€‚
    å¯¹äº NetworkX å†™å…¥çš„å­—æ®µï¼Œä½¿ç”¨ _clean_text_for_xml å³å¯ï¼ˆNetworkX ä¼šè‡ªåŠ¨è½¬ä¹‰ï¼‰ã€‚
    """
    if not text:
        return ""
    
    # å…ˆæ¸…ç†æ§åˆ¶å­—ç¬¦
    cleaned = _clean_text_for_xml(text)
    
    # ç„¶åè½¬ä¹‰ XML ç‰¹æ®Šå­—ç¬¦
    return (cleaned
            .replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
            .replace('"', "&quot;")
            .replace("'", "&apos;")
            )
def _sanitize_xml_text(text: str) -> str:
    """æ¸…æ´— GraphML/XML ä¸­ä¸åˆæ³•å­—ç¬¦ï¼Œå¹¶è½¬ä¹‰ XML ç‰¹æ®Šå­—ç¬¦ã€‚

    ä¸»è¦è§£å†³ï¼š
    - XML 1.0 ä¸å…è®¸çš„æ§åˆ¶å­—ç¬¦ï¼ˆä¼šå¯¼è‡´ not well-formed / invalid tokenï¼‰
    - æœªè½¬ä¹‰çš„ç‰¹æ®Šå­—ç¬¦ & < > ' "
    """
    if text is None:
        return ""
    if not isinstance(text, str):
        text = str(text)

    # 1) ç§»é™¤ XML 1.0 ä¸å…è®¸çš„å­—ç¬¦
    # å…è®¸ï¼š\t \n \rï¼›ä»¥åŠ U+0020 ä»¥ä¸Šçš„å¸¸è§„å­—ç¬¦ï¼ˆå¹¶æ’é™¤ surrogate åŒºï¼‰
    cleaned_chars = []
    for ch in text:
        code = ord(ch)
        if ch in ("\t", "\n", "\r"):
            cleaned_chars.append(ch)
            continue
        # åŸºæœ¬å¯è§å­—ç¬¦
        if 0x20 <= code <= 0xD7FF:
            cleaned_chars.append(ch)
            continue
        # æ’é™¤ surrogateï¼š0xD800-0xDFFF
        if 0xE000 <= code <= 0xFFFD:
            cleaned_chars.append(ch)
            continue
        # é BMP å­—ç¬¦ï¼ˆPython å¯è¡¨ç¤ºï¼‰
        if 0x10000 <= code <= 0x10FFFF:
            cleaned_chars.append(ch)
            continue
        # å…¶å®ƒï¼šä¸¢å¼ƒ
    text = "".join(cleaned_chars)

    # 2) è½¬ä¹‰ XML ç‰¹æ®Šå­—ç¬¦ï¼ˆGraphML æœ¬è´¨æ˜¯ XMLï¼‰
    return (text
            .replace("&", "&amp;")
            .replace("<", "&lt;")
            .replace(">", "&gt;")
            .replace('"', "&quot;")
            .replace("'", "&apos;")
            )


_ANSI_ESCAPE_RE = re.compile(r"\x1B\[[0-?]*[ -/]*[@-~]")


def _sanitize_comment_text(text: str) -> str:
    """
    æ¸…æ´—è¯„è®ºæ­£æ–‡/æ—¥å¿—æ–‡æœ¬ï¼Œä½¿å…¶å¯ä»¥ä½œä¸º GraphML æ–‡æœ¬å®‰å…¨å†™å…¥ï¼š
    - å»æ‰ ANSI é¢œè‰²æ§åˆ¶åºåˆ—ï¼ˆä¾‹å¦‚ [32mï¼‰
    - å»æ‰ XML 1.0 ä¸å…è®¸çš„æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº† \t \n \rï¼‰
    å…¶ä½™å†…å®¹åŸæ ·ä¿ç•™ï¼Œè®© NetworkX/ElementTree è´Ÿè´£æ­£å¸¸çš„ XML è½¬ä¹‰ã€‚
    """
    if not text:
        return ""

    # 1) å»æ‰ ANSI é¢œè‰²æ§åˆ¶åºåˆ—
    cleaned = _ANSI_ESCAPE_RE.sub("", text)

    # 2) è¿‡æ»¤ XML ä¸å…è®¸çš„æ§åˆ¶å­—ç¬¦
    return "".join(
        ch
        for ch in cleaned
        if (
            ch == "\t"
            or ch == "\n"
            or ch == "\r"
            or 0x20 <= ord(ch) <= 0xD7FF
            or 0xE000 <= ord(ch) <= 0xFFFD
        )
    )


def build_actor_actor_graph(
    events: List[Dict],
    repo_name: str,
    month: str,
) -> nx.MultiDiGraph:
    """
    æ„å»º Actor-Actor åä½œå›¾
    
    è¾¹ç±»å‹ï¼š
    - ISSUE_INTERACTION: åœ¨ä»–äººåˆ›å»ºçš„ Issue ä¸‹è¯„è®º
    - PR_REVIEW: å®¡æŸ¥ä»–äººçš„ PR
    - PR_MERGE: åˆå¹¶ä»–äººçš„ PR
    - ISSUE_CO_PARTICIPANT: åŒ Issue ä¸‹çš„å‚ä¸è€…
    """
    graph = nx.MultiDiGraph()
    actors: Dict[int, ActorStats] = {}
    
    issue_creators: Dict[int, int] = {}
    pr_creators: Dict[int, int] = {}
    issue_participants: Dict[int, Set[int]] = defaultdict(set)
    edges: List[Dict] = []
    
    def _ensure_actor(actor_data: Dict) -> Optional[int]:
        actor_id = actor_data.get("id")
        if actor_id is None:
            return None
        if actor_id not in actors:
            actors[actor_id] = ActorStats(
                actor_id=actor_id,
                login=_clean_text_for_xml(actor_data.get("login") or ""),
            )
        return actor_id
    
    def _update_actor_stats(actor_id: int, event_type: str, created_at: str):
        if actor_id in actors:
            actors[actor_id].event_count += 1
            actors[actor_id].event_types[event_type] = \
                actors[actor_id].event_types.get(event_type, 0) + 1
    
    # ç¬¬ä¸€éï¼šæ”¶é›†åˆ›å»ºè€…
    for event in events:
        event_type = event.get("type") or ""
        payload = event.get("payload") or {}
        actor = event.get("actor") or {}
        actor_id = _ensure_actor(actor)
        
        if event_type == "IssuesEvent" and payload.get("action") == "opened":
            issue = payload.get("issue") or {}
            issue_number = issue.get("number")
            if issue_number and actor_id:
                issue_creators[issue_number] = actor_id
        
        elif event_type == "PullRequestEvent" and payload.get("action") == "opened":
            pr = payload.get("pull_request") or {}
            pr_number = pr.get("number")
            if pr_number and actor_id:
                pr_creators[pr_number] = actor_id
    
    # ç¬¬äºŒéï¼šæ„å»ºè¾¹
    for event in events:
        event_id = event.get("id")
        event_type = event.get("type") or ""
        created_at = event.get("created_at") or ""
        actor = event.get("actor") or {}
        actor_id = _ensure_actor(actor)
        
        if actor_id is None:
            continue
        
        _update_actor_stats(actor_id, event_type, created_at)
        payload = event.get("payload") or {}
        
        if event_type == "IssueCommentEvent":
            issue = payload.get("issue") or {}
            issue_number = issue.get("number")
            if issue_number:
                issue_participants[issue_number].add(actor_id)
                creator_id = issue_creators.get(issue_number)
                if not creator_id:
                    issue_user = issue.get("user") or {}
                    creator_id = issue_user.get("id")
                    if creator_id:
                        issue_creators[issue_number] = creator_id
                        _ensure_actor(issue_user)
                
                if creator_id and creator_id != actor_id:
                    comment = payload.get("comment") or {}
                    comment_body = _sanitize_comment_text(comment.get("body", ""))
                    edges.append({
                        "source": actor_id, "target": creator_id,
                        "edge_type": "ISSUE_INTERACTION",
                        "event_id": event_id, "created_at": created_at,
                        "comment_body": comment_body,
                    })
        
        elif event_type == "PullRequestReviewCommentEvent":
            pr = payload.get("pull_request") or {}
            pr_number = pr.get("number")
            if pr_number:
                creator_id = pr_creators.get(pr_number)
                if not creator_id:
                    pr_user = pr.get("user") or {}
                    creator_id = pr_user.get("id")
                    if creator_id:
                        pr_creators[pr_number] = creator_id
                        _ensure_actor(pr_user)
                
                if creator_id and creator_id != actor_id:
                    comment = payload.get("comment") or {}
                    comment_body = _sanitize_comment_text(comment.get("body", ""))
                    edges.append({
                        "source": actor_id, "target": creator_id,
                        "edge_type": "PR_REVIEW",
                        "event_id": event_id, "created_at": created_at,
                        "comment_body": comment_body,
                    })
        
        elif event_type == "PullRequestEvent":
            action = payload.get("action")
            pr = payload.get("pull_request") or {}
            pr_number = pr.get("number")
            
            if action == "closed" and pr.get("merged") and pr_number:
                creator_id = pr_creators.get(pr_number)
                if not creator_id:
                    pr_user = pr.get("user") or {}
                    creator_id = pr_user.get("id")
                    if creator_id:
                        pr_creators[pr_number] = creator_id
                        _ensure_actor(pr_user)
                
                if creator_id and creator_id != actor_id:
                    edges.append({
                        "source": actor_id, "target": creator_id,
                        "edge_type": "PR_MERGE",
                        "event_id": event_id, "created_at": created_at,
                    })
    
    # æ·»åŠ å…±åŒå‚ä¸è€…è¾¹
    for issue_number, participants in issue_participants.items():
        participants_list = list(participants)
        for i in range(len(participants_list)):
            for j in range(i + 1, len(participants_list)):
                a1, a2 = participants_list[i], participants_list[j]
                edges.append({
                    "source": a1, "target": a2,
                    "edge_type": "ISSUE_CO_PARTICIPANT",
                    "event_id": f"co_{issue_number}_{a1}_{a2}",
                    "created_at": "",
                })
    
    # æ„å»ºå›¾
    actors_with_edges = set()
    for edge in edges:
        actors_with_edges.add(edge["source"])
        actors_with_edges.add(edge["target"])
    
    for actor_id in actors_with_edges:
        if actor_id in actors:
            graph.add_node(f"actor:{actor_id}", **actors[actor_id].to_dict())
    
    for edge_data in edges:
        source = f"actor:{edge_data['source']}"
        target = f"actor:{edge_data['target']}"
        edge_key = f"{edge_data['edge_type']}_{edge_data['event_id']}"
        graph.add_edge(source, target, key=edge_key,
                       edge_type=edge_data["edge_type"],
                       created_at=edge_data.get("created_at") or "",
                       comment_body=_sanitize_xml_text(edge_data.get("comment_body", "")))
    
    graph.graph["repo_name"] = _sanitize_xml_text(repo_name)
    graph.graph["month"] = _sanitize_xml_text(month)
    graph.graph["graph_type"] = "actor-actor"
    graph.graph["total_events"] = len(events)
    
    return graph


def build_actor_repo_graph(
    events: List[Dict],
    repo_name: str,
    month: str,
) -> nx.MultiDiGraph:
    """
    æ„å»º Actor-Repository äºŒéƒ¨å›¾
    
    è¾¹ç±»å‹ï¼šåŸºäºäº‹ä»¶ç±»å‹ï¼ˆPUSH, CREATE, ISSUE, PR, COMMENT ç­‰ï¼‰
    æ–°å¢ï¼šè¾¹åŒ…å«è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼ˆcommit_count, pr_merged ç­‰ï¼‰
    """
    graph = nx.MultiDiGraph()
    actors: Dict[int, ActorStats] = {}
    repos: Dict[int, RepoStats] = {}
    edges: List[Dict] = []
    
    for event in events:
        event_id = event.get("id")
        event_type = event.get("type") or ""
        created_at = event.get("created_at") or ""
        
        actor = event.get("actor") or {}
        actor_id = actor.get("id")
        repo = event.get("repo") or {}
        repo_id = repo.get("id")
        
        if actor_id is None or repo_id is None:
            continue
        
        # ç¡®ä¿ Actor å­˜åœ¨
        if actor_id not in actors:
            actors[actor_id] = ActorStats(
                actor_id=actor_id,
                login=_clean_text_for_xml(actor.get("login") or ""),
            )
        actors[actor_id].event_count += 1
        actors[actor_id].event_types[event_type] = \
            actors[actor_id].event_types.get(event_type, 0) + 1
        
        # ç¡®ä¿ Repo å­˜åœ¨
        if repo_id not in repos:
            repos[repo_id] = RepoStats(
                repo_id=repo_id,
                name=_clean_text_for_xml(repo.get("name") or ""),
            )
        repos[repo_id].event_count += 1
        repos[repo_id].event_types[event_type] = \
            repos[repo_id].event_types.get(event_type, 0) + 1
        
        # ç®€åŒ–è¾¹ç±»å‹
        edge_type_map = {
            "PushEvent": "PUSH",
            "CreateEvent": "CREATE",
            "PullRequestEvent": "PR",
            "IssuesEvent": "ISSUE",
            "IssueCommentEvent": "COMMENT",
            "PullRequestReviewCommentEvent": "REVIEW",
            "WatchEvent": "STAR",
            "ForkEvent": "FORK",
            "DeleteEvent": "DELETE",
            "ReleaseEvent": "RELEASE",
        }
        edge_type = edge_type_map.get(event_type, event_type)

        # åªæœ‰"è¯„è®ºç±»äº‹ä»¶"æ‰æœ‰æ­£æ–‡
        comment_body = ""
        if event_type in ("IssueCommentEvent", "PullRequestReviewCommentEvent"):
            comment = (event.get("payload") or {}).get("comment") or {}
            comment_body = _sanitize_comment_text(comment.get("body", "") or "")
        
        # ä» payload ä¸­æå–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        payload = event.get("payload") or {}
        commit_count = 0
        pr_merged = 0
        pr_opened = 0
        pr_closed = 0
        issue_opened = 0
        issue_closed = 0
        is_comment = 0
        
        if event_type == "PushEvent":
            commits = payload.get("commits") or []
            commit_count = len(commits)
        elif event_type == "PullRequestEvent":
            action = payload.get("action")
            pr = payload.get("pull_request") or {}
            if action == "opened":
                pr_opened = 1
            elif action == "closed":
                pr_closed = 1
                if pr.get("merged"):
                    pr_merged = 1
        elif event_type == "IssuesEvent":
            action = payload.get("action")
            if action == "opened":
                issue_opened = 1
            elif action == "closed":
                issue_closed = 1
        elif event_type in ("IssueCommentEvent", "PullRequestReviewCommentEvent"):
            is_comment = 1

        edges.append({
            "actor_id": actor_id,
            "repo_id": repo_id,
            "edge_type": edge_type,
            "event_id": event_id,
            "created_at": created_at,
            "comment_body": comment_body,
            # æ–°å¢ï¼šè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            "commit_count": commit_count,
            "pr_merged": pr_merged,
            "pr_opened": pr_opened,
            "pr_closed": pr_closed,
            "issue_opened": issue_opened,
            "issue_closed": issue_closed,
            "is_comment": is_comment,
        })
    
    # æ·»åŠ èŠ‚ç‚¹
    for actor_id, actor_stats in actors.items():
        graph.add_node(f"actor:{actor_id}", **actor_stats.to_dict())
    
    for repo_id, repo_stats in repos.items():
        graph.add_node(f"repo:{repo_id}", **repo_stats.to_dict())
    
    # æ·»åŠ è¾¹ï¼ˆæ¯æ¡äº‹ä»¶ä»ç„¶æ˜¯ç‹¬ç«‹çš„è¾¹ï¼Œä½†åŒ…å«ç»Ÿè®¡ä¿¡æ¯ï¼‰
    for edge_data in edges:
        source = f"actor:{edge_data['actor_id']}"
        target = f"repo:{edge_data['repo_id']}"
        edge_key = f"{edge_data['edge_type']}_{edge_data['event_id']}"
        graph.add_edge(
            source, 
            target, 
            key=edge_key,
            edge_type=edge_data["edge_type"],
            created_at=edge_data.get("created_at") or "",
            comment_body=_sanitize_xml_text(edge_data.get("comment_body", "")),
            # æ–°å¢ï¼šç»Ÿè®¡ä¿¡æ¯
            commit_count=edge_data.get("commit_count", 0),
            pr_merged=edge_data.get("pr_merged", 0),
            pr_opened=edge_data.get("pr_opened", 0),
            pr_closed=edge_data.get("pr_closed", 0),
            issue_opened=edge_data.get("issue_opened", 0),
            issue_closed=edge_data.get("issue_closed", 0),
            is_comment=edge_data.get("is_comment", 0),
        )
    
    graph.graph["repo_name"] = _sanitize_xml_text(repo_name)
    graph.graph["month"] = _sanitize_xml_text(month)
    graph.graph["graph_type"] = "actor-repo"
    graph.graph["total_events"] = len(events)
    
    return graph


def build_actor_discussion_graph(
    events: List[Dict],
    repo_name: str,
    month: str,
) -> nx.MultiDiGraph:
    """
    æ„å»º Actor-Discussion äºŒéƒ¨å›¾ï¼ˆIssue/PR è®¨è®ºå›¾ï¼‰
    
    èŠ‚ç‚¹ï¼šActor, Issue, PullRequest
    è¾¹ï¼šCREATED, COMMENTED, REVIEWED, CLOSED, MERGED
    """
    graph = nx.MultiDiGraph()
    actors: Dict[int, ActorStats] = {}
    discussions: Dict[str, DiscussionStats] = {}
    edges: List[Dict] = []
    
    def _ensure_actor(actor_data: Dict) -> Optional[int]:
        actor_id = actor_data.get("id")
        if actor_id is None:
            return None
        if actor_id not in actors:
            actors[actor_id] = ActorStats(
                actor_id=actor_id,
                login=_clean_text_for_xml(actor_data.get("login") or ""),
            )
        return actor_id
    
    def _update_actor(actor_id: int, event_type: str, created_at: str):
        if actor_id in actors:
            actors[actor_id].event_count += 1
            actors[actor_id].event_types[event_type] = \
                actors[actor_id].event_types.get(event_type, 0) + 1
    
    for event in events:
        event_id = event.get("id")
        event_type = event.get("type") or ""
        created_at = event.get("created_at") or ""
        
        actor = event.get("actor") or {}
        actor_id = _ensure_actor(actor)
        if actor_id is None:
            continue
        
        _update_actor(actor_id, event_type, created_at)
        
        repo = event.get("repo") or {}
        repo_id = repo.get("id")
        payload = event.get("payload") or {}
        
        if event_type == "IssuesEvent":
            action = payload.get("action")
            issue = payload.get("issue") or {}
            issue_number = issue.get("number")
            
            if issue_number and repo_id:
                key = f"issue:{repo_id}:{issue_number}"
                
                if key not in discussions:
                    issue_user = issue.get("user") or {}
                    discussions[key] = DiscussionStats(
                        key=key,
                        node_type="Issue",
                        number=issue_number,
                        title=_sanitize_xml_text(_clean_text_for_xml(issue.get("title") or "")),
                        state=issue.get("state") or "",
                        creator_id=issue_user.get("id"),
                        creator_login=_clean_text_for_xml(issue_user.get("login") or ""),
                        created_at=issue.get("created_at"),
                    )
                
                discussions[key].participants.add(actor_id)
                
                edge_type = "CREATED_ISSUE" if action == "opened" else "CLOSED_ISSUE"
                edges.append({
                    "actor_id": actor_id,
                    "discussion_key": key,
                    "edge_type": edge_type,
                    "event_id": event_id,
                    "created_at": created_at,
                })
        
        elif event_type == "IssueCommentEvent":
            issue = payload.get("issue") or {}
            issue_number = issue.get("number")
            
            if issue_number and repo_id:
                key = f"issue:{repo_id}:{issue_number}"
                
                if key not in discussions:
                    issue_user = issue.get("user") or {}
                    discussions[key] = DiscussionStats(
                        key=key,
                        node_type="Issue",
                        number=issue_number,
                        title=_sanitize_xml_text(_clean_text_for_xml(issue.get("title") or "")),
                        state=issue.get("state") or "",
                        creator_id=issue_user.get("id"),
                        creator_login=_clean_text_for_xml(issue_user.get("login") or ""),
                        created_at=issue.get("created_at"),
                    )
                
                discussions[key].comment_count += 1
                discussions[key].participants.add(actor_id)
                
                # ç¡®ä¿åˆ›å»ºè€…ä¹Ÿæ˜¯ actor
                issue_user = issue.get("user") or {}
                if issue_user.get("id"):
                    _ensure_actor(issue_user)
                
                # è·å–è¯„è®ºæ­£æ–‡ï¼ˆç›´æ¥ä»äº‹ä»¶æ•°æ®ä¸­æå–ï¼‰
                comment = payload.get("comment") or {}
                comment_body = _sanitize_comment_text(comment.get("body", ""))

                edges.append({
                    "actor_id": actor_id,
                    "discussion_key": key,
                    "edge_type": "COMMENTED_ISSUE",
                    "event_id": event_id,
                    "created_at": created_at,
                    "comment_body": comment_body,
                })
        
        elif event_type == "PullRequestEvent":
            action = payload.get("action")
            pr = payload.get("pull_request") or {}
            pr_number = pr.get("number")
            
            if pr_number and repo_id:
                key = f"pr:{repo_id}:{pr_number}"
                
                if key not in discussions:
                    pr_user = pr.get("user") or {}
                    discussions[key] = DiscussionStats(
                        key=key,
                        node_type="PullRequest",
                        number=pr_number,
                        title=_sanitize_xml_text(_clean_text_for_xml(pr.get("title") or "")),
                        state=pr.get("state") or "",
                        creator_id=pr_user.get("id"),
                        creator_login=_clean_text_for_xml(pr_user.get("login") or ""),
                        created_at=pr.get("created_at"),
                    )
                
                discussions[key].participants.add(actor_id)
                
                if action == "opened":
                    edge_type = "CREATED_PR"
                elif action == "closed" and pr.get("merged"):
                    edge_type = "MERGED_PR"
                elif action == "closed":
                    edge_type = "CLOSED_PR"
                else:
                    edge_type = "PR_ACTION"
                
                edges.append({
                    "actor_id": actor_id,
                    "discussion_key": key,
                    "edge_type": edge_type,
                    "event_id": event_id,
                    "created_at": created_at,
                })
        
        elif event_type == "PullRequestReviewCommentEvent":
            pr = payload.get("pull_request") or {}
            pr_number = pr.get("number")
            
            if pr_number and repo_id:
                key = f"pr:{repo_id}:{pr_number}"
                
                if key not in discussions:
                    pr_user = pr.get("user") or {}
                    discussions[key] = DiscussionStats(
                        key=key,
                        node_type="PullRequest",
                        number=pr_number,
                        title=_sanitize_xml_text(_clean_text_for_xml(pr.get("title") or "")),
                        state=pr.get("state") or "",
                        creator_id=pr_user.get("id"),
                        creator_login=_clean_text_for_xml(pr_user.get("login") or ""),
                        created_at=pr.get("created_at"),
                    )
                
                discussions[key].comment_count += 1
                discussions[key].participants.add(actor_id)
                
                # ç¡®ä¿åˆ›å»ºè€…ä¹Ÿæ˜¯ actor
                pr_user = pr.get("user") or {}
                if pr_user.get("id"):
                    _ensure_actor(pr_user)
                
                # è·å–è¯„è®ºæ­£æ–‡ï¼ˆç›´æ¥ä»äº‹ä»¶æ•°æ®ä¸­æå–ï¼‰
                comment = payload.get("comment") or {}
                comment_body = _sanitize_comment_text(comment.get("body", ""))
                edges.append({
                    "actor_id": actor_id,
                    "discussion_key": key,
                    "edge_type": "REVIEWED_PR",
                    "event_id": event_id,
                    "created_at": created_at,
                    "comment_body": comment_body,
                })
    
    # æ·»åŠ èŠ‚ç‚¹
    for actor_id, actor_stats in actors.items():
        graph.add_node(f"actor:{actor_id}", **actor_stats.to_dict())
    
    for key, disc_stats in discussions.items():
        graph.add_node(key, **disc_stats.to_dict())
    
    # æ·»åŠ è¾¹
    for edge_data in edges:
        source = f"actor:{edge_data['actor_id']}"
        target = edge_data["discussion_key"]
        edge_key = f"{edge_data['edge_type']}_{edge_data['event_id']}"
        graph.add_edge(source, target, key=edge_key,
                       edge_type=edge_data["edge_type"],
                       created_at=edge_data.get("created_at") or "",
                       comment_body=_sanitize_xml_text(edge_data.get("comment_body", "")))
    
    graph.graph["repo_name"] = _sanitize_xml_text(repo_name)
    graph.graph["month"] = _sanitize_xml_text(month)
    graph.graph["graph_type"] = "actor-discussion"
    graph.graph["total_events"] = len(events)
    
    return graph


# ==================== ä¸»æµç¨‹ ====================

def _filter_months(
    monthly_repo_data: Dict[str, Dict[str, List[Dict]]],
    start_month: Optional[str] = None,
    end_month: Optional[str] = None,
) -> Dict[str, Dict[str, List[Dict]]]:
    """æŒ‰æœˆä»½èŒƒå›´è¿‡æ»¤"""
    if not start_month and not end_month:
        return monthly_repo_data
    filtered = {}
    for month, repos in monthly_repo_data.items():
        if start_month and month < start_month:
            continue
        if end_month and month > end_month:
            continue
        filtered[month] = repos
    return filtered


def build_monthly_graphs(
    data_dir: str = "data/filtered/",
    output_dir: str = "output/monthly-graphs/",
    graph_types: List[str] = None,
    start_month: Optional[str] = None,
    end_month: Optional[str] = None,
    merge_index: bool = True,
) -> Dict[str, Dict[str, Dict[str, str]]]:
    """
    æ„å»ºæ‰€æœ‰é¡¹ç›®çš„æœˆåº¦å›¾ï¼ˆä¸‰ç±»ï¼‰
    
    Args:
        data_dir: è¾“å…¥æ•°æ®ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        graph_types: è¦æ„å»ºçš„å›¾ç±»å‹ï¼Œé»˜è®¤å…¨éƒ¨ ["actor-actor", "actor-repo", "actor-discussion"]
        start_month: åªæ„å»ºè¯¥æœˆåŠä¹‹åçš„å›¾ (YYYY-MM)
        end_month: åªæ„å»ºè¯¥æœˆåŠä¹‹å‰çš„å›¾ (YYYY-MM)
        merge_index: è‹¥è¾“å‡ºç›®å½•å·²æœ‰ index.jsonï¼Œæ˜¯å¦åˆå¹¶è€Œéè¦†ç›–
    
    Returns:
        {repo_name: {graph_type: {month: graph_path}}}
    """
    if graph_types is None:
        graph_types = ["actor-actor", "actor-repo", "actor-discussion"]
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # å›¾æ„å»ºå‡½æ•°æ˜ å°„
    builder_map = {
        "actor-actor": build_actor_actor_graph,
        "actor-repo": build_actor_repo_graph,
        "actor-discussion": build_actor_discussion_graph,
    }
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½è¿‡æ»¤åçš„æ•°æ®...")
    daily_data = load_filtered_data(data_dir)
    
    # æŒ‰æœˆå’Œé¡¹ç›®åˆ†ç»„
    print("æŒ‰æœˆå’Œé¡¹ç›®åˆ†ç»„...")
    monthly_repo_data = group_by_month_and_repo(daily_data)
    
    # æŒ‰æœˆä»½èŒƒå›´è¿‡æ»¤
    if start_month or end_month:
        monthly_repo_data = _filter_months(monthly_repo_data, start_month, end_month)
        print(f"æœˆä»½è¿‡æ»¤: {start_month or 'ä¸é™'} ~ {end_month or 'ä¸é™'}ï¼Œå…± {len(monthly_repo_data)} ä¸ªæœˆ")
    
    # ç»Ÿè®¡
    total_combos = sum(len(repos) for repos in monthly_repo_data.values())
    total_graphs = total_combos * len(graph_types)
    print(f"å°†æ„å»ºçº¦ {total_graphs} ä¸ªå›¾ï¼ˆ{len(monthly_repo_data)} ä¸ªæœˆ Ã— å¤šä¸ªé¡¹ç›® Ã— {len(graph_types)} ç§å›¾ï¼‰")
    
    result = defaultdict(lambda: defaultdict(dict))
    graph_count = 0
    skipped_count = 0
    error_count = 0
    
    # è®¡ç®—æ€»ä»»åŠ¡æ•°
    months_list = sorted(monthly_repo_data.keys())
    total_months = len(months_list)
    
    for month_idx, month in enumerate(months_list, 1):
        repos = monthly_repo_data[month]
        repos_in_month = len(repos)
        
        print(f"")
        print(f"{'='*60}")
        print(f"å¤„ç†æœˆä»½: {month} ({month_idx}/{total_months})")
        print(f"è¯¥æœˆä»½åŒ…å« {repos_in_month} ä¸ªé¡¹ç›®")
        print(f"{'='*60}")
        
        for repo_idx, (repo_name, events) in enumerate(repos.items(), 1):
            if len(events) < 3:  # è·³è¿‡äº‹ä»¶å¤ªå°‘çš„
                skipped_count += 1
                continue
            
            safe_repo_name = repo_name.replace("/", "-")
            repo_dir = output_path / safe_repo_name
            
            graphs_built_for_repo = []
            
            for graph_type in graph_types:
                builder = builder_map.get(graph_type)
                if not builder:
                    continue
                
                # åˆ›å»ºå›¾ç±»å‹ç›®å½•
                type_dir = repo_dir / graph_type
                type_dir.mkdir(parents=True, exist_ok=True)
                
                # æ„å»ºå›¾
                try:
                    import sys
                    sys.stdout.write(f"\r    æ„å»º {graph_type}...")
                    sys.stdout.flush()
                    graph = builder(events, repo_name, month)
                except Exception as e:
                    print(f"\n    è­¦å‘Š: æ„å»ºå›¾å¤±è´¥ {repo_name}/{graph_type}/{month}: {e}")
                    error_count += 1
                    continue
                
                # è·³è¿‡ç©ºå›¾
                if graph.number_of_nodes() < 2:
                    skipped_count += 1
                    continue
                
                # ä¿å­˜
                graph_file = type_dir / f"{month}.graphml"
                try:
                    sanitize_graphml_attributes(graph)
                    nx.write_graphml(graph, str(graph_file))

                    result[repo_name][graph_type][month] = str(graph_file)
                    graph_count += 1
                    graphs_built_for_repo.append(graph_type)

                except Exception as e:
                    print(f"\n    è­¦å‘Š: ä¿å­˜å›¾å¤±è´¥ {graph_file}: {e}")
                    error_count += 1
                    continue

            
            # æ¯å¤„ç†ä¸€ä¸ªé¡¹ç›®è¾“å‡ºè¿›åº¦
            if graphs_built_for_repo:
                # æ¸…é™¤å½“å‰è¡Œå¹¶æ‰“å°å®Œæˆä¿¡æ¯
                print(
                    f"\r  [{repo_idx}/{repos_in_month}] {repo_name}: "
                    f"{len(events)} äº‹ä»¶ â†’ {', '.join(graphs_built_for_repo)}          "
                )
        
        # æ¯ä¸ªæœˆç»“æŸæ—¶è¾“å‡ºç»Ÿè®¡
        print(f"æœˆä»½ {month} å¤„ç†å®Œæˆï¼Œç´¯è®¡: {graph_count} ä¸ªå›¾")
    
    print("")
    print("=" * 60)
    print("æ„å»ºå®Œæˆç»Ÿè®¡:")
    print(f"  æˆåŠŸæ„å»º: {graph_count} ä¸ªå›¾")
    print(f"  è·³è¿‡: {skipped_count} ä¸ªï¼ˆäº‹ä»¶å¤ªå°‘æˆ–ç©ºå›¾ï¼‰")
    print(f"  é”™è¯¯: {error_count} ä¸ª")
    print(f"  æ¶‰åŠé¡¹ç›®: {len(result)} ä¸ª")
    print("=" * 60)
    
    # ä¿å­˜ç´¢å¼•ï¼ˆè‹¥å·²æœ‰åˆ™åˆå¹¶ï¼‰
    index_file = output_path / "index.json"
    to_save = {k: dict(v) for k, v in result.items()}
    if merge_index and index_file.exists():
        with open(index_file, "r", encoding="utf-8") as f:
            existing = json.load(f)
        for repo, graph_types_dict in to_save.items():
            if repo not in existing:
                existing[repo] = {}
            for gt, months in graph_types_dict.items():
                existing[repo].setdefault(gt, {}).update(months)
        to_save = existing
        print(f"å·²åˆå¹¶åˆ°ç°æœ‰ç´¢å¼•")
    with open(index_file, "w", encoding="utf-8") as f:
        json.dump(to_save, f, indent=2, ensure_ascii=False)
    
    print(f"ç´¢å¼•å·²ä¿å­˜: {index_file}")
    
    return dict(result)


def _process_single_repo(args_tuple):
    """å¤„ç†å•ä¸ªé¡¹ç›®ï¼ˆç”¨äºå¹¶è¡Œï¼‰"""
    repo_name, events, month, graph_types, output_path = args_tuple
    
    builder_map = {
        "actor-actor": build_actor_actor_graph,
        "actor-repo": build_actor_repo_graph,
        "actor-discussion": build_actor_discussion_graph,
    }
    
    safe_repo_name = repo_name.replace("/", "-")
    repo_dir = output_path / safe_repo_name
    
    results = []
    
    for graph_type in graph_types:
        builder = builder_map.get(graph_type)
        if not builder:
            continue
        
        type_dir = repo_dir / graph_type
        type_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            graph = builder(events, repo_name, month)
        except Exception as e:
            continue
        
        if graph.number_of_nodes() < 2:
            continue
        
        graph_file = type_dir / f"{month}.graphml"
        try:
            sanitize_graphml_attributes(graph)
            nx.write_graphml(graph, str(graph_file))
            results.append((repo_name, graph_type, month, str(graph_file)))
        except:
            continue

    
    return results


def build_monthly_graphs_parallel(
    data_dir: str = "data/filtered/",
    output_dir: str = "output/monthly-graphs/",
    graph_types: List[str] = None,
    workers: int = 4,
    start_month: Optional[str] = None,
    end_month: Optional[str] = None,
    merge_index: bool = True,
) -> Dict[str, Dict[str, Dict[str, str]]]:
    """
    å¹¶è¡Œæ„å»ºæ‰€æœ‰é¡¹ç›®çš„æœˆåº¦å›¾
    """
    from concurrent.futures import ProcessPoolExecutor, as_completed
    
    if graph_types is None:
        graph_types = ["actor-actor", "actor-repo", "actor-discussion"]
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("åŠ è½½è¿‡æ»¤åçš„æ•°æ®...")
    daily_data = load_filtered_data(data_dir)
    
    print("æŒ‰æœˆå’Œé¡¹ç›®åˆ†ç»„...")
    monthly_repo_data = group_by_month_and_repo(daily_data)
    
    # æŒ‰æœˆä»½èŒƒå›´è¿‡æ»¤
    if start_month or end_month:
        monthly_repo_data = _filter_months(monthly_repo_data, start_month, end_month)
        print(f"æœˆä»½è¿‡æ»¤: {start_month or 'ä¸é™'} ~ {end_month or 'ä¸é™'}ï¼Œå…± {len(monthly_repo_data)} ä¸ªæœˆ")
    
    # å‡†å¤‡æ‰€æœ‰ä»»åŠ¡
    tasks = []
    for month in sorted(monthly_repo_data.keys()):
        repos = monthly_repo_data[month]
        for repo_name, events in repos.items():
            if len(events) >= 3:
                tasks.append((repo_name, events, month, graph_types, output_path))
    
    total_tasks = len(tasks)
    print(f"å…± {total_tasks} ä¸ªä»»åŠ¡ï¼Œä½¿ç”¨ {workers} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†")
    
    result = defaultdict(lambda: defaultdict(dict))
    completed = 0
    graph_count = 0
    
    with ProcessPoolExecutor(max_workers=workers) as executor:
        futures = {executor.submit(_process_single_repo, task): task for task in tasks}
        
        for future in as_completed(futures):
            completed += 1
            try:
                results = future.result()
                for repo_name, graph_type, month, path in results:
                    result[repo_name][graph_type][month] = path
                    graph_count += 1
            except Exception as e:
                pass
            
            if completed % 20 == 0 or completed == total_tasks:
                print(f"è¿›åº¦: {completed}/{total_tasks} ({completed*100//total_tasks}%), å·²ç”Ÿæˆ {graph_count} ä¸ªå›¾")
    
    print("")
    print("=" * 60)
    print("æ„å»ºå®Œæˆç»Ÿè®¡:")
    print(f"  æˆåŠŸæ„å»º: {graph_count} ä¸ªå›¾")
    print(f"  æ¶‰åŠé¡¹ç›®: {len(result)} ä¸ª")
    print("=" * 60)
    
    # ä¿å­˜ç´¢å¼•ï¼ˆè‹¥å·²æœ‰åˆ™åˆå¹¶ï¼‰
    index_file = output_path / "index.json"
    to_save = {k: dict(v) for k, v in result.items()}
    if merge_index and index_file.exists():
        with open(index_file, "r", encoding="utf-8") as f:
            existing = json.load(f)
        for repo, graph_types_dict in to_save.items():
            if repo not in existing:
                existing[repo] = {}
            for gt, months in graph_types_dict.items():
                existing[repo].setdefault(gt, {}).update(months)
        to_save = existing
        print(f"å·²åˆå¹¶åˆ°ç°æœ‰ç´¢å¼•")
    with open(index_file, "w", encoding="utf-8") as f:
        json.dump(to_save, f, indent=2, ensure_ascii=False)
    
    print(f"ç´¢å¼•å·²ä¿å­˜: {index_file}")
    
    return dict(result)


if __name__ == "__main__":
    import argparse
    import logging
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[logging.StreamHandler()]
    )
    
    parser = argparse.ArgumentParser(description="æŒ‰æœˆæ„å»ºé¡¹ç›®å›¾ï¼ˆä¸‰ç±»æ—¶åºå›¾ï¼‰")
    parser.add_argument("--data-dir", type=str, default="data/filtered/", help="è¾“å…¥æ•°æ®ç›®å½•")
    parser.add_argument("--output-dir", type=str, default="output/monthly-graphs/", help="è¾“å‡ºç›®å½•")
    parser.add_argument(
        "--graph-types",
        type=str,
        default="actor-actor,actor-repo,actor-discussion",
        help="è¦æ„å»ºçš„å›¾ç±»å‹ï¼Œé€—å·åˆ†éš”ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰"
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=4,
        help="å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼‰"
    )
    parser.add_argument(
        "--serial",
        action="store_true",
        help="ä½¿ç”¨ä¸²è¡Œæ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨å¹¶è¡Œï¼‰"
    )
    parser.add_argument(
        "--start-month",
        type=str,
        default=None,
        help="åªæ„å»ºè¯¥æœˆåŠä¹‹åçš„å›¾ (YYYY-MMï¼Œå¦‚ 2020-01)"
    )
    parser.add_argument(
        "--end-month",
        type=str,
        default=None,
        help="åªæ„å»ºè¯¥æœˆåŠä¹‹å‰çš„å›¾ (YYYY-MMï¼Œå¦‚ 2020-12)"
    )
    parser.add_argument(
        "--no-merge-index",
        action="store_true",
        help="ä¸åˆå¹¶åˆ°ç°æœ‰ç´¢å¼•ï¼Œç›´æ¥è¦†ç›–ï¼ˆé»˜è®¤ä¼šåˆå¹¶ï¼‰"
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("å¼€å§‹æ„å»ºæœˆåº¦æ—¶åºå›¾")
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"å›¾ç±»å‹: {args.graph_types}")
    print(f"æœˆä»½èŒƒå›´: {args.start_month or 'ä¸é™'} ~ {args.end_month or 'ä¸é™'}")
    print(f"æ¨¡å¼: {'ä¸²è¡Œ' if args.serial else f'å¹¶è¡Œ ({args.workers} è¿›ç¨‹)'}")
    print("=" * 60)
    
    graph_types = [t.strip() for t in args.graph_types.split(",")]
    merge_index = not args.no_merge_index
    
    if args.serial:
        build_monthly_graphs(
            data_dir=args.data_dir,
            output_dir=args.output_dir,
            graph_types=graph_types,
            start_month=args.start_month,
            end_month=args.end_month,
            merge_index=merge_index,
        )
    else:
        build_monthly_graphs_parallel(
            data_dir=args.data_dir,
            output_dir=args.output_dir,
            graph_types=graph_types,
            workers=args.workers,
            start_month=args.start_month,
            end_month=args.end_month,
            merge_index=merge_index,
        )