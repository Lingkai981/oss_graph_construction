"""
äººå‘˜æµåŠ¨åˆ†ææ¨¡å—

åŸºäºå€¦æ€ åˆ†æç»“æœï¼Œç ”ç©¶å„ repo çš„äººå‘˜æµåŠ¨æƒ…å†µï¼Œé‡ç‚¹å…³æ³¨æ ¸å¿ƒæˆå‘˜ï¼š

1. æ ¸å¿ƒæˆå‘˜æ—¶é—´çº¿ï¼šé¦–æ¬¡/æœ«æ¬¡å‡ºç°ã€ä»»æœŸã€æ´»è·ƒæœˆä»½
2. æµå…¥/æµå‡ºäº‹ä»¶ï¼šè°ä½•æ—¶æˆä¸ºæ ¸å¿ƒã€è°ä½•æ—¶ç¦»å¼€
3. ç•™å­˜ç‡ï¼šN ä¸ªæœˆæ ¸å¿ƒæˆå‘˜ç•™å­˜æ›²çº¿
4. æµåŠ¨ç‡ï¼šæŒ‰æœˆ/æŒ‰å­£çš„æµå…¥æµå‡ºç»Ÿè®¡
5. å…³é”®æµå¤±ï¼šé•¿æœŸæ ¸å¿ƒæˆå‘˜ç¦»èŒè¯†åˆ«
6. è·¨ repo æµå‘ï¼šç¦»å¼€ååœ¨å“ªäº›å…¶ä»–é¡¹ç›®ä¸­æˆä¸ºæ ¸å¿ƒ

æ¦‚å¿µè¯´æ˜ï¼š
- ã€Œç¦»å¼€ã€= æŸæœˆä¸å†å¤„äºè¯¥ repo çš„æ ¸å¿ƒæˆå‘˜åå•ï¼ˆè´¡çŒ®è·Œå‡ºå‰çº¦ 50%ï¼‰ã€‚
  ä¸è¡¨ç¤ºå®Œå…¨ä¸å‚ä¸ï¼Œå¯èƒ½æ˜¯å‚ä¸å‡å°‘ã€å®Œå…¨é€€å‡ºæˆ–è§’è‰²å˜åŒ–ã€‚
- æ ¸å¿ƒæˆå‘˜ï¼šç”±åŠ æƒè´¡çŒ®é‡ï¼ˆPR åˆå¹¶/è¯„å®¡ã€Issue äº’åŠ¨ç­‰ï¼‰+ ç½‘ç»œä½ç½®ï¼ˆk-coreï¼‰åŠ¨æ€è®¡ç®—ã€‚
"""

from __future__ import annotations

import json
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import networkx as nx

from src.utils.logger import get_logger

logger = get_logger()


@dataclass
class CoreMemberRecord:
    """å•ä¸ªæœˆçš„æ ¸å¿ƒæˆå‘˜è®°å½•"""
    login: str
    degree: int
    rank: int  # 1-based æ’å


@dataclass
class MemberTimeline:
    """æ ¸å¿ƒæˆå‘˜æ—¶é—´çº¿"""
    login: str
    first_month: str
    last_month: str
    tenure_months: int
    active_months: List[str]
    rank_history: List[Tuple[str, int, int]]  # (month, degree, rank)
    avg_rank: float = 0.0


@dataclass
class JoinEvent:
    """æµå…¥äº‹ä»¶ï¼šæŸäººæˆä¸ºæ ¸å¿ƒæˆå‘˜"""
    month: str
    login: str
    degree: int
    rank: int
    repo_name: str


@dataclass
class LeaveEvent:
    """æµå‡ºäº‹ä»¶ï¼šæŸäººä¸å†ä¸ºæ ¸å¿ƒæˆå‘˜"""
    month: str
    login: str
    tenure_months: int
    was_top_n: bool  # æ˜¯å¦ä¸ºå‰ N å
    repo_name: str


class PersonnelFlowAnalyzer:
    """äººå‘˜æµåŠ¨åˆ†æå™¨"""

    def __init__(
        self,
        input_path: str = "output/burnout-analysis/full_analysis.json",
        output_dir: str = "output/personnel-flow/",
        scope: str = "core",
        graphs_dir: Optional[str] = None,
    ):
        self.input_path = Path(input_path)
        self.output_dir = Path(output_dir)
        self.scope = scope  # "core" | "all"
        self.graphs_dir = Path(graphs_dir) if graphs_dir else None
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _scope_label(self) -> str:
        return "å…¨éƒ¨è´¡çŒ®è€…" if self.scope == "all" else "æ ¸å¿ƒæˆå‘˜"

    def _load_graph(self, graph_path: str) -> Optional[nx.MultiDiGraph]:
        """åŠ è½½å›¾ï¼ˆå…¼å®¹ Windows è·¯å¾„ï¼‰"""
        try:
            normalized_path = Path(str(graph_path).replace("\\", "/"))
            g = nx.read_graphml(normalized_path)
            if isinstance(g, nx.MultiDiGraph):
                return g
            if isinstance(g, nx.DiGraph):
                mg = nx.MultiDiGraph()
                mg.add_nodes_from(g.nodes(data=True))
                mg.add_edges_from(g.edges(data=True))
                mg.graph.update(g.graph)
                return mg
            if isinstance(g, (nx.Graph, nx.MultiGraph)):
                dg = g.to_directed()
                mg = nx.MultiDiGraph()
                mg.add_nodes_from(dg.nodes(data=True))
                mg.add_edges_from(dg.edges(data=True))
                mg.graph.update(dg.graph)
                return mg
            return None
        except Exception as e:
            logger.warning(f"åŠ è½½å›¾å¤±è´¥: {graph_path}, é”™è¯¯: {e}")
            return None

    def _load_burnout_data(self) -> Dict[str, Any]:
        """åŠ è½½å€¦æ€ åˆ†ææ•°æ®"""
        if not self.input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {self.input_path}")
        with open(self.input_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _build_all_actors_data_from_graphs(
        self,
        repo_names: List[str],
    ) -> Dict[str, Any]:
        """ä»æœˆåº¦å›¾åŠ è½½æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ„å»ºä¸ burnout æ ¼å¼å…¼å®¹çš„æ•°æ®ï¼ˆcore_actors = å…¨éƒ¨è´¡çŒ®è€…ï¼‰"""
        if not self.graphs_dir or not self.graphs_dir.exists():
            raise FileNotFoundError(f"å›¾ç›®å½•ä¸å­˜åœ¨: {self.graphs_dir}ï¼Œè¯·æŒ‡å®š --graphs-dir")
        index_file = self.graphs_dir / "index.json"
        if not index_file.exists():
            raise FileNotFoundError(f"ç´¢å¼•ä¸å­˜åœ¨: {index_file}")
        with open(index_file, "r", encoding="utf-8") as f:
            index = json.load(f)

        result = {}
        for repo_name in repo_names:
            graph_types = index.get(repo_name, {})
            first_val = next(iter(graph_types.values()), {})
            if isinstance(first_val, dict) and not first_val.get("node_type"):
                months_data = graph_types.get("actor-actor", {})
            else:
                months_data = graph_types
            if not months_data or not isinstance(months_data, dict):
                continue
            metrics_series = []
            for month, graph_path in sorted(months_data.items()):
                graph = self._load_graph(graph_path)
                if graph is None:
                    continue
                degrees = dict(graph.degree())
                # æŒ‰åº¦æ•°é™åºï¼Œæ„å»º (login, degree) åˆ—è¡¨
                actors = []
                for node_id in graph.nodes():
                    login = graph.nodes[node_id].get("login", str(node_id))
                    degree = degrees.get(node_id, 0)
                    actors.append((login, degree))
                actors.sort(key=lambda x: -x[1])
                total_events = graph.graph.get("total_events", 0)
                metrics_series.append({
                    "month": month,
                    "repo_name": repo_name,
                    "unique_actors": len(actors),
                    "total_events": total_events,
                    "core_actors": actors,
                })
            if metrics_series:
                result[repo_name] = {"metrics": metrics_series}
        return result

    def _extract_core_per_month(
        self,
        metrics_series: List[Dict],
    ) -> Dict[str, List[CoreMemberRecord]]:
        """ä» metrics åºåˆ—æå–æ¯æœˆæ ¸å¿ƒæˆå‘˜åˆ—è¡¨"""
        core_by_month: Dict[str, List[CoreMemberRecord]] = {}
        sorted_metrics = sorted(metrics_series, key=lambda m: m.get("month", ""))

        for m in sorted_metrics:
            month = m.get("month", "")
            core_actors = m.get("core_actors", [])
            records = []
            for rank, item in enumerate(core_actors, 1):
                if isinstance(item, (list, tuple)):
                    login = item[0] if len(item) > 0 else ""
                    degree = item[1] if len(item) > 1 else 0
                else:
                    login = str(item)
                    degree = 0
                if login:
                    records.append(CoreMemberRecord(login=login, degree=degree, rank=rank))
            core_by_month[month] = records
        return core_by_month

    def _build_member_timelines(
        self,
        core_by_month: Dict[str, List[CoreMemberRecord]],
    ) -> Dict[str, MemberTimeline]:
        """æ„å»ºæ¯ä¸ªæ ¸å¿ƒæˆå‘˜çš„æ—¶é—´çº¿"""
        # login -> {months: [(degree, rank), ...]}
        member_months: Dict[str, Dict[str, Tuple[int, int]]] = defaultdict(dict)

        for month, records in core_by_month.items():
            for r in records:
                member_months[r.login][month] = (r.degree, r.rank)

        timelines = {}
        for login, months_data in member_months.items():
            active_months = sorted(months_data.keys())
            first_month = active_months[0]
            last_month = active_months[-1]
            tenure = len(active_months)
            rank_history = [
                (m, months_data[m][0], months_data[m][1])
                for m in active_months
            ]
            avg_rank = sum(r[2] for r in rank_history) / len(rank_history) if rank_history else 0

            timelines[login] = MemberTimeline(
                login=login,
                first_month=first_month,
                last_month=last_month,
                tenure_months=tenure,
                active_months=active_months,
                rank_history=rank_history,
                avg_rank=round(avg_rank, 2),
            )
        return timelines

    def _detect_join_leave_events(
        self,
        core_by_month: Dict[str, List[CoreMemberRecord]],
        repo_name: str,
        timelines: Dict[str, MemberTimeline],
    ) -> Tuple[List[Dict], List[Dict]]:
        """æ£€æµ‹æµå…¥æµå‡ºäº‹ä»¶"""
        months = sorted(core_by_month.keys())
        join_events = []
        leave_events = []

        prev_core = set()

        for month in months:
            curr_core_logins = {r.login for r in core_by_month[month]}
            curr_core_map = {r.login: (r.degree, r.rank) for r in core_by_month[month]}

            # æµå…¥ï¼šæœ¬æœˆåœ¨ã€ä¸Šæœˆä¸åœ¨
            for login in curr_core_logins - prev_core:
                degree, rank = curr_core_map[login]
                join_events.append({
                    "month": month,
                    "login": login,
                    "degree": degree,
                    "rank": rank,
                    "repo_name": repo_name,
                })

            # æµå‡ºï¼šä¸Šæœˆåœ¨ã€æœ¬æœˆä¸åœ¨ï¼ˆtenure ä» timeline è¡¥å…¨ï¼‰
            for login in prev_core - curr_core_logins:
                t = timelines.get(login)
                tenure = t.tenure_months if t else 0
                was_top_3 = any(
                    r.login == login and r.rank <= 3
                    for recs in core_by_month.values()
                    for r in recs
                )
                leave_events.append({
                    "month": month,
                    "login": login,
                    "tenure_months": tenure,
                    "was_top_n": was_top_3,
                    "repo_name": repo_name,
                })

            prev_core = curr_core_logins

        return join_events, leave_events

    def _compute_retention_rates(
        self,
        timelines: Dict[str, MemberTimeline],
        total_months: int,
    ) -> Dict[int, float]:
        """è®¡ç®— N ä¸ªæœˆç•™å­˜ç‡ï¼šä»»æœŸ >= N æœˆçš„æ ¸å¿ƒæˆå‘˜å æ¯”"""
        if total_months < 2 or not timelines:
            return {}
        retention = {}
        for n in range(1, min(total_months, 13)):  # æœ€å¤š 12 ä¸ªæœˆ
            retained = sum(1 for t in timelines.values() if t.tenure_months >= n)
            retention[n] = round(retained / len(timelines), 4)
        return retention

    def _compute_period_churn(
        self,
        core_by_month: Dict[str, List[CoreMemberRecord]],
    ) -> List[Dict]:
        """è®¡ç®—å„æœŸæµåŠ¨ç»Ÿè®¡"""
        months = sorted(core_by_month.keys())
        result = []
        prev_core = set()

        for month in months:
            curr_core = {r.login for r in core_by_month[month]}
            joined = len(curr_core - prev_core)
            left = len(prev_core - curr_core)
            result.append({
                "month": month,
                "core_count": len(curr_core),
                "joined": joined,
                "left": left,
                "net_change": len(curr_core) - len(prev_core),
            })
            prev_core = curr_core
        return result

    def _identify_critical_departures(
        self,
        leave_events: List[Dict],
        timelines: Dict[str, MemberTimeline],
        min_tenure: int = 6,
    ) -> List[Dict]:
        """è¯†åˆ«å…³é”®æµå¤±ï¼šä»»æœŸè¾ƒé•¿çš„æ ¸å¿ƒæˆå‘˜ç¦»å¼€"""
        critical = []
        for evt in leave_events:
            login = evt["login"]
            timeline = timelines.get(login)
            if timeline and timeline.tenure_months >= min_tenure:
                critical.append({
                    **evt,
                    "tenure_months": timeline.tenure_months,
                    "avg_rank": timeline.avg_rank,
                })
        return critical

    def _build_global_core_index(
        self,
        burnout_data: Dict[str, Any],
    ) -> Dict[str, List[Dict[str, Any]]]:
        """æ„å»ºå…¨å±€æ ¸å¿ƒæˆå‘˜ç´¢å¼•ï¼šlogin -> [(repo, month, rank, degree), ...]"""
        index: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        for repo_name, repo_data in burnout_data.items():
            for m in repo_data.get("metrics", []):
                month = m.get("month", "")
                for rank, item in enumerate(m.get("core_actors", []), 1):
                    if isinstance(item, (list, tuple)):
                        login = (item[0] or "").strip()
                        degree = item[1] if len(item) > 1 else 0
                    else:
                        login = str(item).strip()
                        degree = 0
                    if login:
                        index[login].append({
                            "repo": repo_name,
                            "month": month,
                            "rank": rank,
                            "degree": degree,
                        })
        for k in index:
            index[k].sort(key=lambda x: (x["month"], x["repo"]))
        return dict(index)

    def _find_flow_destinations(
        self,
        login: str,
        from_repo: str,
        leave_month: str,
        global_index: Dict[str, List[Dict[str, Any]]],
        months_after: int = 12,
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾æŸäººç¦»å¼€æŸ repo åæµå‘çš„å…¶ä»–é¡¹ç›®ã€‚
        åœ¨ç¦»å¼€å½“æœˆåŠä¹‹å N ä¸ªæœˆå†…ï¼Œåœ¨å…¶å®ƒ repo ä¸­é¦–æ¬¡æˆä¸ºæ ¸å¿ƒçš„è§†ä¸ºæµå‘ã€‚
        """
        appearances = global_index.get(login, [])
        flow_to = []
        seen_repos = set()

        for a in appearances:
            repo = a["repo"]
            month = a["month"]
            if repo == from_repo:
                continue
            if repo in seen_repos:
                continue
            if month < leave_month:
                continue
            # ç®€å•æœˆæ•°å·®ï¼šå‡è®¾ YYYY-MM å¯ç›´æ¥æ¯”è¾ƒ
            if months_after >= 0 and month > self._month_add(leave_month, months_after):
                break
            seen_repos.add(repo)
            flow_to.append({
                "repo": repo,
                "first_month": month,
                "rank": a["rank"],
                "degree": a["degree"],
            })
        return flow_to

    def _month_add(self, month_str: str, n: int) -> str:
        """YYYY-MM + n ä¸ªæœˆ"""
        y, m = map(int, month_str.split("-"))
        m += n
        while m > 12:
            m -= 12
            y += 1
        while m < 1:
            m += 12
            y -= 1
        return f"{y:04d}-{m:02d}"

    def analyze_repo(
        self,
        repo_name: str,
        metrics_series: List[Dict],
    ) -> Dict[str, Any]:
        """åˆ†æå•ä¸ª repo çš„äººå‘˜æµåŠ¨"""
        if len(metrics_series) < 2:
            return {"error": "æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ 2 ä¸ªæœˆ"}

        core_by_month = self._extract_core_per_month(metrics_series)
        timelines = self._build_member_timelines(core_by_month)
        join_events, leave_events = self._detect_join_leave_events(
            core_by_month, repo_name, timelines
        )
        period_churn = self._compute_period_churn(core_by_month)
        critical_departures = self._identify_critical_departures(leave_events, timelines)

        months = sorted(core_by_month.keys())
        retention = self._compute_retention_rates(timelines, len(months))

        # æ±‡æ€»
        total_joins = len(join_events)
        total_leaves = len(leave_events)
        unique_core = len(timelines)
        avg_tenure = (
            sum(t.tenure_months for t in timelines.values()) / len(timelines)
            if timelines else 0
        )

        return {
            "repo_name": repo_name,
            "period": {"start": months[0], "end": months[-1], "months": len(months)},
            "summary": {
                "unique_core_members": unique_core,
                "total_join_events": total_joins,
                "total_leave_events": total_leaves,
                "avg_tenure_months": round(avg_tenure, 2),
                "critical_departures": len(critical_departures),
            },
            "member_timelines": [
                {
                    "login": t.login,
                    "first_month": t.first_month,
                    "last_month": t.last_month,
                    "tenure_months": t.tenure_months,
                    "avg_rank": t.avg_rank,
                }
                for t in sorted(timelines.values(), key=lambda x: -x.tenure_months)
            ],
            "join_events": join_events,
            "leave_events": leave_events,
            "critical_departures": critical_departures,
            "period_churn": period_churn,
            "retention_rates": retention,
        }

    def run(
        self,
        flow_months_after: int = 12,
    ) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        scope_label = "å…¨éƒ¨è´¡çŒ®è€…" if self.scope == "all" else "æ ¸å¿ƒæˆå‘˜"
        logger.info(f"äººå‘˜æµåŠ¨åˆ†æï¼ˆ{scope_label}ï¼‰...")

        if self.scope == "all":
            if not self.graphs_dir:
                raise ValueError("scope=all æ—¶éœ€æŒ‡å®š --graphs-dir")
            burnout_data = self._load_burnout_data()
            repo_names = list(burnout_data.keys())
            logger.info(f"ä»å›¾åŠ è½½ {len(repo_names)} ä¸ª repo çš„å…¨éƒ¨è´¡çŒ®è€…...")
            data = self._build_all_actors_data_from_graphs(repo_names)
        else:
            data = self._load_burnout_data()
        self._trend_data = data

        results = {}
        for repo_name, repo_data in data.items():
            metrics = repo_data.get("metrics", [])
            if not metrics:
                continue
            logger.info(f"åˆ†æ: {repo_name}")
            try:
                results[repo_name] = self.analyze_repo(repo_name, metrics)
            except Exception as e:
                logger.warning(f"åˆ†æå¤±è´¥ {repo_name}: {e}")
                results[repo_name] = {"error": str(e)}

        # è·¨ repo æµå‘åˆ†æ
        logger.info("è®¡ç®—è·¨ repo æµå‘...")
        global_index = self._build_global_core_index(data)
        for repo_name, repo_result in results.items():
            if "error" in repo_result:
                continue
            for evt in repo_result.get("leave_events", []):
                flow_to = self._find_flow_destinations(
                    evt["login"],
                    evt["repo_name"],
                    evt["month"],
                    global_index,
                    months_after=flow_months_after,
                )
                evt["flowed_to"] = flow_to
            for evt in repo_result.get("critical_departures", []):
                flow_to = self._find_flow_destinations(
                    evt["login"],
                    evt["repo_name"],
                    evt["month"],
                    global_index,
                    months_after=flow_months_after,
                )
                evt["flowed_to"] = flow_to

        # ä¿å­˜
        output_file = self.output_dir / "personnel_flow.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        logger.info(f"å·²ä¿å­˜: {output_file}")

        # ç”ŸæˆæŠ¥å‘Š
        self._save_summary_report(results)
        self._save_leave_events_detail(results)
        self._save_flow_statistics(results)
        self._save_flow_by_year_report(results)
        self._save_flow_timeline_report(results)
        self._save_repo_trend_report()
        self._save_cross_repo_flow_report(results)
        self._save_yearly_status_report(results)
        return results

    def _save_leave_events_detail(self, results: Dict[str, Any]) -> None:
        """ä¿å­˜å…¨éƒ¨æµå¤±æ˜ç»†ï¼ˆå«è·¨ repo æµå‘ï¼‰"""
        report_path = self.output_dir / "leave_events_detail.txt"
        lines = []
        lines.append("=" * 70)
        lines.append(f"å…¨éƒ¨æµå¤±æ˜ç»†ï¼ˆ{self._scope_label()}ï¼Œå«è·¨ repo æµå‘ï¼‰")
        lines.append("=" * 70)
        lines.append("")
        lines.append(f"ã€æ¦‚å¿µè¯´æ˜ã€‘ã€Œç¦»å¼€ã€= æŸæœˆä¸å†å¤„äºè¯¥ repo {self._scope_label()}åå•ã€‚")
        lines.append("")

        repos_with_data = [
            (r, d) for r, d in results.items()
            if "error" not in d and isinstance(d, dict)
        ]
        for repo_name, repo_result in sorted(repos_with_data, key=lambda x: x[0]):
            leave_events = repo_result.get("leave_events", [])
            if not leave_events:
                continue
            lines.append(f"\nã€{repo_name}ã€‘å…± {len(leave_events)} æ¡æµå¤±")
            lines.append("-" * 50)
            for evt in leave_events:
                flow = evt.get("flowed_to", [])
                flow_str = ""
                if flow:
                    flow_str = " â†’ æµå‘: " + ", ".join(
                        f"{f['repo']}({f['first_month']})" for f in flow[:5]
                    )
                    if len(flow) > 5:
                        flow_str += f" ç­‰{len(flow)}é¡¹"
                lines.append(f"  {evt['login']} äº {evt['month']} ç¦»å¼€ï¼Œä»»æœŸ {evt['tenure_months']} æœˆ{flow_str}")
        lines.append("")
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        logger.info(f"å…¨éƒ¨æµå¤±æ˜ç»†å·²ä¿å­˜: {report_path}")

    def _save_flow_statistics(self, results: Dict[str, Any]) -> None:
        """ç»Ÿè®¡æ•´ä½“ repoâ†’repo æµå‘ï¼ŒæŒ‰é¢‘æ¬¡æ’åº"""
        report_path = self.output_dir / "flow_statistics.txt"
        flow_counts: Dict[Tuple[str, str], int] = defaultdict(int)
        seen = set()
        for repo_name, repo_result in results.items():
            if "error" in repo_result:
                continue
            for evt in repo_result.get("leave_events", []):
                key = (evt["login"], repo_name, evt["month"])
                if key in seen:
                    continue
                seen.add(key)
                from_repo = repo_name
                for dest in evt.get("flowed_to", []):
                    to_repo = dest["repo"]
                    if from_repo != to_repo:
                        flow_counts[(from_repo, to_repo)] += 1

        sorted_flows = sorted(
            flow_counts.items(),
            key=lambda x: -x[1],
        )

        lines = []
        lines.append("=" * 70)
        lines.append("Repo â†’ Repo æµå‘ç»Ÿè®¡ï¼ˆæŒ‰äººæ•°æ’åºï¼‰")
        lines.append("=" * 70)
        lines.append("")
        lines.append(f"è¯´æ˜ï¼šä» A ç¦»å¼€å 12 ä¸ªæœˆå†…äº B æˆä¸º{self._scope_label()}çš„äººæ•°ã€‚")
        lines.append("æ ¼å¼ï¼šä» [æ¥æº] æµå‘ [ç›®æ ‡] : N äºº")
        lines.append("")
        lines.append(f"å…± {len(sorted_flows)} æ¡æµå‘è®°å½•")
        lines.append("")
        for (from_repo, to_repo), count in sorted_flows[:80]:
            lines.append(f"  {from_repo}  â†’  {to_repo}  :  {count} äºº")

        # æ¯ä¸ª repo å‡€æµå…¥/å‡€æµå‡º
        inflow: Dict[str, int] = defaultdict(int)
        outflow: Dict[str, int] = defaultdict(int)
        for (from_repo, to_repo), count in flow_counts.items():
            outflow[from_repo] += count
            inflow[to_repo] += count
        all_repos = sorted(set(inflow) | set(outflow))
        repo_net = [
            (r, inflow[r], outflow[r], inflow[r] - outflow[r])
            for r in all_repos
            if inflow[r] > 0 or outflow[r] > 0
        ]
        repo_net.sort(key=lambda x: -abs(x[3]))  # æŒ‰å‡€å˜åŒ–ç»å¯¹å€¼æ’åº

        lines.append("")
        lines.append("=" * 70)
        lines.append("å„ Repo å‡€æµå…¥/å‡€æµå‡ºï¼ˆæœ‰è·¨ repo æµåŠ¨çš„æ‰ç»Ÿè®¡ï¼‰")
        lines.append("=" * 70)
        lines.append("")
        lines.append(f"è¯´æ˜ï¼šæµå…¥ = ä»å…¶ä»– repo ç¦»å¼€åæµå…¥æœ¬ repo çš„{self._scope_label()}æ•°ï¼›æµå‡º = ä»æœ¬ repo ç¦»å¼€åæµå…¥å…¶ä»– repo çš„äººæ•°ã€‚")
        lines.append("æ ¼å¼ï¼šRepo | æµå…¥ | æµå‡º | å‡€ï¼ˆæ­£=å‡€æµå…¥ï¼Œè´Ÿ=å‡€æµå‡ºï¼‰")
        lines.append("")
        for repo, i, o, net in repo_net:
            net_str = f"+{net}" if net > 0 else str(net)
            lines.append(f"  {repo}")
            lines.append(f"    æµå…¥: {i} äºº  æµå‡º: {o} äºº  å‡€: {net_str} äºº")
        lines.append("")

        # ======================================================================
        # AI è¾…åŠ©æ´å¯Ÿï¼šæµåŠ¨æ¨¡å¼åˆ†æ
        # ======================================================================
        lines.append("=" * 70)
        lines.append("AI è¾…åŠ©æ´å¯Ÿï¼šæµåŠ¨æ¨¡å¼è‡ªåŠ¨åˆ†æ")
        lines.append("=" * 70)
        lines.append("è¯´æ˜ï¼š")
        lines.append("- ç”Ÿæ€å…±è£ï¼šåŒå‘æµåŠ¨é¢‘ç¹ï¼Œè¯´æ˜æŠ€æœ¯æ ˆç´§å¯†è€¦åˆï¼ˆåŒå‘å‡ â‰¥ 5 äººï¼Œä¸”æ¯”ä¾‹ < 3:1ï¼‰")
        lines.append("- å¼ºå•å‘è½¬ç§»ï¼šä¸»è¦çš„è´¡çŒ®è€…æµå‘ï¼ˆAâ†’Bäººæ•°æ˜¯Bâ†’Açš„3å€ä»¥ä¸Šï¼Œä¸”Aâ†’B > 10ï¼‰")
        lines.append("- æ–°å…´ç£é“ï¼šäººæ‰å‡€æµå…¥æ˜¾è‘—çš„å¤§å‹é¡¹ç›®ï¼ˆå‡€æµå…¥ > 30ï¼‰")
        lines.append("- åŸºç¡€è®¾æ–½/äººæ‰åº“ï¼šäººæ‰å‡€æµå‡ºæ˜¾è‘—ï¼Œé€šå¸¸æ˜¯åº•å±‚åº“æˆ–è·³æ¿é¡¹ç›®ï¼ˆå‡€æµå‡º < -30ï¼‰")
        lines.append("")

        # 1. åˆ†æåŒå‘/å•å‘å…³ç³»
        repo_pairs = set()
        for (f, t) in flow_counts.keys():
            if f < t:
                repo_pairs.add((f, t))
            else:
                repo_pairs.add((t, f))
        
        symbiotic = []
        one_way = []

        for r1, r2 in repo_pairs:
            f1_to_2 = flow_counts.get((r1, r2), 0)
            f2_to_1 = flow_counts.get((r2, r1), 0)
            total = f1_to_2 + f2_to_1
            if total == 0:
                continue

            # åŒå‘å¼ºå…³è”åˆ¤å®š: åŒæ–¹éƒ½æœ‰ä¸€å®šæµåŠ¨ï¼Œä¸”ä¸æç«¯å¤±è¡¡
            if f1_to_2 >= 5 and f2_to_1 >= 5:
                ratio = max(f1_to_2, f2_to_1) / min(f1_to_2, f2_to_1)
                if ratio < 3.0:
                    symbiotic.append((r1, r2, f1_to_2, f2_to_1, total))
                    continue
            
            # å•å‘åˆ¤å®š
            if f1_to_2 > 10 and f1_to_2 > f2_to_1 * 3:
                one_way.append((r1, r2, f1_to_2, f2_to_1))
            elif f2_to_1 > 10 and f2_to_1 > f1_to_2 * 3:
                one_way.append((r2, r1, f2_to_1, f1_to_2))

        symbiotic.sort(key=lambda x: -x[4]) # æŒ‰æ€»äº¤æµäººæ•°é™åº
        lines.append("[ ç”Ÿæ€å…±è£ç»„åˆ ] (å¼ºå…³è”/ä¸Šä¸‹æ¸¸è€¦åˆ)")
        for r1, r2, v1, v2, tot in symbiotic[:20]:
            # ä¸ºäº†å±•ç¤ºä¸€è‡´æ€§ï¼Œè®©åå­—çŸ­çš„åœ¨å‰ï¼Œæˆ–è€…ä¸éœ€è¦ç‰¹å®šé¡ºåº
            lines.append(f"  {r1} â†” {r2}")
            lines.append(f"    å…± {tot} äººäº¤äº’ ({r1}â†’{r2}: {v1} äºº, åå‘: {v2} äºº)")
        if not symbiotic:
            lines.append("  (æ— æ˜¾è‘—ç»“æœ)")
        lines.append("")

        one_way.sort(key=lambda x: -x[2]) # æŒ‰æµé‡é™åº
        lines.append("[ å¼ºå•å‘è½¬ç§» ] (æµè¡Œåº¦è½¬ç§»æˆ–ç‰¹å®šä¾èµ–è·¯å¾„)")
        for src, dst, v_forward, v_back in one_way[:20]:
            lines.append(f"  {src} â†’ {dst}")
            lines.append(f"    å•å‘æµåŠ¨: {v_forward} äºº (åå‘ä»… {v_back} äºº)")
        if not one_way:
            lines.append("  (æ— æ˜¾è‘—ç»“æœ)")
        lines.append("")

        # 2. åˆ†æå‡€æµå…¥æµå‡ºï¼ˆæ”¹ä¸ºæ¯”ç‡åˆ†æï¼Œå¹¶é¿å…é‡å ï¼‰
        # è¿™é‡Œçš„é‡å ä¸»è¦æŒ‡ï¼šä¸€ä¸ªé¡¹ç›®æ—¢æ˜¯ç£é“åˆæ˜¯åŸºç¡€è®¾æ–½ï¼ˆä¸å¯èƒ½ï¼Œå› ä¸ºå‡€æµæ­£è´Ÿäº’æ–¥ï¼‰ï¼Œ
        # æˆ–è€…å‡ºç°åœ¨ä¸Šé¢çš„å…³ç³»å¯¹ä¸­ã€‚å…³ç³»å¯¹å’Œå•ç‚¹å±æ€§ä¸å†²çªã€‚
        # ä½¿ç”¨æ¯”ç‡åˆ†æï¼š(æµå…¥-æµå‡º)/(æµå…¥+æµå‡º)ï¼Œæ›´èƒ½ä½“ç°â€œè¶‹åŠ¿â€è€Œéå•çº¯çš„ä½“é‡ã€‚
        
        repo_stats = []
        for r, i, o, net in repo_net:
            total = i + o
            if total < 50: # å¿½ç•¥å°æ ·æœ¬ï¼Œé¿å…æ³¢åŠ¨å¤ªå¤§
                continue
            ratio = net / total if total > 0 else 0
            repo_stats.append((r, i, o, net, ratio))

        # ç£é“ï¼šæ¯”ç‡ > 15% (å³å‡€æµå…¥æ˜¾è‘—)
        magnets = [x for x in repo_stats if x[4] > 0.15]
        magnets.sort(key=lambda x: -x[4]) # æŒ‰æ¯”ç‡é™åº

        # åŸºç¡€è®¾æ–½/æµå¤±ï¼šæ¯”ç‡ < -15% (å³å‡€æµå‡ºæ˜¾è‘—)
        feeders = [x for x in repo_stats if x[4] < -0.15]
        feeders.sort(key=lambda x: x[4]) # æŒ‰æ¯”ç‡å‡åºï¼ˆè´Ÿå¾—è¶Šå¤šè¶Šå‰ï¼‰

        lines.append("[ æ–°å…´ç£é“ ] (é«˜å‡€æµå…¥æ¯” - æ­£åœ¨å¿«é€Ÿå¸çº³äººæ‰)")
        lines.append(f"è¯´æ˜ï¼šæ€»æµåŠ¨ > 50 äººï¼Œä¸” (æµå…¥-æµå‡º)/æ€»æµåŠ¨ > 15%")
        for r, i, o, net, ratio in magnets[:15]:
             lines.append(f"  {r:<30} : å‡€å¢ {ratio:+.1%} (å‡€+{net} | å…¥ {i} / å‡º {o})")
        if not magnets:
            lines.append("  (æ— æ˜¾è‘—ç»“æœ)")
        lines.append("")

        lines.append("[ åŸºç¡€è®¾æ–½/äººæ‰åº“ ] (é«˜å‡€æµå‡ºæ¯” - å¹¿æ³›è¢«ä½¿ç”¨æˆ–ä½œä¸ºè·³æ¿)")
        lines.append(f"è¯´æ˜ï¼šæ€»æµåŠ¨ > 50 äººï¼Œä¸” (æµå…¥-æµå‡º)/æ€»æµåŠ¨ < -15%")
        for r, i, o, net, ratio in feeders[:15]:
             lines.append(f"  {r:<30} : å‡€æµ {ratio:+.1%} (å‡€{net} | å…¥ {i} / å‡º {o})")
        if not feeders:
            lines.append("  (æ— æ˜¾è‘—ç»“æœ)")
        lines.append("")

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        logger.info(f"æµå‘ç»Ÿè®¡å·²ä¿å­˜: {report_path}")

    def _save_flow_by_year_report(self, results: Dict[str, Any]) -> None:
        """æŒ‰å¹´ç»Ÿè®¡äººå‘˜æµå‘ï¼Œå¹¶æ’åæµå…¥æœ€å¤šçš„ç›®æ ‡ repo"""
        report_path = self.output_dir / "flow_by_year.txt"
        # year -> (from, to) -> count
        year_flow_counts: Dict[str, Dict[Tuple[str, str], int]] = defaultdict(lambda: defaultdict(int))

        for repo_name, repo_result in results.items():
            if "error" in repo_result:
                continue
            for evt in repo_result.get("leave_events", []):
                leave_month = evt.get("month", "")
                if not leave_month or len(leave_month) < 4:
                    continue
                year = leave_month[:4]
                from_repo = repo_name
                for dest in evt.get("flowed_to", []):
                    to_repo = dest["repo"]
                    if from_repo != to_repo:
                        year_flow_counts[year][(from_repo, to_repo)] += 1

        years = sorted(year_flow_counts.keys())
        lines = []
        lines.append("=" * 70)
        lines.append("æŒ‰å¹´ç»Ÿè®¡äººå‘˜æµå‘")
        lines.append("=" * 70)
        lines.append("")
        lines.append(f"è¯´æ˜ï¼šæŒ‰ç¦»å¼€å¹´ä»½ç»Ÿè®¡è·¨ repo æµå‘ã€‚å¹´ä»½ä»¥ç¦»å¼€æœˆä¸ºå‡†ã€‚")
        lines.append("")

        # 1. å„å¹´æµå…¥æœ€å¤šçš„ç›®æ ‡ Repo æ’åï¼ˆTop Nï¼‰
        lines.append("=" * 70)
        lines.append("å„å¹´æµå…¥æœ€å¤šçš„ç›®æ ‡ Repo æ’åï¼ˆTop 15ï¼‰")
        lines.append("=" * 70)
        lines.append("")
        for year in years:
            counts = year_flow_counts[year]
            inflow: Dict[str, int] = defaultdict(int)
            for (_, to_repo), c in counts.items():
                inflow[to_repo] += c
            top_dests = sorted(inflow.items(), key=lambda x: -x[1])[:15]
            lines.append(f"ã€{year} å¹´ã€‘")
            for rank, (repo, count) in enumerate(top_dests, 1):
                lines.append(f"  {rank}. {repo}  æµå…¥ {count} äºº")
            lines.append("")

        # 2. æ•´ä½“æµå…¥æœ€å¤šçš„ç›®æ ‡ Repo æ’åï¼ˆTop 30ï¼‰
        all_inflow: Dict[str, int] = defaultdict(int)
        for year_counts in year_flow_counts.values():
            for (_, to_repo), c in year_counts.items():
                all_inflow[to_repo] += c
        top_all = sorted(all_inflow.items(), key=lambda x: -x[1])[:30]
        lines.append("=" * 70)
        lines.append("æ•´ä½“æµå…¥æœ€å¤šçš„ç›®æ ‡ Repo æ’åï¼ˆTop 30ï¼‰")
        lines.append("=" * 70)
        lines.append("")
        for rank, (repo, count) in enumerate(top_all, 1):
            lines.append(f"  {rank}. {repo}  æµå…¥ {count} äºº")
        lines.append("")

        # 3. å„å¹´æ˜ç»†ï¼šæµå‘æ¡æ•°ã€æµå…¥/æµå‡º repo ç»Ÿè®¡
        lines.append("=" * 70)
        lines.append("å„å¹´æµå‘æ˜ç»†")
        lines.append("=" * 70)
        lines.append("")
        for year in years:
            counts = year_flow_counts[year]
            inflow_y: Dict[str, int] = defaultdict(int)
            outflow_y: Dict[str, int] = defaultdict(int)
            for (from_repo, to_repo), c in counts.items():
                outflow_y[from_repo] += c
                inflow_y[to_repo] += c
            total_flows = sum(counts.values())
            lines.append(f"ã€{year} å¹´ã€‘å…± {total_flows} æ¡æµå‘")
            top_in = sorted(inflow_y.items(), key=lambda x: -x[1])[:10]
            top_out = sorted(outflow_y.items(), key=lambda x: -x[1])[:10]
            lines.append(f"  æµå…¥æœ€å¤š: {', '.join(f'{r}({c})' for r, c in top_in)}")
            lines.append(f"  æµå‡ºæœ€å¤š: {', '.join(f'{r}({c})' for r, c in top_out)}")
            lines.append("")

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        logger.info(f"æŒ‰å¹´æµå‘ç»Ÿè®¡å·²ä¿å­˜: {report_path}")

    def _save_flow_timeline_report(self, results: Dict[str, Any]) -> None:
        """æŒ‰æ—¶é—´é¡ºåºç»Ÿè®¡äººæ‰æµåŠ¨æƒ…å†µ"""
        report_path = self.output_dir / "flow_timeline.txt"
        events = []
        for repo_name, repo_result in results.items():
            if "error" in repo_result:
                continue
            for evt in repo_result.get("leave_events", []):
                events.append({
                    "month": evt["month"],
                    "repo": repo_name,
                    "login": evt["login"],
                    "tenure": evt.get("tenure_months", 0),
                    "flowed_to": evt.get("flowed_to", []),
                })
        events.sort(key=lambda x: (x["month"], x["repo"], x["login"]))

        lines = []
        lines.append("=" * 70)
        lines.append("äººæ‰æµåŠ¨æ—¶é—´çº¿ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰")
        lines.append("=" * 70)
        lines.append("")
        lines.append(f"è¯´æ˜ï¼šæŒ‰æœˆåˆ—å‡º{self._scope_label()}ç¦»å¼€äº‹ä»¶åŠæµå‘ã€‚")
        lines.append("")

        current_month = None
        month_leave_count = 0
        month_with_flow_count = 0

        for evt in events:
            m = evt["month"]
            if m != current_month:
                if current_month is not None:
                    lines.append("")
                    lines.append(f"  ã€{current_month} æ±‡æ€»ã€‘{month_leave_count} äººç¦»å¼€ï¼Œ"
                                f"å…¶ä¸­ {month_with_flow_count} äººæœ‰è·¨ repo æµå‘")
                current_month = m
                month_leave_count = 0
                month_with_flow_count = 0
                lines.append(f"\n--- {m} ---")

            month_leave_count += 1
            flow_str = ""
            if evt["flowed_to"]:
                month_with_flow_count += 1
                dests = [f"{d['repo']}({d['first_month']})" for d in evt["flowed_to"][:3]]
                flow_str = " â†’ " + ", ".join(dests)
                if len(evt["flowed_to"]) > 3:
                    flow_str += f" ç­‰{len(evt['flowed_to'])}é¡¹"
            lines.append(f"  {evt['login']} ç¦»å¼€ {evt['repo']}ï¼ˆä»»æœŸ{evt['tenure']}æœˆï¼‰{flow_str}")

        if current_month is not None:
            lines.append("")
            lines.append(f"  ã€{current_month} æ±‡æ€»ã€‘{month_leave_count} äººç¦»å¼€ï¼Œ"
                        f"å…¶ä¸­ {month_with_flow_count} äººæœ‰è·¨ repo æµå‘")

        lines.append("")
        lines.append("=" * 70)
        lines.append("æŒ‰æœˆæµåŠ¨é‡ç»Ÿè®¡")
        lines.append("=" * 70)
        month_totals: Dict[str, int] = defaultdict(int)
        month_with_flow: Dict[str, int] = defaultdict(int)
        for evt in events:
            month_totals[evt["month"]] += 1
            if evt["flowed_to"]:
                month_with_flow[evt["month"]] += 1
        for m in sorted(month_totals.keys()):
            lines.append(f"  {m}: å…± {month_totals[m]} äººç¦»å¼€ï¼Œ{month_with_flow[m]} äººæœ‰æµå‘")

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        logger.info(f"æµåŠ¨æ—¶é—´çº¿å·²ä¿å­˜: {report_path}")

    def _save_repo_trend_report(self) -> None:
        """ç»Ÿè®¡ repo æµè¡Œè¶‹åŠ¿ï¼ˆæŒ‰æ—¶é—´ç»´åº¦çš„æ´»è·ƒåº¦å˜åŒ–ï¼‰"""
        report_path = self.output_dir / "repo_trend.txt"
        data = getattr(self, "_trend_data", None) or self._load_burnout_data()

        # æå–æ¯ä¸ª repo çš„æœˆåº¦åºåˆ—
        repo_series: Dict[str, List[Dict]] = {}
        for repo_name, repo_data in data.items():
            metrics = repo_data.get("metrics", [])
            if not metrics:
                continue
            sorted_m = sorted(metrics, key=lambda x: x.get("month", ""))
            repo_series[repo_name] = [
                {
                    "month": m.get("month", ""),
                    "unique_actors": m.get("unique_actors", 0),
                    "total_events": m.get("total_events", 0),
                    "core_count": len(m.get("core_actors", [])),
                }
                for m in sorted_m
            ]

        def _trend_score(series: List[Dict]) -> Tuple[float, str]:
            """è®¡ç®—è¶‹åŠ¿ï¼šå‰åŠæ®µ vs ååŠæ®µå‡å€¼æ¯”è¾ƒï¼Œè¿”å› (å¾—åˆ†, è¶‹åŠ¿æè¿°)"""
            if len(series) < 4:
                return 0.0, "æ•°æ®ä¸è¶³"
            n = len(series)
            mid = n // 2
            early = series[:mid]
            late = series[mid:]
            early_actors = sum(s["unique_actors"] for s in early) / len(early)
            late_actors = sum(s["unique_actors"] for s in late) / len(late)
            early_events = sum(s["total_events"] for s in early) / len(early)
            late_events = sum(s["total_events"] for s in late) / len(late)
            if early_actors <= 0:
                pct_actors = 1.0 if late_actors > 0 else 0.0
            else:
                pct_actors = (late_actors - early_actors) / early_actors
            if early_events <= 0:
                pct_events = 1.0 if late_events > 0 else 0.0
            else:
                pct_events = (late_events - early_events) / early_events
            score = 0.5 * pct_actors + 0.5 * pct_events
            if score > 0.2:
                desc = "ä¸Šå‡"
            elif score < -0.2:
                desc = "ä¸‹é™"
            else:
                desc = "å¹³ç¨³"
            return score, desc

        trends = []
        for repo_name, series in repo_series.items():
            score, desc = _trend_score(series)
            first = series[0]
            last = series[-1]
            trends.append({
                "repo": repo_name,
                "score": score,
                "desc": desc,
                "start_month": first["month"],
                "end_month": last["month"],
                "start_actors": first["unique_actors"],
                "end_actors": last["unique_actors"],
                "start_events": first["total_events"],
                "end_events": last["total_events"],
                "months": len(series),
            })

        trends.sort(key=lambda x: -x["score"])

        lines = []
        lines.append("=" * 70)
        lines.append("Repo æµè¡Œè¶‹åŠ¿")
        lines.append("=" * 70)
        lines.append("")
        lines.append("è¯´æ˜ï¼šæ¯”è¾ƒå„ repo å‰åŠæ®µä¸ååŠæ®µçš„æ´»è·ƒåº¦ï¼ˆå‚ä¸è€…æ•°ã€äº‹ä»¶æ•°ï¼‰ï¼Œåˆ¤æ–­è¶‹åŠ¿ã€‚")
        lines.append("è¶‹åŠ¿ï¼šä¸Šå‡ = ååŠæ®µæ˜æ˜¾å¢é•¿ï¼Œä¸‹é™ = ååŠæ®µæ˜æ˜¾å‡å°‘ï¼Œå¹³ç¨³ = å˜åŒ–ä¸å¤§ã€‚")
        lines.append("")

        lines.append("ã€ä¸Šå‡è¶‹åŠ¿ã€‘")
        for t in [x for x in trends if x["desc"] == "ä¸Šå‡"][:25]:
            lines.append(f"  {t['repo']}")
            lines.append(f"    {t['start_month']} ~ {t['end_month']}: "
                        f"å‚ä¸è€… {t['start_actors']}â†’{t['end_actors']}, "
                        f"äº‹ä»¶ {t['start_events']}â†’{t['end_events']}")

        lines.append("")
        lines.append("ã€ä¸‹é™è¶‹åŠ¿ã€‘")
        for t in [x for x in trends if x["desc"] == "ä¸‹é™"][:25]:
            lines.append(f"  {t['repo']}")
            lines.append(f"    {t['start_month']} ~ {t['end_month']}: "
                        f"å‚ä¸è€… {t['start_actors']}â†’{t['end_actors']}, "
                        f"äº‹ä»¶ {t['start_events']}â†’{t['end_events']}")

        lines.append("")
        lines.append("ã€å¹³ç¨³ã€‘")
        for t in [x for x in trends if x["desc"] == "å¹³ç¨³"][:15]:
            lines.append(f"  {t['repo']} ({t['start_month']}~{t['end_month']})")

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        logger.info(f"Repo æµè¡Œè¶‹åŠ¿å·²ä¿å­˜: {report_path}")

    def _save_cross_repo_flow_report(self, results: Dict[str, Any]) -> None:
        """ä¿å­˜è·¨ repo æµå‘ä¸“é¢˜æŠ¥å‘Š"""
        report_path = self.output_dir / "cross_repo_flow.txt"
        lines = []
        lines.append("=" * 70)
        lines.append("è·¨ Repo æµå‘æŠ¥å‘Š")
        lines.append("=" * 70)
        lines.append("")
        label = self._scope_label()
        lines.append(f"ã€ã€Œç¦»å¼€ã€å«ä¹‰ã€‘æŸæœˆä¸å†å¤„äºè¯¥ repo {label}åå•ï¼Œä¸è¡¨ç¤ºå®Œå…¨ä¸å‚ä¸é¡¹ç›®ã€‚")
        lines.append("")
        lines.append("ã€æµå‘ã€‘ç¦»å¼€ A åï¼Œè‹¥åœ¨ 12 ä¸ªæœˆå†…äº B/C ç­‰å‚ä¸ï¼ˆscope=allï¼‰æˆ–æˆä¸ºæ ¸å¿ƒï¼ˆscope=coreï¼‰ï¼Œåˆ™è®°ä¸ºæµå‘ã€‚")
        lines.append("ã€æ³¨ã€‘éƒ¨åˆ†è´¦å·ä¸º botï¼ˆå¦‚ github-actions[bot]ï¼‰ï¼Œä¼šåŒæ—¶å‡ºç°åœ¨å¤šé¡¹ç›®ä¸­ã€‚")
        lines.append("")

        # æ”¶é›†æœ‰æµå‘çš„å…³é”®æµå¤±
        flows: List[Dict] = []
        for repo_name, repo_result in results.items():
            if "error" in repo_result:
                continue
            for c in repo_result.get("critical_departures", []):
                ft = c.get("flowed_to", [])
                if ft:
                    flows.append({
                        "login": c["login"],
                        "from_repo": repo_name,
                        "leave_month": c["month"],
                        "tenure": c["tenure_months"],
                        "flowed_to": ft,
                    })

        flows.sort(key=lambda x: -len(x["flowed_to"]))

        lines.append(f"æœ‰æµå‘è®°å½•çš„å…³é”®æµå¤±: {len(flows)} äºº")
        lines.append("")
        for f in flows[:50]:
            dests = ", ".join(f"{d['repo']}({d['first_month']})" for d in f["flowed_to"][:5])
            if len(f["flowed_to"]) > 5:
                dests += f" ç­‰{len(f['flowed_to'])}é¡¹"
            lines.append(f"  {f['login']}: {f['from_repo']} ({f['leave_month']}, ä»»æœŸ{f['tenure']}æœˆ)")
            lines.append(f"    â†’ {dests}")
            lines.append("")

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        logger.info(f"è·¨ repo æµå‘æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    def _save_summary_report(self, results: Dict[str, Any]) -> None:
        """ä¿å­˜ç®€è¦æ–‡æœ¬æŠ¥å‘Š"""
        report_path = self.output_dir / "summary_report.txt"
        lines = []
        lines.append("=" * 70)
        lines.append(f"äººå‘˜æµåŠ¨åˆ†ææŠ¥å‘Š - {self._scope_label()}æ‘˜è¦")
        lines.append("=" * 70)
        lines.append("")
        lines.append("ã€æ¦‚å¿µè¯´æ˜ã€‘")
        label = self._scope_label()
        lines.append(f"  ã€Œç¦»å¼€ã€= æŸæœˆä¸å†å¤„äºè¯¥ repo çš„{label}åå•ã€‚")
        if self.scope == "core":
            lines.append("  æ ¸å¿ƒæˆå‘˜ç”±ã€ŒåŠ æƒè´¡çŒ®é‡+ç½‘ç»œä½ç½®ã€åŠ¨æ€è®¡ç®—ï¼Œæ¯æœˆé€‰å‡ºè´¡çŒ®çº¦å‰ 50% è€…ã€‚")
        lines.append("  ç¦»å¼€ â‰  å®Œå…¨ä¸å‚ä¸ï¼Œå¯èƒ½æ˜¯ï¼šå‚ä¸å‡å°‘ã€å®Œå…¨é€€å‡ºã€æˆ–è§’è‰²å˜åŒ–ã€‚")
        lines.append("")
        lines.append("ã€å…¶ä»–æŠ¥å‘Šã€‘")
        lines.append("  leave_events_detail.txt  - å…¨éƒ¨æµå¤±æ˜ç»†")
        lines.append("  flow_statistics.txt      - Repoâ†’Repo æµå‘ç»Ÿè®¡ï¼ˆæ•´ä½“æ’åºï¼‰")
        lines.append("  flow_by_year.txt         - æŒ‰å¹´ç»Ÿè®¡æµå‘ + æµå…¥æœ€å¤šçš„ç›®æ ‡ Repo æ’å")
        lines.append("  flow_timeline.txt        - äººæ‰æµåŠ¨æ—¶é—´çº¿ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰")
        lines.append("  repo_trend.txt           - Repo æµè¡Œè¶‹åŠ¿ï¼ˆä¸Šå‡/ä¸‹é™/å¹³ç¨³ï¼‰")
        lines.append("")

        # æŒ‰å…³é”®æµå¤±æ•°æ’åº
        repos_with_data = [
            (r, d) for r, d in results.items()
            if "error" not in d and isinstance(d, dict)
        ]
        repos_with_data.sort(
            key=lambda x: x[1].get("summary", {}).get("critical_departures", 0),
            reverse=True,
        )

        for repo_name, repo_result in repos_with_data[:30]:
            summary = repo_result.get("summary", {})
            period = repo_result.get("period", {})
            lines.append(f"\nã€{repo_name}ã€‘")
            lines.append(f"  åˆ†æå‘¨æœŸ: {period.get('start', '')} ~ {period.get('end', '')} ({period.get('months', 0)} æœˆ)")
            lines.append(f"  {label}æ•°: {summary.get('unique_core_members', 0)}")
            lines.append(f"  æµå…¥äº‹ä»¶: {summary.get('total_join_events', 0)}  æµå‡ºäº‹ä»¶: {summary.get('total_leave_events', 0)}")
            lines.append(f"  å¹³å‡ä»»æœŸ: {summary.get('avg_tenure_months', 0)} æœˆ")
            lines.append(f"  å…³é”®æµå¤±(ä»»æœŸâ‰¥6æœˆ): {summary.get('critical_departures', 0)}")

            critical = repo_result.get("critical_departures", [])[:5]
            if critical:
                lines.append("  å…³é”®æµå¤±æ˜ç»†ï¼ˆå«è·¨ repo æµå‘ï¼‰:")
                for c in critical:
                    flow = c.get("flowed_to", [])
                    flow_str = ""
                    if flow:
                        flow_str = " â†’ æµå‘: " + ", ".join(
                            f"{f['repo']}({f['first_month']})" for f in flow[:3]
                        )
                        if len(flow) > 3:
                            flow_str += f" ç­‰{len(flow)}é¡¹"
                    lines.append(f"    - {c['login']} äº {c['month']} ç¦»å¼€ï¼Œä»»æœŸ {c['tenure_months']} æœˆ{flow_str}")

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        logger.info(f"æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    def _save_yearly_status_report(self, results: Dict[str, Any]) -> None:
        """æŒ‰å¹´åˆ†æé¡¹ç›®çŠ¶æ€ï¼ˆç£é“/åŸºç¡€è®¾æ–½ç­‰ï¼‰"""
        report_path = self.output_dir / "repo_yearly_status.txt"
        
        # 1. æ„å»ºæ¯å¹´çš„æµåŠ¨æ•°æ®
        # year_stats[year][repo] = {"in": 0, "out": 0}
        year_stats: Dict[str, Dict[str, Dict[str, int]]] = defaultdict(lambda: defaultdict(lambda: {"in": 0, "out": 0}))

        for repo_name, repo_result in results.items():
            if "error" in repo_result:
                continue
            
            # ç»Ÿè®¡æµå‡º
            for evt in repo_result.get("leave_events", []):
                month = evt.get("month", "")
                year = month[:4] if len(month) >= 4 else "Unknown"
                if year == "Unknown": continue

                for dest in evt.get("flowed_to", []):
                    to_repo = dest["repo"]
                    # åªæœ‰å½“ç›®æ ‡åœ¨æˆ‘ä»¬çš„åˆ†æèŒƒå›´å†…æ—¶æ‰è®¡å…¥ï¼ˆç¡®ä¿é—­ç¯ï¼‰
                    if to_repo in results:
                        year_stats[year][repo_name]["out"] += 1
                        year_stats[year][to_repo]["in"] += 1

        years = sorted(year_stats.keys())
        lines = []
        lines.append("=" * 80)
        lines.append("é¡¹ç›®å¹´åº¦æµåŠ¨çŠ¶æ€åˆ†æ")
        lines.append("=" * 80)
        lines.append("è¯´æ˜ï¼š")
        lines.append("- ç£é“å‹ (Magnet): å‡€æµå…¥æ¯” > 15% (ä¸”æ€»æµåŠ¨ > 5)")
        lines.append("- è¾“è¡€å‹ (Feeder): å‡€æµå‡ºæ¯” < -15% (ä¸”æ€»æµåŠ¨ > 5)")
        lines.append("- å¹³è¡¡å‹ (Balanced): ä»‹äºä¸¤è€…ä¹‹é—´")
        lines.append("- æ²‰å¯‚å‹ (Quiet): æ€»æµåŠ¨ â‰¤ 5")
        lines.append("")

        for year in years:
            lines.append(f"\n[ {year} å¹´åº¦çŠ¶æ€ ]")
            lines.append("-" * 80)
            
            # åˆ†ç±»å­˜å‚¨
            magnets = []
            feeders = []
            balanced = []
            quiet = []

            stats_map = year_stats[year]
            # ç¡®ä¿æˆ‘ä»¬è¦åˆ†æçš„æ‰€æœ‰ repo éƒ½åœ¨ stats_map é‡Œï¼ˆå³ä½¿æ²¡æœ‰æµåŠ¨è®°ä¸º0ï¼‰
            current_repos = sorted(results.keys())
            
            for repo in current_repos:
                s = stats_map.get(repo, {"in": 0, "out": 0})
                i, o = s["in"], s["out"]
                net = i - o
                total = i + o
                ratio = net / total if total > 0 else 0

                item = (repo, i, o, net, ratio, total) # å¢åŠ  total

                if total <= 5:
                    quiet.append(item)
                elif ratio > 0.15:
                    magnets.append(item)
                elif ratio < -0.15:
                    feeders.append(item)
                else:
                    balanced.append(item)

            # æ’åºé€»è¾‘ï¼šç£é“æŒ‰å‡€æµå…¥é™åºï¼Œè¾“è¡€æŒ‰å‡€æµå‡ºå‡åºï¼ˆè´Ÿæœ€å¤šï¼‰
            magnets.sort(key=lambda x: -x[4])
            feeders.sort(key=lambda x: x[4])
            balanced.sort(key=lambda x: -x[5]) # å¹³è¡¡å‹æŒ‰æ€»æ´»è·ƒåº¦
            quiet.sort(key=lambda x: -x[5]) # æ²‰å¯‚å‹æŒ‰æ€»æ´»è·ƒåº¦

            if magnets:
                lines.append("  ğŸš€ ç£é“å‹ (å¸çº³äººæ‰):")
                for r, i, o, n, rat, t in magnets: # æ˜¾ç¤ºå…¨éƒ¨
                    lines.append(f"    {r:<30} : å‡€å¢ {rat:+.1%} (å‡€{n:+d} | å…¥{i}/å‡º{o})")
            
            if feeders:
                lines.append("\n  ğŸŒ± è¾“è¡€å‹ (äººæ‰è¾“å‡º):")
                for r, i, o, n, rat, t in feeders: # æ˜¾ç¤ºå…¨éƒ¨
                    lines.append(f"    {r:<30} : å‡€æµ {rat:+.1%} (å‡€{n:+d} | å…¥{i}/å‡º{o})")

            if balanced:
                lines.append("\n  âš–ï¸ å¹³è¡¡å‹ (æµåŠ¨ç¨³å®š):")
                for r, i, o, n, rat, t in balanced: # æ˜¾ç¤ºå…¨éƒ¨
                    lines.append(f"    {r:<30} : å‡€ {rat:+.1%} (å…¥{i}/å‡º{o})")

            if quiet:
                lines.append("\n  ğŸ’¤ æ²‰å¯‚å‹ (æµåŠ¨æå°‘ â‰¤ 5):")
                # æ²‰å¯‚å‹å¯ä»¥æŠ˜å æ˜¾ç¤ºï¼Œæˆ–è€…åªåˆ—åå­—ï¼Œé¿å…å¤ªé•¿
                # æŒ‰æ¯è¡Œ3ä¸ªæ˜¾ç¤º
                quiet_repos = [r for r, _, _, _, _, _ in quiet]
                for k in range(0, len(quiet_repos), 3):
                    chunk = quiet_repos[k:k+3]
                    lines.append("    " + "  ,  ".join(chunk))

        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        logger.info(f"å¹´åº¦çŠ¶æ€åˆ†æå·²ä¿å­˜: {report_path}")


def main():
    import argparse
    parser = argparse.ArgumentParser(description="äººå‘˜æµåŠ¨åˆ†æ")
    parser.add_argument(
        "--input",
        type=str,
        default="output/burnout-analysis/full_analysis.json",
        help="å€¦æ€ åˆ†æç»“æœæ–‡ä»¶",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼špersonnel-flow æˆ– personnel-flow-allï¼‰",
    )
    parser.add_argument(
        "--scope",
        type=str,
        choices=["core", "all"],
        default="core",
        help="åˆ†æèŒƒå›´ï¼šcore=æ ¸å¿ƒæˆå‘˜ï¼Œall=å…¨éƒ¨è´¡çŒ®è€…ï¼ˆéœ€ --graphs-dirï¼‰",
    )
    parser.add_argument(
        "--graphs-dir",
        type=str,
        default=None,
        help="æœˆåº¦å›¾ç›®å½•ï¼ˆscope=all æ—¶å¿…å¡«ï¼Œéœ€ä¸ burnout åˆ†æä½¿ç”¨çš„å›¾ä¸€è‡´ï¼‰",
    )
    parser.add_argument(
        "--flow-months",
        type=int,
        default=12,
        help="ç¦»å¼€åè¿½è¸ªæµå‘çš„æœˆæ•° (é»˜è®¤: 12)"
    )
    args = parser.parse_args()

    output_dir = args.output_dir
    if output_dir is None:
        output_dir = "output/personnel-flow-all/" if args.scope == "all" else "output/personnel-flow/"

    if args.scope == "all" and not args.graphs_dir:
        parser.error("scope=all æ—¶å¿…é¡»æŒ‡å®š --graphs-dir")

    analyzer = PersonnelFlowAnalyzer(
        input_path=args.input,
        output_dir=output_dir,
        scope=args.scope,
        graphs_dir=args.graphs_dir,
    )
    analyzer.run(flow_months_after=args.flow_months)


if __name__ == "__main__":
    main()
