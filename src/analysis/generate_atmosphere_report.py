"""
è¯¦ç»†ç¤¾åŒºæ°›å›´åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨

æŒ‰é¡¹ç›®è¾“å‡ºæ¯ä¸€é¡¹å¾—åˆ†çš„æ¥æºå’Œæ•°å€¼å˜åŒ–
"""

import json
import argparse
import sys
import io
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


def generate_repo_report(repo_name: str, repo_data: Dict[str, Any]) -> str:
    """ç”Ÿæˆå•ä¸ªä»“åº“çš„è¯¦ç»†æŠ¥å‘Š"""
    lines = []
    lines.append("=" * 80)
    lines.append(f"ğŸ“Š é¡¹ç›®: {repo_name}")
    lines.append("=" * 80)
    
    # è·å–æ°›å›´è¯„åˆ†
    atmosphere = repo_data.get("atmosphere_score", {})
    score = atmosphere.get("score", 0)
    level = atmosphere.get("level", "unknown")
    period = atmosphere.get("period", "N/A")
    months = atmosphere.get("months_analyzed", 0)
    
    # æ°›å›´ç­‰çº§å›¾æ ‡
    level_icons = {
        "excellent": "ğŸŸ¢ ä¼˜ç§€",
        "good": "ğŸŸ¢ è‰¯å¥½",
        "moderate": "ğŸŸ¡ ä¸­ç­‰",
        "poor": "ğŸ”´ è¾ƒå·®",
        "unknown": "âšª æœªçŸ¥"
    }
    
    lines.append(f"\nğŸ¯ ç»¼åˆæ°›å›´è¯„åˆ†: {score:.2f} / 100")
    lines.append(f"   æ°›å›´ç­‰çº§: {level_icons.get(level, level)}")
    lines.append(f"   åˆ†æå‘¨æœŸ: {period} ({months} ä¸ªæœˆ)")
    
    # è·å–æŒ‡æ ‡æ—¶é—´åºåˆ—
    metrics = repo_data.get("metrics", [])
    if len(metrics) < 2:
        lines.append("\nâš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æ")
        return "\n".join(lines)
    
    # æŒ‰æœˆä»½æ’åº
    sorted_metrics = sorted(metrics, key=lambda m: m.get("month", ""))
    earliest = sorted_metrics[0]
    latest = sorted_metrics[-1]
    
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“ˆ å„å› å­è¯¦ç»†åˆ†æï¼ˆä¸‰å¤§å› å­ï¼šæƒ…ç»ªæ°›å›´20% + ç¤¾åŒºç´§å¯†åº¦40% + ç½‘ç»œæ•ˆç‡40%ï¼‰")
    lines.append("-" * 80)
    
    factors = atmosphere.get("factors", {})
    
    # 1. æƒ…ç»ªæ°›å›´å› å­
    lines.append("\nã€1. æƒ…ç»ªæ°›å›´å› å­ã€‘(0-20åˆ†ï¼Œæƒé‡20%)")
    emotion = factors.get("emotion", {})
    emotion_value = emotion.get("value", 0)
    emotion_score = emotion.get("score", 0)
    
    early_emotion = earliest.get("average_emotion", 0)
    late_emotion = latest.get("average_emotion", 0)
    
    lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"      é¦–æœˆå¹³å‡æƒ…ç»ª: {early_emotion:+.3f}  â†’  æœ«æœˆå¹³å‡æƒ…ç»ª: {late_emotion:+.3f}")
    lines.append(f"      æ•´ä½“å¹³å‡æƒ…ç»ª: {emotion_value:+.3f} (èŒƒå›´: -1.0 åˆ° +1.0)")
    
    # è®¡ç®—è¶‹åŠ¿
    emotion_values = [m.get("average_emotion", 0) for m in sorted_metrics]
    if len(emotion_values) >= 3:
        early_avg = sum(emotion_values[:3]) / min(3, len(emotion_values))
        recent_avg = sum(emotion_values[-3:]) / min(3, len(emotion_values))
        change = recent_avg - early_avg
        lines.append(f"      æ—©æœŸ3æœˆå‡å€¼: {early_avg:+.3f}  â†’  è¿‘æœŸ3æœˆå‡å€¼: {recent_avg:+.3f}")
        if change > 0:
            lines.append(f"      âœ… æƒ…ç»ªè¶‹åŠ¿å‘å¥½ (æå‡ {change:+.3f})")
        elif change < 0:
            lines.append(f"      âš ï¸ æƒ…ç»ªè¶‹åŠ¿ä¸‹é™ (ä¸‹é™ {abs(change):.3f})")
        else:
            lines.append(f"      â¡ï¸ æƒ…ç»ªä¿æŒç¨³å®š")
    
    lines.append(f"   â¡ï¸ å› å­å¾—åˆ†: {emotion_score:.2f} / 20")
    lines.append(f"      (å½’ä¸€åŒ–å…¬å¼: (avg_emotion + 1.0) / 2.0 * 20)")
    
    # 2. ç¤¾åŒºç´§å¯†åº¦å› å­ï¼ˆèšç±»ç³»æ•°ï¼‰
    lines.append("\nã€2. ç¤¾åŒºç´§å¯†åº¦å› å­ã€‘(0-40åˆ†ï¼Œæƒé‡40%)")
    clustering = factors.get("clustering", {})
    clustering_value = clustering.get("value", 0)
    clustering_score = clustering.get("score", 0)
    
    early_clustering = earliest.get("average_local_clustering", 0)
    late_clustering = latest.get("average_local_clustering", 0)
    
    lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"      é¦–æœˆå¹³å‡èšç±»ç³»æ•°: {early_clustering:.3f}  â†’  æœ«æœˆå¹³å‡èšç±»ç³»æ•°: {late_clustering:.3f}")
    lines.append(f"      æ•´ä½“å¹³å‡èšç±»ç³»æ•°: {clustering_value:.3f} (èŒƒå›´: 0.0 åˆ° 1.0)")
    
    # è®¡ç®—è¶‹åŠ¿
    clustering_values = [m.get("average_local_clustering", 0) for m in sorted_metrics]
    if len(clustering_values) >= 3:
        early_avg = sum(clustering_values[:3]) / min(3, len(clustering_values))
        recent_avg = sum(clustering_values[-3:]) / min(3, len(clustering_values))
        change = recent_avg - early_avg
        lines.append(f"      æ—©æœŸ3æœˆå‡å€¼: {early_avg:.3f}  â†’  è¿‘æœŸ3æœˆå‡å€¼: {recent_avg:.3f}")
        if change > 0.01:
            lines.append(f"      âœ… ç´§å¯†åº¦æå‡ (æå‡ {change:+.3f})")
        elif change < -0.01:
            lines.append(f"      âš ï¸ ç´§å¯†åº¦ä¸‹é™ (ä¸‹é™ {abs(change):+.3f})")
        else:
            lines.append(f"      â¡ï¸ ç´§å¯†åº¦ä¿æŒç¨³å®š")
    
    # è§£é‡Šå½’ä¸€åŒ–é€»è¾‘
    clustering_threshold = 0.6
    clustering_growth_factor = 2.0
    if clustering_value <= 0.0:
        norm_explanation = "0.0 (èšç±»ç³»æ•°ä¸º0)"
    elif clustering_value >= clustering_threshold:
        norm_explanation = "1.0 (è¾¾åˆ°é˜ˆå€¼0.6)"
    else:
        norm_explanation = f"{1.0 / (1.0 + clustering_growth_factor * (clustering_threshold - clustering_value) / clustering_threshold):.3f} (å¹³æ»‘å¢é•¿å‡½æ•°)"
    
    lines.append(f"   â¡ï¸ å› å­å¾—åˆ†: {clustering_score:.2f} / 40")
    lines.append(f"      (å½’ä¸€åŒ–å€¼: {norm_explanation})")
    lines.append(f"      (å½’ä¸€åŒ–å…¬å¼: å¹³æ»‘å¢é•¿å‡½æ•°ï¼Œé˜ˆå€¼={clustering_threshold}, å¢é•¿å› å­={clustering_growth_factor})")
    
    # 3. ç½‘ç»œæ•ˆç‡å› å­
    lines.append("\nã€3. ç½‘ç»œæ•ˆç‡å› å­ã€‘(0-40åˆ†ï¼Œæƒé‡40%)")
    network = factors.get("network_efficiency", {})
    network_value = network.get("value", {})
    network_score = network.get("score", 0)
    
    avg_diameter = network_value.get("average_diameter", 0)
    avg_path_length = network_value.get("average_path_length", 0)
    
    early_diameter = earliest.get("diameter", 0)
    late_diameter = latest.get("diameter", 0)
    early_path = earliest.get("average_path_length", 0)
    late_path = latest.get("average_path_length", 0)
    
    lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"      é¦–æœˆç½‘ç»œç›´å¾„: {early_diameter:.1f}  â†’  æœ«æœˆç½‘ç»œç›´å¾„: {late_diameter:.1f}")
    lines.append(f"      é¦–æœˆå¹³å‡è·¯å¾„é•¿åº¦: {early_path:.2f}  â†’  æœ«æœˆå¹³å‡è·¯å¾„é•¿åº¦: {late_path:.2f}")
    lines.append(f"      æ•´ä½“å¹³å‡ç›´å¾„: {avg_diameter:.3f}")
    lines.append(f"      æ•´ä½“å¹³å‡è·¯å¾„é•¿åº¦: {avg_path_length:.3f}")
    
    # è®¡ç®—è¶‹åŠ¿
    diameter_values = [m.get("diameter", 0) for m in sorted_metrics]
    path_values = [m.get("average_path_length", 0) for m in sorted_metrics]
    
    if len(diameter_values) >= 3:
        early_dia_avg = sum(diameter_values[:3]) / min(3, len(diameter_values))
        recent_dia_avg = sum(diameter_values[-3:]) / min(3, len(diameter_values))
        change_dia = recent_dia_avg - early_dia_avg
        
        early_path_avg = sum(path_values[:3]) / min(3, len(path_values))
        recent_path_avg = sum(path_values[-3:]) / min(3, len(path_values))
        change_path = recent_path_avg - early_path_avg
        
        lines.append(f"      æ—©æœŸ3æœˆå¹³å‡ç›´å¾„: {early_dia_avg:.2f}  â†’  è¿‘æœŸ3æœˆå¹³å‡ç›´å¾„: {recent_dia_avg:.2f}")
        if change_dia < -0.1:
            lines.append(f"      âœ… ç›´å¾„å‡å°ï¼Œæ•ˆç‡æå‡ (å‡å°‘ {abs(change_dia):.2f})")
        elif change_dia > 0.1:
            lines.append(f"      âš ï¸ ç›´å¾„å¢å¤§ï¼Œæ•ˆç‡ä¸‹é™ (å¢åŠ  {change_dia:.2f})")
        else:
            lines.append(f"      â¡ï¸ ç›´å¾„ä¿æŒç¨³å®š")
        
        lines.append(f"      æ—©æœŸ3æœˆå¹³å‡è·¯å¾„: {early_path_avg:.2f}  â†’  è¿‘æœŸ3æœˆå¹³å‡è·¯å¾„: {recent_path_avg:.2f}")
        if change_path < -0.1:
            lines.append(f"      âœ… è·¯å¾„ç¼©çŸ­ï¼Œæ•ˆç‡æå‡ (å‡å°‘ {abs(change_path):.2f})")
        elif change_path > 0.1:
            lines.append(f"      âš ï¸ è·¯å¾„å¢é•¿ï¼Œæ•ˆç‡ä¸‹é™ (å¢åŠ  {change_path:.2f})")
        else:
            lines.append(f"      â¡ï¸ è·¯å¾„ä¿æŒç¨³å®š")
    
    # è§£é‡Šå½’ä¸€åŒ–é€»è¾‘
    diameter_decay_factor = 0.3
    path_decay_factor = 0.4
    
    if avg_diameter <= 1.0:
        dia_norm = 1.0
        dia_explanation = "1.0 (ç›´å¾„â‰¤1ï¼Œæœ€ä¼˜)"
    else:
        dia_norm = 1.0 / (1.0 + diameter_decay_factor * (avg_diameter - 1.0))
        dia_norm = max(0.05, dia_norm)
        dia_explanation = f"{dia_norm:.3f} (å¯¹æ•°è¡°å‡ï¼Œè¡°å‡å› å­={diameter_decay_factor})"
    
    if avg_path_length <= 1.0:
        path_norm = 1.0
        path_explanation = "1.0 (è·¯å¾„â‰¤1ï¼Œæœ€ä¼˜)"
    else:
        path_norm = 1.0 / (1.0 + path_decay_factor * (avg_path_length - 1.0))
        path_norm = max(0.05, path_norm)
        path_explanation = f"{path_norm:.3f} (å¯¹æ•°è¡°å‡ï¼Œè¡°å‡å› å­={path_decay_factor})"
    
    network_norm = 0.5 * dia_norm + 0.5 * path_norm
    
    lines.append(f"   â¡ï¸ å› å­å¾—åˆ†: {network_score:.2f} / 40")
    lines.append(f"      ç›´å¾„å½’ä¸€åŒ–: {dia_explanation}")
    lines.append(f"      è·¯å¾„å½’ä¸€åŒ–: {path_explanation}")
    lines.append(f"      ç»¼åˆå½’ä¸€åŒ–: {network_norm:.3f} (ç›´å¾„50% + è·¯å¾„50%)")
    lines.append(f"      (å½’ä¸€åŒ–å…¬å¼: å¯¹æ•°è¡°å‡å‡½æ•°ï¼Œé¿å…ç¡¬æˆªæ–­)")
    
    # æ±‡æ€»
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“‹ è¯„åˆ†æ±‡æ€»")
    lines.append("-" * 80)
    
    emotion_score = factors.get("emotion", {}).get("score", 0)
    clustering_score = factors.get("clustering", {}).get("score", 0)
    network_score = factors.get("network_efficiency", {}).get("score", 0)
    
    lines.append(f"   æƒ…ç»ªæ°›å›´å› å­:     {emotion_score:6.2f} / 20  (æƒé‡20%)")
    lines.append(f"   ç¤¾åŒºç´§å¯†åº¦å› å­:    {clustering_score:6.2f} / 40  (æƒé‡40%)")
    lines.append(f"   ç½‘ç»œæ•ˆç‡å› å­:      {network_score:6.2f} / 40  (æƒé‡40%)")
    lines.append(f"   " + "-" * 30)
    lines.append(f"   æ€»åˆ†:              {score:6.2f} / 100")
    
    # æœˆåº¦è¶‹åŠ¿
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“… æœˆåº¦æŒ‡æ ‡è¶‹åŠ¿")
    lines.append("-" * 80)
    lines.append(f"   {'æœˆä»½':<10} {'æƒ…ç»ª':>8} {'èšç±»ç³»æ•°':>10} {'ç›´å¾„':>8} {'è·¯å¾„é•¿åº¦':>10} {'èŠ‚ç‚¹æ•°':>8} {'è¾¹æ•°':>8}")
    lines.append("   " + "-" * 70)
    
    for m in sorted_metrics:
        month = m.get("month", "N/A")
        emotion = m.get("average_emotion", 0)
        clustering = m.get("average_local_clustering", 0)
        diameter = m.get("diameter", 0)
        path_length = m.get("average_path_length", 0)
        nodes = m.get("actor_graph_nodes", 0)
        edges = m.get("actor_graph_edges", 0)
        lines.append(f"   {month:<10} {emotion:>+8.3f} {clustering:>10.3f} {diameter:>8.1f} {path_length:>10.2f} {nodes:>8} {edges:>8}")
    
    lines.append("")
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆè¯¦ç»†ç¤¾åŒºæ°›å›´åˆ†ææŠ¥å‘Š")
    parser.add_argument(
        "--input",
        type=str,
        default="output/community-atmosphere-analysis/full_analysis.json",
        help="è¾“å…¥çš„å®Œæ•´åˆ†ææ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output/community-atmosphere-analysis/detailed_report.txt",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--repo",
        type=str,
        default=None,
        help="åªåˆ†ææŒ‡å®šçš„ä»“åº“ï¼ˆå¯ç”¨é€—å·åˆ†éš”å¤šä¸ªï¼‰"
    )
    parser.add_argument(
        "--top",
        type=int,
        default=None,
        help="åªè¾“å‡ºæ°›å›´è¯„åˆ†æœ€é«˜çš„å‰ N ä¸ªé¡¹ç›®"
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=None,
        help="åªè¾“å‡ºæ°›å›´è¯„åˆ†å¤§äºç­‰äºè¯¥å€¼çš„é¡¹ç›®"
    )
    parser.add_argument(
        "--max-score",
        type=float,
        default=None,
        help="åªè¾“å‡ºæ°›å›´è¯„åˆ†å°äºç­‰äºè¯¥å€¼çš„é¡¹ç›®"
    )
    
    args = parser.parse_args()
    
    # è¯»å–åˆ†ææ•°æ®
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    print(f"ğŸ“– è¯»å–åˆ†ææ•°æ®: {input_path}")
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # ç­›é€‰ä»“åº“
    repos_to_analyze = list(data.keys())
    
    if args.repo:
        specified_repos = [r.strip() for r in args.repo.split(",")]
        repos_to_analyze = [r for r in repos_to_analyze if r in specified_repos]
        if not repos_to_analyze:
            print(f"âŒ æœªæ‰¾åˆ°æŒ‡å®šçš„ä»“åº“: {args.repo}")
            return
    
    # æŒ‰æ°›å›´è¯„åˆ†æ’åº
    repos_with_scores = []
    for repo in repos_to_analyze:
        score = data[repo].get("atmosphere_score", {}).get("score", 0)
        repos_with_scores.append((repo, score))
    
    repos_with_scores.sort(key=lambda x: x[1], reverse=True)
    
    # ç­›é€‰æ¡ä»¶
    if args.min_score is not None:
        repos_with_scores = [(r, s) for r, s in repos_with_scores if s >= args.min_score]
    
    if args.max_score is not None:
        repos_with_scores = [(r, s) for r, s in repos_with_scores if s <= args.max_score]
    
    if args.top is not None:
        repos_with_scores = repos_with_scores[:args.top]
    
    if not repos_with_scores:
        print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„é¡¹ç›®")
        return
    
    print(f"ğŸ“Š å°†åˆ†æ {len(repos_with_scores)} ä¸ªé¡¹ç›®")
    
    # ç”ŸæˆæŠ¥å‘Š
    reports = []
    reports.append("=" * 80)
    reports.append("ğŸ” OSS é¡¹ç›®ç¤¾åŒºæ°›å›´è¯¦ç»†åˆ†ææŠ¥å‘Š")
    reports.append("=" * 80)
    reports.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    reports.append(f"åˆ†æé¡¹ç›®æ•°: {len(repos_with_scores)}")
    reports.append("")
    
    for repo, score in repos_with_scores:
        report = generate_repo_report(repo, data[repo])
        reports.append(report)
    
    full_report = "\n".join(reports)
    
    # è¾“å‡ºåˆ°æ–‡ä»¶
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(full_report)
    
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆå¦‚æœé¡¹ç›®æ•°å°‘äºç­‰äº3ï¼‰
    if len(repos_with_scores) <= 3:
        print("\n" + full_report)
    else:
        # åªè¾“å‡ºå‰3ä¸ª
        print("\nğŸ“‹ å‰ 3 ä¸ªé¡¹ç›®é¢„è§ˆ:\n")
        for repo, score in repos_with_scores[:3]:
            print(generate_repo_report(repo, data[repo]))


if __name__ == "__main__":
    main()
