"""
è¯¦ç»†ç¤¾åŒºæ°›å›´åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨

æŒ‰é¡¹ç›®è¾“å‡ºæ¯ä¸€é¡¹å¾—åˆ†çš„æ¥æºå’Œæ•°å€¼å˜åŒ–
åŸºäº full_analysis.json å’Œ summary.json ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š

è¯„åˆ†ä½“ç³»è¯´æ˜ï¼š
========================================
ç»¼åˆè¯„åˆ† = å¤§æ¨¡å‹è¯„åˆ†(40%) + èšç±»ç³»æ•°(30%) + ç½‘ç»œç›´å¾„(30%)
========================================

1. å¤§æ¨¡å‹è¯„åˆ† (40%)ï¼š
   - åŸºäºæ¯’æ€§æŒ‡æ ‡ï¼ˆToxiCRï¼‰å’Œ CHAOSS æŒ‡æ ‡
   - ç”± DeepSeek å¤§æ¨¡å‹ç»¼åˆåˆ†æåç»™å‡ºè¯„åˆ†
   - åŒ…å«æ¯’æ€§è¯„åˆ†å’Œå“åº”æ•ˆç‡è¯„åˆ†ä¸¤ä¸ªå­ç»´åº¦

2. èšç±»ç³»æ•° (30%)ï¼š
   - æŒ‡æ ‡ï¼šglobal_clustering_coefficient
   - æ„ä¹‰ï¼šç¤¾åŒºè¶Šç´§å¯†ï¼Œåä½œæ•ˆç‡è¶Šé«˜
   - æ­£å‘æŒ‡æ ‡ï¼šæ•°å€¼è¶Šå¤§è¶Šå¥½

3. ç½‘ç»œç›´å¾„ (30%)ï¼š
   - æŒ‡æ ‡ï¼šaverage_path_length
   - æ„ä¹‰ï¼šæ²Ÿé€šè·¯å¾„è¶ŠçŸ­ï¼Œä¿¡æ¯ä¼ é€’æ•ˆç‡è¶Šé«˜
   - è´Ÿå‘æŒ‡æ ‡ï¼šæ•°å€¼è¶Šå°è¶Šå¥½
"""

import json
import argparse
import sys
import io
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


def recalculate_score(metrics: List[Dict[str, Any]]) -> Dict[str, Any]:
    """æ ¹æ®metricsåˆ—è¡¨é‡æ–°è®¡ç®—è¯„åˆ† (4:3:3)"""
    if not metrics:
        return {"score": 0, "level": "unknown", "factors": {}}
    
    # æå–å¹¶æ˜ å°„å­—æ®µ
    tox_values = []
    resp_values = []
    close_values = []
    
    for m in metrics:
        # æ¯’æ€§: toxicity_ratio æˆ– toxic_rate_0_5
        tox = m.get("toxicity_ratio")
        if tox is None:
            tox = m.get("toxic_rate_0_5", 0.0)
        tox_values.append(tox)
        
        # å“åº”æ—¶é—´: avg_response_time æˆ– time_to_first_response_mean
        resp = m.get("avg_response_time")
        if resp is None:
            resp = m.get("time_to_first_response_mean", 0.0)
        resp_values.append(resp)
        
        # å…³é—­ç‡: closing_rate æˆ– change_request_closure_ratio
        close = m.get("closing_rate")
        if close is None:
            close = m.get("change_request_closure_ratio", 0.0)
        close_values.append(close)
    
    avg_toxicity = sum(tox_values) / len(tox_values)
    avg_response_time = sum(resp_values) / len(resp_values)
    avg_closing_rate = sum(close_values) / len(close_values)
    
    # è®¡ç®—å¾—åˆ†
    # 1) æ¯’æ€§ (40%)
    toxicity_score_raw = max(0.0, 1.0 - avg_toxicity / 0.05) * 100
    toxicity_weighted = toxicity_score_raw * 0.40
    
    # 2) å“åº”æ—¶é—´ (30%)
    response_score_raw = 100.0 / (1.0 + avg_response_time / 48.0)
    response_weighted = response_score_raw * 0.30
    
    # 3) å…³é—­ç‡ (30%)
    closing_score_raw = min(100.0, avg_closing_rate * 100.0)
    closing_weighted = closing_score_raw * 0.30
    
    total_score = toxicity_weighted + response_weighted + closing_weighted
    
    level = "poor"
    if total_score >= 80:
        level = "excellent"
    elif total_score >= 60:
        level = "good"
    elif total_score >= 40:
        level = "moderate"
        
    return {
        "score": total_score,
        "level": level,
        "factors": {
            "toxicity": {
                "value": avg_toxicity,
                "score": toxicity_score_raw,
                "weighted_score": toxicity_weighted
            },
            "response_time": {
                "value": avg_response_time,
                "score": response_score_raw,
                "weighted_score": response_weighted
            },
            "closing_rate": {
                "value": avg_closing_rate,
                "score": closing_score_raw,
                "weighted_score": closing_weighted
            }
        }
    }


def generate_repo_report(repo_name: str, repo_data: Dict[str, Any]) -> str:
    """ç”Ÿæˆå•ä¸ªä»“åº“çš„è¯¦ç»†æŠ¥å‘Š"""
    lines = []
    lines.append("=" * 80)
    lines.append(f"ğŸ“Š é¡¹ç›®: {repo_name}")
    lines.append("=" * 80)
    
    # è·å–æŒ‡æ ‡æ—¶é—´åºåˆ—
    metrics = repo_data.get("metrics", [])
    if len(metrics) < 2:
        lines.append("\nâš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æ")
        return "\n".join(lines)
        
    # é‡æ–°è®¡ç®—è¯„åˆ† (è¦†ç›–åŸæ¥çš„ atmosphere_score)
    atmosphere = recalculate_score(metrics)
    score = atmosphere.get("score", 0)
    level = atmosphere.get("level", "unknown")
    
    # è·å–åŸæ¥çš„å…ƒæ•°æ® (period, months)
    orig_atmosphere = repo_data.get("atmosphere_score", {})
    if not orig_atmosphere:
         orig_atmosphere = {}
    
    # å°è¯•ä» metrics æ¨æ–­
    months_list = [m.get("month") for m in metrics]
    months_list.sort()
    if months_list:
        period = f"{months_list[0]} to {months_list[-1]}"
        months = len(months_list)
    else:
        period = orig_atmosphere.get("period", "N/A")
        months = orig_atmosphere.get("months_analyzed", 0)
    
    # æ°›å›´ç­‰çº§å›¾æ ‡
    level_icons = {
        "excellent": "ğŸŸ¢ ä¼˜ç§€",
        "good": "ğŸŸ¢ è‰¯å¥½",
        "moderate": "ğŸŸ¡ ä¸­ç­‰",
        "poor": "ğŸ”´ è¾ƒå·®",
        "unknown": "âšª æœªçŸ¥"
    }
    
    lines.append(f"\nğŸ¯ ç»¼åˆæ°›å›´è¯„åˆ†: {score:.2f} / 100")
    lines.append(f"   æ°›å›´ç­‰çº§: {level_icons.get(level, level)}")
    lines.append(f"   åˆ†æå‘¨æœŸ: {period} ({months} ä¸ªæœˆ)")
    
    # æŒ‰æœˆä»½æ’åº
    sorted_metrics = sorted(metrics, key=lambda m: m.get("month", ""))
    earliest = sorted_metrics[0]
    latest = sorted_metrics[-1]
    
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“ˆ å„å› å­è¯¦ç»†åˆ†æï¼ˆä¸‰å¤§å› å­ï¼šæ¯’æ€§40% + å“åº”æ—¶é—´30% + å…³é—­ç‡30%ï¼‰")
    lines.append("-" * 80)
    
    factors = atmosphere.get("factors", {})
    
    # ========================================
    # 1. æ¯’æ€§å› å­ (40%)
    # ========================================
    lines.append("\nã€1. æ¯’æ€§å› å­ã€‘(0-40åˆ†ï¼Œæƒé‡40%)")
    tox_factor = factors.get("toxicity", {})
    
    # æ”¶é›†æ•°æ®
    tox_values = []
    for m in sorted_metrics:
        # å…¼å®¹æ–°æ—§å­—æ®µ
        v = m.get("toxicity_ratio")
        if v is None:
            v = m.get("toxic_rate_0_5", 0.0)
        tox_values.append(v)
        
    avg_tox = tox_factor.get("value", 0)
    early_tox = tox_values[0]
    late_tox = tox_values[-1]
    
    lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"      é¦–æœˆæ¯’æ€§å æ¯”: {early_tox:.2%}  â†’  æœ«æœˆæ¯’æ€§å æ¯”: {late_tox:.2%}")
    lines.append(f"      æ•´ä½“å¹³å‡æ¯’æ€§: {avg_tox:.2%}")
    
    lines.append(f"   â¡ï¸ å› å­å¾—åˆ†: {tox_factor.get('weighted_score', 0):.2f} / 40 (åŸå§‹åˆ†: {tox_factor.get('score', 0):.2f})")
    lines.append(f"      (ç›®æ ‡: 0%æ¯’æ€§ -> 100åˆ†)")

    # ========================================
    # 2. å“åº”æ—¶é—´å› å­ (30%)
    # ========================================
    lines.append("\nã€2. å“åº”æ—¶é—´å› å­ã€‘(0-30åˆ†ï¼Œæƒé‡30%)")
    resp_factor = factors.get("response_time", {})
    
    resp_values = []
    for m in sorted_metrics:
        v = m.get("avg_response_time")
        if v is None:
            v = m.get("time_to_first_response_mean") # ä¼˜å…ˆç”¨ mean
        if v is None:
            v = m.get("time_to_first_response_median", 0.0)
        resp_values.append(v)
        
    avg_resp = resp_factor.get("value", 0)
    early_resp = resp_values[0]
    late_resp = resp_values[-1]
    
    lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"      é¦–æœˆå¹³å‡å“åº”: {early_resp:.1f}h  â†’  æœ«æœˆå¹³å‡å“åº”: {late_resp:.1f}h")
    lines.append(f"      æ•´ä½“å¹³å‡å“åº”: {avg_resp:.1f}h")
    
    lines.append(f"   â¡ï¸ å› å­å¾—åˆ†: {resp_factor.get('weighted_score', 0):.2f} / 30 (åŸå§‹åˆ†: {resp_factor.get('score', 0):.2f})")

    # ========================================
    # 3. å…³é—­ç‡å› å­ (30%)
    # ========================================
    lines.append("\nã€3. å…³é—­ç‡å› å­ã€‘(0-30åˆ†ï¼Œæƒé‡30%)")
    close_factor = factors.get("closing_rate", {})
    
    close_values = []
    for m in sorted_metrics:
        v = m.get("closing_rate")
        if v is None:
            v = m.get("change_request_closure_ratio", 0.0)
        close_values.append(v)
        
    avg_close = close_factor.get("value", 0)
    early_close = close_values[0]
    late_close = close_values[-1]
    
    lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"      é¦–æœˆå…³é—­ç‡: {early_close:.2%}  â†’  æœ«æœˆå…³é—­ç‡: {late_close:.2%}")
    lines.append(f"      æ•´ä½“å¹³å‡å…³é—­ç‡: {avg_close:.2%}")
    
    lines.append(f"   â¡ï¸ å› å­å¾—åˆ†: {close_factor.get('weighted_score', 0):.2f} / 30 (åŸå§‹åˆ†: {close_factor.get('score', 0):.2f})")
    
    # ========================================
    # è¯„åˆ†æ±‡æ€»
    # ========================================
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“‹ è¯„åˆ†æ±‡æ€»")
    lines.append("-" * 80)
    
    lines.append(f"   æ¯’æ€§å› å­:         {tox_factor.get('weighted_score', 0):.2f} / 40  (æƒé‡40%)")
    lines.append(f"   å“åº”æ—¶é—´å› å­:      {resp_factor.get('weighted_score', 0):.2f} / 30  (æƒé‡30%)")
    lines.append(f"   å…³é—­ç‡å› å­:        {close_factor.get('weighted_score', 0):.2f} / 30  (æƒé‡30%)")
    lines.append(f"   " + "-" * 30)
    lines.append(f"   æ€»åˆ†:               {score:.2f} / 100")
    
    # ========================================
    # æœˆåº¦æŒ‡æ ‡è¶‹åŠ¿
    # ========================================
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“… æœˆåº¦æŒ‡æ ‡è¶‹åŠ¿")
    lines.append("-" * 80)
    
    # è¡¨å¤´
    lines.append(f"   {'æœˆä»½':<10} {'æ¯’æ€§':>10} {'å“åº”æ—¶é—´(h)':>15} {'å…³é—­ç‡':>12}")
    lines.append("   " + "-" * 70)
    
    for i, m in enumerate(sorted_metrics):
        month = m.get("month", "N/A")
        # Reuse collected values
        tox = tox_values[i]
        resp = resp_values[i]
        close = close_values[i]
        
        lines.append(f"   {month:<10} {tox:>10.2%} {resp:>15.1f} {close:>12.2%}")
    
    lines.append("")
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆè¯¦ç»†ç¤¾åŒºæ°›å›´åˆ†ææŠ¥å‘Š")
    parser.add_argument(
        "--input",
        type=str,
        default="output/community-atmosphere-analysis/full_analysis.json",
        help="è¾“å…¥çš„å®Œæ•´åˆ†ææ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--summary",
        type=str,
        default="output/community-atmosphere-analysis/summary.json",
        help="è¾“å…¥çš„æ‘˜è¦æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output/community-atmosphere-analysis/detailed_report.txt",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--repo",
        type=str,
        default=None,
        help="åªåˆ†ææŒ‡å®šçš„ä»“åº“ï¼ˆå¯ç”¨é€—å·åˆ†éš”å¤šä¸ªï¼‰"
    )
    parser.add_argument(
        "--top",
        type=int,
        default=None,
        help="åªè¾“å‡ºæ°›å›´è¯„åˆ†æœ€é«˜çš„å‰ N ä¸ªé¡¹ç›®"
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=None,
        help="åªè¾“å‡ºæ°›å›´è¯„åˆ†å¤§äºç­‰äºè¯¥å€¼çš„é¡¹ç›®"
    )
    parser.add_argument(
        "--max-score",
        type=float,
        default=None,
        help="åªè¾“å‡ºæ°›å›´è¯„åˆ†å°äºç­‰äºè¯¥å€¼çš„é¡¹ç›®"
    )
    parser.add_argument(
        "--level",
        type=str,
        choices=["excellent", "good", "moderate", "poor"],
        default=None,
        help="åªè¾“å‡ºæŒ‡å®šç­‰çº§çš„é¡¹ç›®"
    )
    
    args = parser.parse_args()
    
    # è¯»å–åˆ†ææ•°æ®
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    print(f"ğŸ“– è¯»å–åˆ†ææ•°æ®: {input_path}")
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # è¯»å–æ‘˜è¦æ•°æ®ï¼ˆç”¨äºæ’åºï¼‰
    summary_path = Path(args.summary)
    summary_data = []
    if summary_path.exists():
        print(f"ğŸ“– è¯»å–æ‘˜è¦æ•°æ®: {summary_path}")
        with open(summary_path, "r", encoding="utf-8") as f:
            summary_data = json.load(f)
    
    # ç­›é€‰ä»“åº“
    repos_to_analyze = list(data.keys())
    
    if args.repo:
        specified_repos = [r.strip() for r in args.repo.split(",")]
        repos_to_analyze = [r for r in repos_to_analyze if r in specified_repos]
        if not repos_to_analyze:
            print(f"âŒ æœªæ‰¾åˆ°æŒ‡å®šçš„ä»“åº“: {args.repo}")
            return
    
    # æŒ‰æ°›å›´è¯„åˆ†æ’åº
    repos_with_scores = []
    for repo in repos_to_analyze:
        # ä½¿ç”¨æ–°çš„è¯„åˆ†é€»è¾‘è¿›è¡Œæ’åº
        metrics = data[repo].get("metrics", [])
        new_atmosphere = recalculate_score(metrics)
        score = new_atmosphere.get("score", 0)
        level = new_atmosphere.get("level", "unknown")
        repos_with_scores.append((repo, score, level))
    
    repos_with_scores.sort(key=lambda x: x[1], reverse=True)
    
    # ç­›é€‰æ¡ä»¶
    if args.min_score is not None:
        repos_with_scores = [(r, s, l) for r, s, l in repos_with_scores if s >= args.min_score]
    
    if args.max_score is not None:
        repos_with_scores = [(r, s, l) for r, s, l in repos_with_scores if s <= args.max_score]
    
    if args.level is not None:
        repos_with_scores = [(r, s, l) for r, s, l in repos_with_scores if l == args.level]
    
    if args.top is not None:
        repos_with_scores = repos_with_scores[:args.top]
    
    if not repos_with_scores:
        print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„é¡¹ç›®")
        return
    
    print(f"ğŸ“Š å°†åˆ†æ {len(repos_with_scores)} ä¸ªé¡¹ç›®")
    
    # ç”ŸæˆæŠ¥å‘Š
    reports = []
    reports.append("=" * 80)
    reports.append("ğŸ” OSS é¡¹ç›®ç¤¾åŒºæ°›å›´è¯¦ç»†åˆ†ææŠ¥å‘Š")
    reports.append("=" * 80)
    reports.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    reports.append(f"åˆ†æé¡¹ç›®æ•°: {len(repos_with_scores)}")
    reports.append("")
    
    # è¯„åˆ†ä½“ç³»è¯´æ˜
    reports.append("ğŸ“ è¯„åˆ†ä½“ç³»è¯´æ˜:")
    reports.append("   ç»¼åˆè¯„åˆ† = æ¯’æ€§å› å­(40%) + å“åº”æ—¶é—´å› å­(30%) + å…³é—­ç‡å› å­(30%)")
    reports.append("")
    
    # æ‘˜è¦è¡¨æ ¼
    reports.append("-" * 80)
    reports.append("ğŸ“‹ é¡¹ç›®è¯„åˆ†æ‘˜è¦")
    reports.append("-" * 80)
    reports.append(f"   {'æ’å':<4} {'é¡¹ç›®åç§°':<40} {'è¯„åˆ†':>8} {'ç­‰çº§':<12}")
    reports.append("   " + "-" * 70)
    
    for idx, (repo, score, level) in enumerate(repos_with_scores, 1):
        level_icons = {
            "excellent": "ğŸŸ¢ä¼˜ç§€",
            "good": "ğŸŸ¢è‰¯å¥½",
            "moderate": "ğŸŸ¡ä¸­ç­‰",
            "poor": "ğŸ”´è¾ƒå·®",
            "insufficient_data": "âšªæ•°æ®ä¸è¶³",
        }
        level_str = level_icons.get(level, level)
        reports.append(f"   {idx:<4} {repo:<40} {score:>8.2f} {level_str:<12}")
    
    reports.append("")
    
    # è¯¦ç»†æŠ¥å‘Š
    for repo, score, level in repos_with_scores:
        report = generate_repo_report(repo, data[repo])
        reports.append(report)
    
    full_report = "\n".join(reports)
    
    # è¾“å‡ºåˆ°æ–‡ä»¶
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(full_report)
    
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆå¦‚æœé¡¹ç›®æ•°å°‘äºç­‰äº3ï¼‰
    if len(repos_with_scores) <= 3:
        print("\n" + full_report)
    else:
        # åªè¾“å‡ºå‰3ä¸ª
        print("\nğŸ“‹ å‰ 3 ä¸ªé¡¹ç›®é¢„è§ˆ:\n")
        for repo, score, level in repos_with_scores[:3]:
            print(generate_repo_report(repo, data[repo]))


if __name__ == "__main__":
    main()
