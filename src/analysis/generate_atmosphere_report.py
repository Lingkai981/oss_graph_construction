"""
è¯¦ç»†ç¤¾åŒºæ°›å›´åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨

æŒ‰é¡¹ç›®è¾“å‡ºæ¯ä¸€é¡¹å¾—åˆ†çš„æ¥æºå’Œæ•°å€¼å˜åŒ–
åŸºäº full_analysis.json å’Œ summary.json ç”Ÿæˆè¯¦ç»†åˆ†ææŠ¥å‘Š

è¯„åˆ†ä½“ç³»è¯´æ˜ï¼š
========================================
ç»¼åˆè¯„åˆ† = å¤§æ¨¡å‹è¯„åˆ†(40%) + èšç±»ç³»æ•°(30%) + ç½‘ç»œç›´å¾„(30%)
========================================

1. å¤§æ¨¡å‹è¯„åˆ† (40%)ï¼š
   - åŸºäºæ¯’æ€§æŒ‡æ ‡ï¼ˆToxiCRï¼‰å’Œ CHAOSS æŒ‡æ ‡
   - ç”± DeepSeek å¤§æ¨¡å‹ç»¼åˆåˆ†æåç»™å‡ºè¯„åˆ†
   - åŒ…å«æ¯’æ€§è¯„åˆ†å’Œå“åº”æ•ˆç‡è¯„åˆ†ä¸¤ä¸ªå­ç»´åº¦

2. èšç±»ç³»æ•° (30%)ï¼š
   - æŒ‡æ ‡ï¼šglobal_clustering_coefficient
   - æ„ä¹‰ï¼šç¤¾åŒºè¶Šç´§å¯†ï¼Œåä½œæ•ˆç‡è¶Šé«˜
   - æ­£å‘æŒ‡æ ‡ï¼šæ•°å€¼è¶Šå¤§è¶Šå¥½

3. ç½‘ç»œç›´å¾„ (30%)ï¼š
   - æŒ‡æ ‡ï¼šaverage_path_length
   - æ„ä¹‰ï¼šæ²Ÿé€šè·¯å¾„è¶ŠçŸ­ï¼Œä¿¡æ¯ä¼ é€’æ•ˆç‡è¶Šé«˜
   - è´Ÿå‘æŒ‡æ ‡ï¼šæ•°å€¼è¶Šå°è¶Šå¥½
"""

import json
import argparse
import sys
import io
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


def generate_repo_report(repo_name: str, repo_data: Dict[str, Any]) -> str:
    """ç”Ÿæˆå•ä¸ªä»“åº“çš„è¯¦ç»†æŠ¥å‘Š"""
    lines = []
    lines.append("=" * 80)
    lines.append(f"ğŸ“Š é¡¹ç›®: {repo_name}")
    lines.append("=" * 80)
    
    # è·å–æ°›å›´è¯„åˆ†
    atmosphere = repo_data.get("atmosphere_score", {})
    score = atmosphere.get("score", 0)
    level = atmosphere.get("level", "unknown")
    period = atmosphere.get("period", "N/A")
    months = atmosphere.get("months_analyzed", 0)
    
    # æ°›å›´ç­‰çº§å›¾æ ‡
    level_icons = {
        "excellent": "ğŸŸ¢ ä¼˜ç§€",
        "good": "ğŸŸ¢ è‰¯å¥½",
        "moderate": "ğŸŸ¡ ä¸­ç­‰",
        "poor": "ğŸ”´ è¾ƒå·®",
        "insufficient_data": "âšª æ•°æ®ä¸è¶³",
        "unknown": "âšª æœªçŸ¥"
    }
    
    lines.append(f"\nğŸ¯ ç»¼åˆæ°›å›´è¯„åˆ†: {score:.2f} / 100")
    lines.append(f"   æ°›å›´ç­‰çº§: {level_icons.get(level, level)}")
    lines.append(f"   åˆ†æå‘¨æœŸ: {period} ({months} ä¸ªæœˆ)")
    
    # è·å–æŒ‡æ ‡æ—¶é—´åºåˆ—
    metrics = repo_data.get("metrics", [])
    if len(metrics) < 3:
        lines.append("\nâš ï¸ æ•°æ®ä¸è¶³ï¼ˆå°‘äº3ä¸ªæœˆï¼‰ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æ")
        return "\n".join(lines)
    
    # æŒ‰æœˆä»½æ’åº
    sorted_metrics = sorted(metrics, key=lambda m: m.get("month", ""))
    earliest = sorted_metrics[0]
    latest = sorted_metrics[-1]
    
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“ˆ å„å› å­è¯¦ç»†åˆ†æï¼ˆä¸‰å±‚æ¶æ„ï¼šé•¿æœŸè¶‹åŠ¿40% + è¿‘æœŸçŠ¶æ€40% + ç¨³å®šæ€§20%ï¼‰")
    lines.append("-" * 80)
    
    factors = atmosphere.get("factors", {})
    weights = atmosphere.get("weights", {"llm": 0.4, "clustering": 0.3, "diameter": 0.3})
    
    # ========================================
    # 1. å¤§æ¨¡å‹è¯„åˆ†å› å­ (40%)
    # ========================================
    lines.append("\nã€1. å¤§æ¨¡å‹è¯„åˆ†å› å­ã€‘(æƒé‡40%)")
    llm_score_factor = factors.get("llm_score", 0)
    
    # æ”¶é›† LLM è¯„åˆ†æ•°æ®
    llm_scores = [m.get("llm_score", 0) for m in sorted_metrics]
    has_llm_data = any(s > 0 for s in llm_scores)
    
    if has_llm_data:
        early_llm = sorted_metrics[0].get("llm_score", 0)
        late_llm = sorted_metrics[-1].get("llm_score", 0)
        
        lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        lines.append(f"      é¦–æœˆ LLM è¯„åˆ†: {early_llm:.1f}  â†’  æœ«æœˆ LLM è¯„åˆ†: {late_llm:.1f}")
        
        # è®¡ç®—è¶‹åŠ¿
        if len(llm_scores) >= 3:
            early_avg = sum(llm_scores[:3]) / 3
            recent_avg = sum(llm_scores[-3:]) / 3
            change = recent_avg - early_avg
            lines.append(f"      æ—©æœŸ3æœˆå‡å€¼: {early_avg:.1f}  â†’  è¿‘æœŸ3æœˆå‡å€¼: {recent_avg:.1f}")
            if change > 0:
                lines.append(f"      âœ… LLM è¯„åˆ†è¶‹åŠ¿å‘å¥½ (æå‡ {change:+.1f})")
            elif change < 0:
                lines.append(f"      âš ï¸ LLM è¯„åˆ†è¶‹åŠ¿ä¸‹é™ (ä¸‹é™ {abs(change):.1f})")
            else:
                lines.append(f"      â¡ï¸ LLM è¯„åˆ†ä¿æŒç¨³å®š")
        
        # å±•ç¤ºæ¯’æ€§å’Œå“åº”å­ç»´åº¦
        lines.append(f"\n   ğŸ“‹ å­ç»´åº¦åˆ†æ:")
        
        # æ¯’æ€§åˆ†æ
        toxicity_means = [m.get("toxicity_mean", 0) for m in sorted_metrics]
        toxic_rates = [m.get("toxic_rate_0_5", 0) for m in sorted_metrics]
        avg_toxicity = sum(toxicity_means) / len(toxicity_means)
        avg_toxic_rate = sum(toxic_rates) / len(toxic_rates)
        
        early_toxicity = toxicity_means[0]
        late_toxicity = toxicity_means[-1]
        
        lines.append(f"      [æ¯’æ€§åˆ†æ]")
        lines.append(f"         é¦–æœˆå¹³å‡æ¯’æ€§: {early_toxicity:.4f}  â†’  æœ«æœˆå¹³å‡æ¯’æ€§: {late_toxicity:.4f}")
        lines.append(f"         æ•´ä½“å¹³å‡æ¯’æ€§: {avg_toxicity:.4f}")
        lines.append(f"         å¹³å‡é«˜æ¯’æ€§è¯„è®ºå æ¯”: {avg_toxic_rate:.2%}")
        
        # å“åº”æ—¶é—´åˆ†æ
        response_times = [m.get("time_to_first_response_median", 0) for m in sorted_metrics]
        closure_ratios = [m.get("change_request_closure_ratio", 0) for m in sorted_metrics]
        avg_response = sum(response_times) / len(response_times)
        avg_closure = sum(closure_ratios) / len(closure_ratios)
        
        early_response = response_times[0]
        late_response = response_times[-1]
        
        lines.append(f"      [å“åº”æ•ˆç‡]")
        lines.append(f"         é¦–æœˆå“åº”æ—¶é—´ä¸­ä½æ•°: {early_response:.1f}h  â†’  æœ«æœˆ: {late_response:.1f}h")
        lines.append(f"         æ•´ä½“å¹³å‡å“åº”æ—¶é—´: {avg_response:.1f}h")
        lines.append(f"         å¹³å‡å˜æ›´è¯·æ±‚å…³é—­ç‡: {avg_closure:.2f}")
        
        # LLM ç»™å‡ºçš„ç†ç”±ï¼ˆå–æœ€æ–°æœˆä»½çš„ï¼‰
        latest_reason = latest.get("llm_overall_reason", "")
        if latest_reason:
            lines.append(f"\n   ğŸ’¬ LLM è¯„ä»· (æœ€æ–°æœˆä»½):")
            # å°†ç†ç”±æŒ‰è¡Œæ˜¾ç¤ºï¼Œæ¯è¡Œæœ€å¤š60å­—ç¬¦
            reason_lines = [latest_reason[i:i+60] for i in range(0, len(latest_reason), 60)]
            for rl in reason_lines[:3]:  # æœ€å¤šæ˜¾ç¤º3è¡Œ
                lines.append(f"      {rl}")
    else:
        lines.append(f"   âš ï¸ æ—  LLM è¯„åˆ†æ•°æ®ï¼ˆAPI æœªé…ç½®æˆ–è¯„åˆ†å¤±è´¥ï¼‰")
        lines.append(f"   â¡ï¸ è¯¥ç»´åº¦ä½¿ç”¨å ä½å€¼ 0ï¼Œä»…ä½¿ç”¨èšç±»ç³»æ•°å’Œç½‘ç»œç›´å¾„è¿›è¡Œè¯„åˆ†")
    
    lines.append(f"\n   â¡ï¸ å› å­å¾—åˆ†: {llm_score_factor:.2f} (ä¸‰å±‚åˆ†æåçš„ç»¼åˆåˆ†ï¼Œæ»¡åˆ†100)")
    
    # ========================================
    # 2. èšç±»ç³»æ•°å› å­ (30%)
    # ========================================
    lines.append("\nã€2. èšç±»ç³»æ•°å› å­ã€‘(æƒé‡30%)")
    clustering_score_factor = factors.get("clustering_score", 0)
    
    clustering_values = [m.get("global_clustering_coefficient", 0) for m in sorted_metrics]
    avg_clustering = sum(clustering_values) / len(clustering_values)
    
    early_clustering = earliest.get("global_clustering_coefficient", 0)
    late_clustering = latest.get("global_clustering_coefficient", 0)
    
    lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"      é¦–æœˆèšç±»ç³»æ•°: {early_clustering:.4f}  â†’  æœ«æœˆèšç±»ç³»æ•°: {late_clustering:.4f}")
    lines.append(f"      æ•´ä½“å¹³å‡èšç±»ç³»æ•°: {avg_clustering:.4f} (èŒƒå›´: 0.0 åˆ° 1.0)")
    
    # è®¡ç®—è¶‹åŠ¿
    if len(clustering_values) >= 3:
        early_avg = sum(clustering_values[:3]) / 3
        recent_avg = sum(clustering_values[-3:]) / 3
        change = recent_avg - early_avg
        lines.append(f"      æ—©æœŸ3æœˆå‡å€¼: {early_avg:.4f}  â†’  è¿‘æœŸ3æœˆå‡å€¼: {recent_avg:.4f}")
        if change > 0.01:
            lines.append(f"      âœ… èšç±»ç³»æ•°æå‡ (æå‡ {change:+.4f})")
        elif change < -0.01:
            lines.append(f"      âš ï¸ èšç±»ç³»æ•°ä¸‹é™ (ä¸‹é™ {abs(change):.4f})")
        else:
            lines.append(f"      â¡ï¸ èšç±»ç³»æ•°ä¿æŒç¨³å®š")
    
    # ç¤¾åŒºè§„æ¨¡ä¿¡æ¯
    avg_nodes = sum(m.get("actor_graph_nodes", 0) for m in sorted_metrics) / len(sorted_metrics)
    avg_edges = sum(m.get("actor_graph_edges", 0) for m in sorted_metrics) / len(sorted_metrics)
    lines.append(f"      å¹³å‡å‚ä¸è€…æ•°é‡: {avg_nodes:.0f} äºº")
    lines.append(f"      å¹³å‡åä½œè¾¹æ•°é‡: {avg_edges:.0f} æ¡")
    
    lines.append(f"\n   â¡ï¸ å› å­å¾—åˆ†: {clustering_score_factor:.2f} (ä¸‰å±‚åˆ†æåçš„ç»¼åˆåˆ†ï¼Œæ»¡åˆ†100)")
    lines.append(f"      (æ­£å‘æŒ‡æ ‡ï¼šèšç±»ç³»æ•°è¶Šé«˜ï¼Œç¤¾åŒºè¶Šç´§å¯†ï¼Œå¾—åˆ†è¶Šé«˜)")
    
    # ========================================
    # 3. ç½‘ç»œç›´å¾„å› å­ (30%)
    # ========================================
    lines.append("\nã€3. ç½‘ç»œç›´å¾„å› å­ã€‘(æƒé‡30%)")
    diameter_score_factor = factors.get("diameter_score", 0)
    
    path_values = [m.get("average_path_length", 0) for m in sorted_metrics]
    diameter_values = [m.get("diameter", 0) for m in sorted_metrics]
    avg_path = sum(path_values) / len(path_values)
    avg_diameter = sum(diameter_values) / len(diameter_values)
    
    early_path = earliest.get("average_path_length", 0)
    late_path = latest.get("average_path_length", 0)
    early_diameter = earliest.get("diameter", 0)
    late_diameter = latest.get("diameter", 0)
    
    lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"      é¦–æœˆå¹³å‡è·¯å¾„é•¿åº¦: {early_path:.2f}  â†’  æœ«æœˆå¹³å‡è·¯å¾„é•¿åº¦: {late_path:.2f}")
    lines.append(f"      é¦–æœˆç½‘ç»œç›´å¾„: {early_diameter:.0f}  â†’  æœ«æœˆç½‘ç»œç›´å¾„: {late_diameter:.0f}")
    lines.append(f"      æ•´ä½“å¹³å‡è·¯å¾„é•¿åº¦: {avg_path:.2f}")
    lines.append(f"      æ•´ä½“å¹³å‡ç½‘ç»œç›´å¾„: {avg_diameter:.1f}")
    
    # è®¡ç®—è¶‹åŠ¿
    if len(path_values) >= 3:
        early_avg = sum(path_values[:3]) / 3
        recent_avg = sum(path_values[-3:]) / 3
        change = recent_avg - early_avg
        lines.append(f"      æ—©æœŸ3æœˆå‡å€¼: {early_avg:.2f}  â†’  è¿‘æœŸ3æœˆå‡å€¼: {recent_avg:.2f}")
        if change < -0.1:
            lines.append(f"      âœ… è·¯å¾„é•¿åº¦ç¼©çŸ­ (å‡å°‘ {abs(change):.2f})")
        elif change > 0.1:
            lines.append(f"      âš ï¸ è·¯å¾„é•¿åº¦å¢åŠ  (å¢åŠ  {change:.2f})")
        else:
            lines.append(f"      â¡ï¸ è·¯å¾„é•¿åº¦ä¿æŒç¨³å®š")
    
    # è¿é€šæ€§ä¿¡æ¯
    connected_count = sum(1 for m in sorted_metrics if m.get("is_connected", False))
    lines.append(f"      å®Œå…¨è¿é€šæœˆä»½: {connected_count}/{len(sorted_metrics)} ä¸ªæœˆ")
    
    lines.append(f"\n   â¡ï¸ å› å­å¾—åˆ†: {diameter_score_factor:.2f} (ä¸‰å±‚åˆ†æåçš„ç»¼åˆåˆ†ï¼Œæ»¡åˆ†100)")
    lines.append(f"      (è´Ÿå‘æŒ‡æ ‡ï¼šè·¯å¾„é•¿åº¦è¶ŠçŸ­ï¼Œæ²Ÿé€šæ•ˆç‡è¶Šé«˜ï¼Œå¾—åˆ†è¶Šé«˜)")
    
    # ========================================
    # è¯„åˆ†æ±‡æ€»
    # ========================================
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“‹ è¯„åˆ†æ±‡æ€»")
    lines.append("-" * 80)
    
    llm_weight = weights.get("llm", 0.4)
    clustering_weight = weights.get("clustering", 0.3)
    diameter_weight = weights.get("diameter", 0.3)
    
    lines.append(f"   å¤§æ¨¡å‹è¯„åˆ†å› å­:   {llm_score_factor:6.2f} Ã— {llm_weight:.0%} = {llm_score_factor * llm_weight:6.2f}")
    lines.append(f"   èšç±»ç³»æ•°å› å­:     {clustering_score_factor:6.2f} Ã— {clustering_weight:.0%} = {clustering_score_factor * clustering_weight:6.2f}")
    lines.append(f"   ç½‘ç»œç›´å¾„å› å­:     {diameter_score_factor:6.2f} Ã— {diameter_weight:.0%} = {diameter_score_factor * diameter_weight:6.2f}")
    lines.append(f"   " + "-" * 40)
    lines.append(f"   æ€»åˆ†:                              {score:6.2f} / 100")
    
    lines.append(f"\n   ğŸ“ åˆ†ææ–¹æ³•: ä¸‰å±‚æ¶æ„ (é•¿æœŸè¶‹åŠ¿40% + è¿‘æœŸçŠ¶æ€40% + ç¨³å®šæ€§20%)")
    
    # ========================================
    # CHAOSS æŒ‡æ ‡è¯¦æƒ…
    # ========================================
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“Š CHAOSS ç¤¾åŒºå¥åº·æŒ‡æ ‡è¯¦æƒ…")
    lines.append("-" * 80)
    
    # å˜æ›´è¯·æ±‚ç»Ÿè®¡
    total_opened_prs = sum(m.get("opened_prs", 0) for m in sorted_metrics)
    total_closed_prs = sum(m.get("closed_prs", 0) for m in sorted_metrics)
    total_opened_issues = sum(m.get("opened_issues", 0) for m in sorted_metrics)
    total_closed_issues = sum(m.get("closed_issues", 0) for m in sorted_metrics)
    
    lines.append(f"   [å˜æ›´è¯·æ±‚å…³é—­ç‡]")
    lines.append(f"      ç´¯è®¡æ‰“å¼€ PR: {total_opened_prs}  â†’  ç´¯è®¡å…³é—­ PR: {total_closed_prs}")
    lines.append(f"      ç´¯è®¡æ‰“å¼€ Issue: {total_opened_issues}  â†’  ç´¯è®¡å…³é—­ Issue: {total_closed_issues}")
    if total_opened_prs + total_opened_issues > 0:
        overall_closure = (total_closed_prs + total_closed_issues) / (total_opened_prs + total_opened_issues)
        lines.append(f"      æ•´ä½“å…³é—­ç‡: {overall_closure:.2f}")
    
    # å“åº”æ—¶é—´ç»Ÿè®¡
    response_medians = [m.get("time_to_first_response_median", 0) for m in sorted_metrics if m.get("time_to_first_response_median", 0) > 0]
    if response_medians:
        lines.append(f"\n   [é¦–æ¬¡å“åº”æ—¶é—´]")
        lines.append(f"      ä¸­ä½æ•°èŒƒå›´: {min(response_medians):.1f}h ~ {max(response_medians):.1f}h")
        lines.append(f"      å¹³å‡ä¸­ä½æ•°: {sum(response_medians)/len(response_medians):.1f}h")
    
    # ========================================
    # æœˆåº¦æŒ‡æ ‡è¶‹åŠ¿
    # ========================================
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“… æœˆåº¦æŒ‡æ ‡è¶‹åŠ¿")
    lines.append("-" * 80)
    
    # è¡¨å¤´
    lines.append(f"   {'æœˆä»½':<10} {'LLMåˆ†':>6} {'æ¯’æ€§':>8} {'èšç±»ç³»æ•°':>10} {'è·¯å¾„é•¿åº¦':>10} {'å“åº”æ—¶é—´':>10} {'å…³é—­ç‡':>8}")
    lines.append("   " + "-" * 72)
    
    for m in sorted_metrics:
        month = m.get("month", "N/A")
        llm = m.get("llm_score", 0)
        toxicity = m.get("toxicity_mean", 0)
        clustering = m.get("global_clustering_coefficient", 0)
        path_length = m.get("average_path_length", 0)
        response = m.get("time_to_first_response_median", 0)
        closure = m.get("change_request_closure_ratio", 0)
        
        lines.append(f"   {month:<10} {llm:>6.0f} {toxicity:>8.4f} {clustering:>10.4f} {path_length:>10.2f} {response:>9.1f}h {closure:>8.2f}")
    
    lines.append("")
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆè¯¦ç»†ç¤¾åŒºæ°›å›´åˆ†ææŠ¥å‘Š")
    parser.add_argument(
        "--input",
        type=str,
        default="output/community-atmosphere-analysis/full_analysis.json",
        help="è¾“å…¥çš„å®Œæ•´åˆ†ææ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--summary",
        type=str,
        default="output/community-atmosphere-analysis/summary.json",
        help="è¾“å…¥çš„æ‘˜è¦æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output/community-atmosphere-analysis/detailed_report.txt",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--repo",
        type=str,
        default=None,
        help="åªåˆ†ææŒ‡å®šçš„ä»“åº“ï¼ˆå¯ç”¨é€—å·åˆ†éš”å¤šä¸ªï¼‰"
    )
    parser.add_argument(
        "--top",
        type=int,
        default=None,
        help="åªè¾“å‡ºæ°›å›´è¯„åˆ†æœ€é«˜çš„å‰ N ä¸ªé¡¹ç›®"
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=None,
        help="åªè¾“å‡ºæ°›å›´è¯„åˆ†å¤§äºç­‰äºè¯¥å€¼çš„é¡¹ç›®"
    )
    parser.add_argument(
        "--max-score",
        type=float,
        default=None,
        help="åªè¾“å‡ºæ°›å›´è¯„åˆ†å°äºç­‰äºè¯¥å€¼çš„é¡¹ç›®"
    )
    parser.add_argument(
        "--level",
        type=str,
        choices=["excellent", "good", "moderate", "poor"],
        default=None,
        help="åªè¾“å‡ºæŒ‡å®šç­‰çº§çš„é¡¹ç›®"
    )
    
    args = parser.parse_args()
    
    # è¯»å–åˆ†ææ•°æ®
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    print(f"ğŸ“– è¯»å–åˆ†ææ•°æ®: {input_path}")
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # è¯»å–æ‘˜è¦æ•°æ®ï¼ˆç”¨äºæ’åºï¼‰
    summary_path = Path(args.summary)
    summary_data = []
    if summary_path.exists():
        print(f"ğŸ“– è¯»å–æ‘˜è¦æ•°æ®: {summary_path}")
        with open(summary_path, "r", encoding="utf-8") as f:
            summary_data = json.load(f)
    
    # ç­›é€‰ä»“åº“
    repos_to_analyze = list(data.keys())
    
    if args.repo:
        specified_repos = [r.strip() for r in args.repo.split(",")]
        repos_to_analyze = [r for r in repos_to_analyze if r in specified_repos]
        if not repos_to_analyze:
            print(f"âŒ æœªæ‰¾åˆ°æŒ‡å®šçš„ä»“åº“: {args.repo}")
            return
    
    # æŒ‰æ°›å›´è¯„åˆ†æ’åº
    repos_with_scores = []
    for repo in repos_to_analyze:
        repo_data = data[repo]
        score = repo_data.get("atmosphere_score", {}).get("score", 0)
        level = repo_data.get("atmosphere_score", {}).get("level", "unknown")
        repos_with_scores.append((repo, score, level))
    
    repos_with_scores.sort(key=lambda x: x[1], reverse=True)
    
    # ç­›é€‰æ¡ä»¶
    if args.min_score is not None:
        repos_with_scores = [(r, s, l) for r, s, l in repos_with_scores if s >= args.min_score]
    
    if args.max_score is not None:
        repos_with_scores = [(r, s, l) for r, s, l in repos_with_scores if s <= args.max_score]
    
    if args.level is not None:
        repos_with_scores = [(r, s, l) for r, s, l in repos_with_scores if l == args.level]
    
    if args.top is not None:
        repos_with_scores = repos_with_scores[:args.top]
    
    if not repos_with_scores:
        print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„é¡¹ç›®")
        return
    
    print(f"ğŸ“Š å°†åˆ†æ {len(repos_with_scores)} ä¸ªé¡¹ç›®")
    
    # ç”ŸæˆæŠ¥å‘Š
    reports = []
    reports.append("=" * 80)
    reports.append("ğŸ” OSS é¡¹ç›®ç¤¾åŒºæ°›å›´è¯¦ç»†åˆ†ææŠ¥å‘Š")
    reports.append("=" * 80)
    reports.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    reports.append(f"åˆ†æé¡¹ç›®æ•°: {len(repos_with_scores)}")
    reports.append("")
    
    # è¯„åˆ†ä½“ç³»è¯´æ˜
    reports.append("ğŸ“ è¯„åˆ†ä½“ç³»è¯´æ˜:")
    reports.append("   ç»¼åˆè¯„åˆ† = å¤§æ¨¡å‹è¯„åˆ†(40%) + èšç±»ç³»æ•°(30%) + ç½‘ç»œç›´å¾„(30%)")
    reports.append("   æ¯ä¸ªç»´åº¦ä½¿ç”¨ä¸‰å±‚åˆ†æï¼šé•¿æœŸè¶‹åŠ¿(40%) + è¿‘æœŸçŠ¶æ€(40%) + ç¨³å®šæ€§(20%)")
    reports.append("")
    
    # æ‘˜è¦è¡¨æ ¼
    reports.append("-" * 80)
    reports.append("ğŸ“‹ é¡¹ç›®è¯„åˆ†æ‘˜è¦")
    reports.append("-" * 80)
    reports.append(f"   {'æ’å':<4} {'é¡¹ç›®åç§°':<40} {'è¯„åˆ†':>8} {'ç­‰çº§':<12}")
    reports.append("   " + "-" * 70)
    
    for idx, (repo, score, level) in enumerate(repos_with_scores, 1):
        level_icons = {
            "excellent": "ğŸŸ¢ä¼˜ç§€",
            "good": "ğŸŸ¢è‰¯å¥½",
            "moderate": "ğŸŸ¡ä¸­ç­‰",
            "poor": "ğŸ”´è¾ƒå·®",
            "insufficient_data": "âšªæ•°æ®ä¸è¶³",
        }
        level_str = level_icons.get(level, level)
        reports.append(f"   {idx:<4} {repo:<40} {score:>8.2f} {level_str:<12}")
    
    reports.append("")
    
    # è¯¦ç»†æŠ¥å‘Š
    for repo, score, level in repos_with_scores:
        report = generate_repo_report(repo, data[repo])
        reports.append(report)
    
    full_report = "\n".join(reports)
    
    # è¾“å‡ºåˆ°æ–‡ä»¶
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(full_report)
    
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆå¦‚æœé¡¹ç›®æ•°å°‘äºç­‰äº3ï¼‰
    if len(repos_with_scores) <= 3:
        print("\n" + full_report)
    else:
        # åªè¾“å‡ºå‰3ä¸ª
        print("\nğŸ“‹ å‰ 3 ä¸ªé¡¹ç›®é¢„è§ˆ:\n")
        for repo, score, level in repos_with_scores[:3]:
            print(generate_repo_report(repo, data[repo]))


if __name__ == "__main__":
    main()
