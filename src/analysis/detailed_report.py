"""
è¯¦ç»†å€¦æ€ åˆ†ææŠ¥å‘Šç”Ÿæˆå™¨

æŒ‰é¡¹ç›®è¾“å‡ºæ¯ä¸€é¡¹å¾—åˆ†çš„æ¥æºå’Œæ•°å€¼å˜åŒ–
"""

import json
import argparse
from pathlib import Path
from typing import Dict, Any, List, Callable


def generate_repo_report(repo_name: str, repo_data: Dict[str, Any]) -> str:
    """ç”Ÿæˆå•ä¸ªä»“åº“çš„è¯¦ç»†æŠ¥å‘Š"""
    lines = []
    lines.append("=" * 80)
    lines.append(f"ğŸ“Š é¡¹ç›®: {repo_name}")
    lines.append("=" * 80)
    
    # è·å–å€¦æ€ è¯„åˆ†
    burnout = repo_data.get("burnout_score", {})
    score = burnout.get("score", 0)
    level = burnout.get("level", "unknown")
    period = burnout.get("period", "N/A")
    months = burnout.get("months_analyzed", 0)
    
    # é£é™©ç­‰çº§å›¾æ ‡
    level_icons = {
        "high": "ğŸ”´ é«˜é£é™©",
        "medium": "ğŸŸ  ä¸­é£é™©", 
        "low": "ğŸŸ¡ ä½é£é™©",
        "healthy": "ğŸŸ¢ å¥åº·",
        "unknown": "âšª æœªçŸ¥"
    }
    
    lines.append(f"\nğŸ¯ ç»¼åˆå€¦æ€ è¯„åˆ†: {score:.2f} / 100")
    lines.append(f"   é£é™©ç­‰çº§: {level_icons.get(level, level)}")
    lines.append(f"   åˆ†æå‘¨æœŸ: {period} ({months} ä¸ªæœˆ)")
    
    # è·å–æŒ‡æ ‡æ—¶é—´åºåˆ—
    metrics = repo_data.get("metrics", [])
    if len(metrics) < 2:
        lines.append("\nâš ï¸ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æ")
        return "\n".join(lines)
    
    # æŒ‰æœˆä»½æ’åº
    sorted_metrics = sorted(metrics, key=lambda m: m.get("month", ""))
    earliest = sorted_metrics[0]
    latest = sorted_metrics[-1]
    
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“ˆ å„å› å­è¯¦ç»†åˆ†æï¼ˆä¸‰å±‚æ¶æ„ï¼šé•¿æœŸè¶‹åŠ¿40% + è¿‘æœŸçŠ¶æ€40% + ç¨³å®šæ€§20%ï¼‰")
    lines.append("-" * 80)
    
    factors = burnout.get("factors", {})
    
    def format_dimension(name: str, dim_data: Dict, early_val: float, late_val: float, unit: str = ""):
        """æ ¼å¼åŒ–å•ä¸ªç»´åº¦çš„åˆ†æ"""
        dim_lines = []
        score = dim_data.get("score", 0)
        
        # é•¿æœŸè¶‹åŠ¿
        trend = dim_data.get("long_term_trend", {})
        slope_pct = trend.get("slope_percent_per_month", 0)
        trend_score = trend.get("score", 0)
        
        # è¿‘æœŸçŠ¶æ€
        recent = dim_data.get("recent_state", {})
        early_avg = recent.get("early_avg", 0)
        recent_avg = recent.get("recent_avg", 0)
        change_pct = recent.get("change_percent", 0)
        recent_score = recent.get("score", 0)
        
        # ç¨³å®šæ€§
        stability = dim_data.get("stability", {})
        volatility_pct = stability.get("volatility_percent", 0)
        stability_score = stability.get("score", 0)
        
        dim_lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        dim_lines.append(f"      é¦–æœˆ: {early_val:.2f}{unit}  â†’  æœ«æœˆ: {late_val:.2f}{unit}")
        
        dim_lines.append(f"   ğŸ“‰ é•¿æœŸè¶‹åŠ¿ (40%æƒé‡):")
        dim_lines.append(f"      çº¿æ€§å›å½’æ–œç‡: {slope_pct:+.2f}%/æœˆ")
        if slope_pct < 0:
            dim_lines.append(f"      âš ï¸ æ¯æœˆå¹³å‡ä¸‹é™ {abs(slope_pct):.1f}%")
        else:
            dim_lines.append(f"      âœ… æ¯æœˆå¹³å‡å¢é•¿ {slope_pct:.1f}%")
        dim_lines.append(f"      â†’ è¶‹åŠ¿å¾—åˆ†: {trend_score:.2f}")
        
        dim_lines.append(f"   ğŸ“… è¿‘æœŸçŠ¶æ€ (40%æƒé‡):")
        dim_lines.append(f"      æ—©æœŸ3æœˆå‡å€¼: {early_avg:.2f}  â†’  è¿‘æœŸ3æœˆå‡å€¼: {recent_avg:.2f}")
        dim_lines.append(f"      å˜åŒ–ç‡: {change_pct:+.1f}%")
        dim_lines.append(f"      â†’ è¿‘æœŸå¾—åˆ†: {recent_score:.2f}")
        
        dim_lines.append(f"   ğŸ“Š ç¨³å®šæ€§ (20%æƒé‡):")
        dim_lines.append(f"      æœˆåº¦æ³¢åŠ¨ç‡: {volatility_pct:.1f}%")
        if volatility_pct > 30:
            dim_lines.append(f"      âš ï¸ æ³¢åŠ¨è¾ƒå¤§ (>30%)")
        else:
            dim_lines.append(f"      âœ… æ³¢åŠ¨å¯æ§ (â‰¤30%)")
        dim_lines.append(f"      â†’ ç¨³å®šæ€§æ‰£åˆ†: {stability_score:.2f}")
        
        dim_lines.append(f"   â¡ï¸ ç»´åº¦æ€»åˆ†: {score:.2f} / 25")
        
        return dim_lines
    
    # 1. æ´»è·ƒåº¦
    lines.append("\nã€1. æ´»è·ƒåº¦ã€‘(0-25åˆ†)")
    activity = factors.get("activity", {})
    early_events = earliest.get("total_events", 0)
    late_events = latest.get("total_events", 0)
    lines.extend(format_dimension("æ´»è·ƒåº¦", activity, early_events, late_events, " äº‹ä»¶"))
    
    # 2. è´¡çŒ®è€…
    lines.append("\nã€2. è´¡çŒ®è€…ã€‘(0-25åˆ†)")
    contributors = factors.get("contributors", {})
    early_actors = earliest.get("unique_actors", earliest.get("node_count", 0))
    late_actors = latest.get("unique_actors", latest.get("node_count", 0))
    lines.extend(format_dimension("è´¡çŒ®è€…", contributors, early_actors, late_actors, " äºº"))
    
    # 3. æ ¸å¿ƒæˆå‘˜ç¨³å®šæ€§
    lines.append("\nã€3. æ ¸å¿ƒæˆå‘˜ç¨³å®šæ€§ã€‘(0-25åˆ†)")
    core_stability = factors.get("core_stability", {})
    early_core_count = core_stability.get("early_core_count", 0)
    late_core_count = core_stability.get("latest_core_count", 0)
    final_retention = core_stability.get("final_retention", 1)
    retained = core_stability.get("retained_count", 0)
    
    # è·å–æ ¸å¿ƒæˆå‘˜åå•
    early_core_actors = earliest.get("core_actors", [])[:5]
    late_core_actors = latest.get("core_actors", [])[:5]
    
    lines.append(f"   ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    lines.append(f"      é¦–æœˆæ ¸å¿ƒæˆå‘˜: {early_core_count} äºº")
    if early_core_actors:
        names = [a[0] if isinstance(a, (list, tuple)) else a for a in early_core_actors]
        lines.append(f"         â””â”€ {', '.join(names[:5])}")
    lines.append(f"      æœ«æœˆæ ¸å¿ƒæˆå‘˜: {late_core_count} äºº")
    if late_core_actors:
        names = [a[0] if isinstance(a, (list, tuple)) else a for a in late_core_actors]
        lines.append(f"         â””â”€ {', '.join(names[:5])}")
    lines.append(f"      æœ€ç»ˆç•™å­˜: {retained}/{early_core_count} äºº ({final_retention:.1%})")
    
    # ä¸‰å±‚åˆ†æ
    trend = core_stability.get("long_term_trend", {})
    recent = core_stability.get("recent_state", {})
    stability = core_stability.get("stability", {})
    
    lines.append(f"   ğŸ“‰ é•¿æœŸè¶‹åŠ¿ (40%æƒé‡):")
    lines.append(f"      æµå¤±ç‡æ–œç‡: {trend.get('slope_percent_per_month', 0):+.2f}%/æœˆ")
    lines.append(f"      â†’ è¶‹åŠ¿å¾—åˆ†: {trend.get('score', 0):.2f}")
    
    lines.append(f"   ğŸ“… è¿‘æœŸçŠ¶æ€ (40%æƒé‡):")
    lines.append(f"      æ—©æœŸæµå¤±ç‡: {recent.get('early_avg', 0)*100:.1f}%  â†’  è¿‘æœŸæµå¤±ç‡: {recent.get('recent_avg', 0)*100:.1f}%")
    lines.append(f"      â†’ è¿‘æœŸå¾—åˆ†: {recent.get('score', 0):.2f}")
    
    lines.append(f"   ğŸ“Š ç¨³å®šæ€§ (20%æƒé‡):")
    lines.append(f"      æœˆåº¦æ³¢åŠ¨ç‡: {stability.get('volatility_percent', 0):.1f}%")
    lines.append(f"      â†’ ç¨³å®šæ€§æ‰£åˆ†: {stability.get('score', 0):.2f}")
    
    lines.append(f"   â¡ï¸ ç»´åº¦æ€»åˆ†: {core_stability.get('score', 0):.2f} / 25")
    
    # 4. åä½œå¯†åº¦
    lines.append("\nã€4. åä½œå¯†åº¦ã€‘(0-25åˆ†)")
    collaboration = factors.get("collaboration", {})
    early_density = earliest.get("density", 0)
    late_density = latest.get("density", 0)
    lines.extend(format_dimension("åä½œå¯†åº¦", collaboration, early_density, late_density, ""))
    
    # æ±‡æ€»
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“‹ è¯„åˆ†æ±‡æ€»")
    lines.append("-" * 80)
    
    activity_score = factors.get("activity", {}).get("score", 0)
    contributor_score = factors.get("contributors", {}).get("score", 0)
    stability_score = factors.get("core_stability", {}).get("score", 0)
    collaboration_score = factors.get("collaboration", {}).get("score", 0)
    
    lines.append(f"   æ´»è·ƒåº¦:         {activity_score:6.2f} / 25")
    lines.append(f"   è´¡çŒ®è€…:         {contributor_score:6.2f} / 25")
    lines.append(f"   æ ¸å¿ƒæˆå‘˜ç¨³å®šæ€§: {stability_score:6.2f} / 25")
    lines.append(f"   åä½œå¯†åº¦:       {collaboration_score:6.2f} / 25")
    lines.append(f"   " + "-" * 30)
    lines.append(f"   æ€»åˆ†:           {score:6.2f} / 100")
    
    # æ˜¾ç¤ºåˆ†ææ–¹æ³•
    method = burnout.get("analysis_method", "legacy")
    if method == "three_layer":
        lines.append(f"\n   ğŸ“ åˆ†ææ–¹æ³•: ä¸‰å±‚æ¶æ„ (é•¿æœŸè¶‹åŠ¿+è¿‘æœŸçŠ¶æ€+ç¨³å®šæ€§)")
    
    # é¢„è­¦ä¿¡æ¯
    alerts = repo_data.get("alerts", [])
    if alerts:
        lines.append("\n" + "-" * 80)
        lines.append(f"âš ï¸ é¢„è­¦äº‹ä»¶ ({len(alerts)} æ¡)")
        lines.append("-" * 80)
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        high_alerts = [a for a in alerts if a.get("severity") == "high"]
        medium_alerts = [a for a in alerts if a.get("severity") == "medium"]
        
        if high_alerts:
            lines.append(f"\n   ğŸ”´ é«˜å±é¢„è­¦ ({len(high_alerts)} æ¡):")
            for alert in high_alerts[:5]:
                lines.append(f"      [{alert.get('month')}] {alert.get('alert_type')}: {alert.get('description')}")
            if len(high_alerts) > 5:
                lines.append(f"      ... è¿˜æœ‰ {len(high_alerts) - 5} æ¡")
        
        if medium_alerts:
            lines.append(f"\n   ğŸŸ  ä¸­å±é¢„è­¦ ({len(medium_alerts)} æ¡):")
            for alert in medium_alerts[:5]:
                lines.append(f"      [{alert.get('month')}] {alert.get('alert_type')}: {alert.get('description')}")
            if len(medium_alerts) > 5:
                lines.append(f"      ... è¿˜æœ‰ {len(medium_alerts) - 5} æ¡")
    
    # æœˆåº¦è¶‹åŠ¿
    lines.append("\n" + "-" * 80)
    lines.append("ğŸ“… æœˆåº¦æŒ‡æ ‡è¶‹åŠ¿")
    lines.append("-" * 80)
    lines.append(f"   {'æœˆä»½':<10} {'äº‹ä»¶æ•°':>8} {'è´¡çŒ®è€…':>8} {'æ ¸å¿ƒæˆå‘˜':>8} {'å¯†åº¦':>12}")
    lines.append("   " + "-" * 50)
    
    for m in sorted_metrics:
        month = m.get("month", "N/A")
        events = m.get("total_events", 0)
        actors = m.get("unique_actors", m.get("node_count", 0))
        core = m.get("core_actor_count", len(m.get("core_actors", [])))
        density = m.get("density", 0)
        lines.append(f"   {month:<10} {events:>8} {actors:>8} {core:>8} {density:>12.6f}")
    
    lines.append("")
    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆè¯¦ç»†å€¦æ€ åˆ†ææŠ¥å‘Š")
    parser.add_argument(
        "--input",
        type=str,
        default="output/burnout-analysis/full_analysis.json",
        help="è¾“å…¥çš„å®Œæ•´åˆ†ææ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output/burnout-analysis/detailed_report.txt",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--repo",
        type=str,
        default=None,
        help="åªåˆ†ææŒ‡å®šçš„ä»“åº“ï¼ˆå¯ç”¨é€—å·åˆ†éš”å¤šä¸ªï¼‰"
    )
    parser.add_argument(
        "--top",
        type=int,
        default=None,
        help="åªè¾“å‡ºå€¦æ€ è¯„åˆ†æœ€é«˜çš„å‰ N ä¸ªé¡¹ç›®"
    )
    parser.add_argument(
        "--min-score",
        type=float,
        default=None,
        help="åªè¾“å‡ºå€¦æ€ è¯„åˆ†å¤§äºç­‰äºè¯¥å€¼çš„é¡¹ç›®"
    )
    
    args = parser.parse_args()
    
    # è¯»å–åˆ†ææ•°æ®
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    print(f"ğŸ“– è¯»å–åˆ†ææ•°æ®: {input_path}")
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # ç­›é€‰ä»“åº“
    repos_to_analyze = list(data.keys())
    
    if args.repo:
        specified_repos = [r.strip() for r in args.repo.split(",")]
        repos_to_analyze = [r for r in repos_to_analyze if r in specified_repos]
        if not repos_to_analyze:
            print(f"âŒ æœªæ‰¾åˆ°æŒ‡å®šçš„ä»“åº“: {args.repo}")
            return
    
    # æŒ‰å€¦æ€ è¯„åˆ†æ’åº
    repos_with_scores = []
    for repo in repos_to_analyze:
        score = data[repo].get("burnout_score", {}).get("score", 0)
        repos_with_scores.append((repo, score))
    
    repos_with_scores.sort(key=lambda x: x[1], reverse=True)
    
    # ç­›é€‰æ¡ä»¶
    if args.min_score is not None:
        repos_with_scores = [(r, s) for r, s in repos_with_scores if s >= args.min_score]
    
    if args.top is not None:
        repos_with_scores = repos_with_scores[:args.top]
    
    if not repos_with_scores:
        print("âŒ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„é¡¹ç›®")
        return
    
    print(f"ğŸ“Š å°†åˆ†æ {len(repos_with_scores)} ä¸ªé¡¹ç›®")
    
    # ç”ŸæˆæŠ¥å‘Š
    reports = []
    reports.append("=" * 80)
    reports.append("ğŸ” OSS é¡¹ç›®ç»´æŠ¤è€…å€¦æ€ è¯¦ç»†åˆ†ææŠ¥å‘Š")
    reports.append("=" * 80)
    reports.append(f"ç”Ÿæˆæ—¶é—´: {Path(args.input).stat().st_mtime}")
    reports.append(f"åˆ†æé¡¹ç›®æ•°: {len(repos_with_scores)}")
    reports.append("")
    
    for repo, score in repos_with_scores:
        report = generate_repo_report(repo, data[repo])
        reports.append(report)
    
    full_report = "\n".join(reports)
    
    # è¾“å‡ºåˆ°æ–‡ä»¶
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(full_report)
    
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
    
    # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼ˆå¦‚æœé¡¹ç›®æ•°å°‘äºç­‰äº3ï¼‰
    if len(repos_with_scores) <= 3:
        print("\n" + full_report)
    else:
        # åªè¾“å‡ºå‰3ä¸ª
        print("\nğŸ“‹ å‰ 3 ä¸ªé«˜é£é™©é¡¹ç›®é¢„è§ˆ:\n")
        for repo, score in repos_with_scores[:3]:
            print(generate_repo_report(repo, data[repo]))


if __name__ == "__main__":
    main()
