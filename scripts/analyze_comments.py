#!/usr/bin/env python3
"""
ç¤ºä¾‹ï¼šä»å›¾ä¸­æå–å¹¶åˆ†æè¯„è®ºå†…å®¹

å±•ç¤ºå¦‚ä½•ï¼š
1. åŠ è½½å«æœ‰è¯„è®ºæ•°æ®çš„å›¾
2. æå– COMMENTED_ISSUE å’Œ REVIEWED_PR è¾¹çš„è¯„è®º
3. è¿›è¡Œç®€å•çš„æ–‡æœ¬åˆ†æï¼ˆè¯é¢‘ã€é•¿åº¦ç»Ÿè®¡ç­‰ï¼‰
"""

import json
from pathlib import Path
from collections import Counter
from typing import List, Dict

import networkx as nx
import numpy as np


def load_graph_with_comments(graph_file: str) -> nx.MultiDiGraph:
    """åŠ è½½åŒ…å«è¯„è®ºçš„å›¾"""
    if not Path(graph_file).exists():
        raise FileNotFoundError(f"å›¾æ–‡ä»¶ä¸å­˜åœ¨: {graph_file}")
    
    graph = nx.read_graphml(graph_file)
    print(f"âœ… åŠ è½½å›¾: {graph_file}")
    print(f"   èŠ‚ç‚¹æ•°: {graph.number_of_nodes()}")
    print(f"   è¾¹æ•°: {graph.number_of_edges()}")
    return graph


def extract_comments_from_edges(
    graph: nx.MultiDiGraph,
    edge_types: List[str] = None,
) -> List[Dict]:
    """
    ä»å›¾ä¸­æå–è¯„è®ºæ•°æ®
    
    Args:
        graph: NetworkX å›¾
        edge_types: è¦æå–çš„è¾¹ç±»å‹ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
    
    Returns:
        è¯„è®ºåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {source, target, edge_type, comment_body, created_at}
    """
    if edge_types is None:
        edge_types = ["COMMENTED_ISSUE", "REVIEWED_PR"]
    
    comments = []
    
    for source, target, key, attrs in graph.edges(keys=True, data=True):
        edge_type = attrs.get("edge_type")
        
        if edge_type not in edge_types:
            continue
        
        comment_body = attrs.get("comment_body", "")
        if not comment_body:
            continue
        
        comments.append({
            "source": source,
            "target": target,
            "edge_type": edge_type,
            "comment_body": comment_body,
            "created_at": attrs.get("created_at", ""),
        })
    
    return comments


def analyze_comments(comments: List[Dict]) -> Dict:
    """
    å¯¹è¯„è®ºè¿›è¡ŒåŸºç¡€ç»Ÿè®¡åˆ†æ
    
    Args:
        comments: è¯„è®ºåˆ—è¡¨
    
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    if not comments:
        return {"total": 0}
    
    lengths = [len(c["comment_body"]) for c in comments]
    word_counts = [len(c["comment_body"].split()) for c in comments]
    
    # ç»Ÿè®¡è¾¹ç±»å‹åˆ†å¸ƒ
    edge_types_count = Counter(c["edge_type"] for c in comments)
    
    # è¯é¢‘åˆ†æï¼ˆç®€å•ç¤ºä¾‹ï¼‰
    all_words = []
    for comment in comments:
        words = comment["comment_body"].lower().split()
        # è¿‡æ»¤çŸ­è¯å’Œå¸¸è§è¯
        words = [w for w in words if len(w) > 3 and not w.startswith("@")]
        all_words.extend(words)
    
    word_freq = Counter(all_words)
    top_words = word_freq.most_common(20)
    
    return {
        "total": len(comments),
        "length": {
            "mean": float(np.mean(lengths)),
            "median": float(np.median(lengths)),
            "max": int(np.max(lengths)),
            "min": int(np.min(lengths)),
        },
        "word_count": {
            "mean": float(np.mean(word_counts)),
            "median": float(np.median(word_counts)),
            "max": int(np.max(word_counts)),
            "min": int(np.min(word_counts)),
        },
        "edge_types": dict(edge_types_count),
        "top_words": top_words,
    }


def print_analysis_report(analysis: Dict):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("è¯„è®ºåˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
    print(f"  æ€»è¯„è®ºæ•°: {analysis['total']}")
    
    if analysis.get("length"):
        length_stats = analysis["length"]
        print(f"\nğŸ“ è¯„è®ºé•¿åº¦ï¼ˆå­—ç¬¦ï¼‰:")
        print(f"  å¹³å‡å€¼: {length_stats['mean']:.1f}")
        print(f"  ä¸­ä½æ•°: {length_stats['median']:.1f}")
        print(f"  èŒƒå›´: {length_stats['min']} ~ {length_stats['max']}")
    
    if analysis.get("word_count"):
        word_stats = analysis["word_count"]
        print(f"\nğŸ“š è¯„è®ºå­—æ•°ï¼ˆå•è¯ï¼‰:")
        print(f"  å¹³å‡å€¼: {word_stats['mean']:.1f}")
        print(f"  ä¸­ä½æ•°: {word_stats['median']:.1f}")
        print(f"  èŒƒå›´: {word_stats['min']} ~ {word_stats['max']}")
    
    if analysis.get("edge_types"):
        print(f"\nğŸ”— è¾¹ç±»å‹åˆ†å¸ƒ:")
        for edge_type, count in sorted(analysis["edge_types"].items(), key=lambda x: -x[1]):
            print(f"  {edge_type}: {count}")
    
    if analysis.get("top_words"):
        print(f"\nğŸ† é«˜é¢‘è¯ Top 20:")
        for i, (word, count) in enumerate(analysis["top_words"], 1):
            print(f"  {i:2d}. {word:15s} ({count:4d})")


def find_most_active_discussants(graph: nx.MultiDiGraph) -> List[Dict]:
    """
    æ‰¾å‡ºæœ€æ´»è·ƒçš„è®¨è®ºå‚ä¸è€…
    
    Returns:
        å‚ä¸è€…åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« {actor, discussion_count, comment_count}
    """
    actor_stats = {}
    
    for source, target, key, attrs in graph.edges(keys=True, data=True):
        edge_type = attrs.get("edge_type")
        
        # åªè®¡ç®—è¯„è®ºç±»è¾¹
        if edge_type not in ["COMMENTED_ISSUE", "REVIEWED_PR"]:
            continue
        
        if source not in actor_stats:
            actor_stats[source] = {
                "actor": source,
                "discussion_count": 0,
                "comment_count": 0,
                "has_comments": 0,
            }
        
        actor_stats[source]["discussion_count"] += 1
        
        if attrs.get("comment_body"):
            actor_stats[source]["comment_count"] += 1
            actor_stats[source]["has_comments"] += 1
    
    # æ’åº
    result = sorted(
        actor_stats.values(),
        key=lambda x: x["comment_count"],
        reverse=True
    )
    
    return result[:10]  # è¿”å› Top 10


def print_active_discussants(discussants: List[Dict]):
    """æ‰“å°æ´»è·ƒè®¨è®ºè€…"""
    if not discussants:
        print("æœªæ‰¾åˆ°æ´»è·ƒè®¨è®ºè€…")
        return
    
    print("\n" + "=" * 60)
    print("æœ€æ´»è·ƒè®¨è®ºè€… Top 10")
    print("=" * 60)
    print(f"{'æ’å':<5} {'Actor':<30} {'è®¨è®ºæ•°':<10} {'æœ‰è¯„è®º':<10}")
    print("-" * 60)
    
    for i, d in enumerate(discussants, 1):
        actor = d["actor"][:28]  # æˆªæ–­é•¿åç§°
        print(f"{i:<5} {actor:<30} {d['discussion_count']:<10} {d['has_comments']:<10}")


def export_comments_to_csv(comments: List[Dict], output_file: str):
    """å¯¼å‡ºè¯„è®ºåˆ° CSV æ–‡ä»¶"""
    import csv
    
    if not comments:
        print("æ²¡æœ‰è¯„è®ºå¯å¯¼å‡º")
        return
    
    with open(output_file, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["source", "target", "edge_type", "created_at", "comment_body"]
        )
        writer.writeheader()
        writer.writerows(comments)
    
    print(f"âœ… å·²å¯¼å‡º {len(comments)} æ¡è¯„è®ºåˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆ†æå›¾ä¸­çš„è¯„è®ºå†…å®¹")
    parser.add_argument(
        "--graph",
        type=str,
        default="output/monthly-graphs/facebook-react/actor-discussion/2023-01.graphml",
        help="å›¾æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--export-csv",
        type=str,
        default=None,
        help="å¯¼å‡ºè¯„è®ºåˆ° CSV æ–‡ä»¶"
    )
    parser.add_argument(
        "--show-samples",
        type=int,
        default=5,
        help="æ˜¾ç¤ºå‰ N æ¡è¯„è®ºæ ·æœ¬"
    )
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½å›¾
        graph = load_graph_with_comments(args.graph)
        
        # æå–è¯„è®º
        print("\næ­£åœ¨æå–è¯„è®º...")
        comments = extract_comments_from_edges(graph)
        print(f"âœ… æå–äº† {len(comments)} æ¡è¯„è®º")
        
        # åˆ†æ
        print("\næ­£åœ¨åˆ†æè¯„è®º...")
        analysis = analyze_comments(comments)
        print_analysis_report(analysis)
        
        # æ´»è·ƒè®¨è®ºè€…
        print("\næ­£åœ¨åˆ†ææ´»è·ƒè®¨è®ºè€…...")
        discussants = find_most_active_discussants(graph)
        print_active_discussants(discussants)
        
        # æ˜¾ç¤ºæ ·æœ¬
        if comments and args.show_samples > 0:
            print(f"\nè¯„è®ºæ ·æœ¬ï¼ˆå‰ {args.show_samples} æ¡ï¼‰:")
            print("-" * 60)
            for i, comment in enumerate(comments[:args.show_samples], 1):
                print(f"\n{i}. {comment['source']} â†’ {comment['target']}")
                print(f"   ç±»å‹: {comment['edge_type']}")
                print(f"   æ—¶é—´: {comment['created_at']}")
                body = comment["comment_body"]
                if len(body) > 200:
                    body = body[:200] + "..."
                print(f"   å†…å®¹: {body}")
        
        # å¯¼å‡º
        if args.export_csv:
            export_comments_to_csv(comments, args.export_csv)
        
        print("\nâœ… åˆ†æå®Œæˆï¼")
    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()
